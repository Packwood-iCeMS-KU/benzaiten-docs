{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the R\u0101poi HPC documentation! \u00b6 The R\u0101poi HPC (High Performance Computing) system, also known as Raapoi, is a computing resource provided by Te Herenga Waka that allows you to run large and complex computing tasks. The cluster uses the Slurm resource manager to schedule jobs and allocate resources, such as CPU, memory, and time, to each job. It's important to carefully consider how much of these resources you need for your job, as requesting too little can result in your job terminating prematurely or running slowly, while requesting too much may unnecessarily delay the execution of other jobs. R\u0101poi consists of different partitions, each of which is a group of compute nodes with a specific hardware configuration. The partition you use for your job will depend on the type of work or task you want to submit. To customize your computing environment, R\u0101poi uses the module system, which allows you to easily access the applications and programming languages you need. In this documentation, you will find all the information you need to get started with running jobs on the R\u0101poi HPC Cluster. If you need more help, check out the training tutorials, which provide step-by-step instructions for popular applications and languages, or the examples section for simpler examples. If you notice any errors or outdated information in the documentation, please don't hesitate to create an issue on the documentation github page or submit a pull request to improve the documentation. You can also find more information about the current R\u0101poi hardware layout .","title":"Overview"},{"location":"#welcome-to-the-rapoi-hpc-documentation","text":"The R\u0101poi HPC (High Performance Computing) system, also known as Raapoi, is a computing resource provided by Te Herenga Waka that allows you to run large and complex computing tasks. The cluster uses the Slurm resource manager to schedule jobs and allocate resources, such as CPU, memory, and time, to each job. It's important to carefully consider how much of these resources you need for your job, as requesting too little can result in your job terminating prematurely or running slowly, while requesting too much may unnecessarily delay the execution of other jobs. R\u0101poi consists of different partitions, each of which is a group of compute nodes with a specific hardware configuration. The partition you use for your job will depend on the type of work or task you want to submit. To customize your computing environment, R\u0101poi uses the module system, which allows you to easily access the applications and programming languages you need. In this documentation, you will find all the information you need to get started with running jobs on the R\u0101poi HPC Cluster. If you need more help, check out the training tutorials, which provide step-by-step instructions for popular applications and languages, or the examples section for simpler examples. If you notice any errors or outdated information in the documentation, please don't hesitate to create an issue on the documentation github page or submit a pull request to improve the documentation. You can also find more information about the current R\u0101poi hardware layout .","title":"Welcome to the R\u0101poi HPC documentation!"},{"location":"accessing_the_cluster/","text":"Accessing the Cluster \u00b6 To access R\u0101poi, you'll first need to get an account provisioned for you by contacting the CAD research support team with your: Full Name VUW staff username Faculty, School or Institute affiliation. If you don't have a VUW staff account, it may still be possible to be given access - please contact us to determine options. Access is via SSH Hostname: raapoi.vuw.ac.nz IP Address: 130.195.19.14 Port: 22 Username: Your VUW username Password: Your VUW password NOTE: A wired network connection or VPN is required if connecting from campus wifi or from off-campus. Some users have had issues with using the hostname and instead need to use the IP address, eg harrelwe@130.195.19.14 More information on VUW VPN services can be found here . Here is a general overview of SSH https://www.howtogeek.com/311287/how-to-connect-to-an-ssh-server-from-windows-macos-or-linux/ . SSH Clients \u00b6 Mac OSX SSH Clients You can use the built-in Terminal.app or you can download iTerm2 or XQuartz. XQuartz is required to be installed if you wish to forward GUI applications (matlab, rstudio, xstata, sas, etc), aka X forwarding. Terminal.app is the default application for command-line interface To login using the built-in Terminal.app on Mac, go to Applications --> Utilities --> Terminal.app Or use Spotlight search (aka Command-Space) iTerm2 is a good replacement for the default Terminal app XQuartz is a Xforwarding application with its own terminal. XQuartz can be used in conjuction with the Terminal.app for GUI apps. NOTE: Mac users should run the following command: sudo defaults write org.macosforge.xquartz.X11 enable_iglx -bool true We have found that this allows some older GUI applications to run with fewer errors. NOTE: Once at the command prompt you can type the following to login (replace \"username\" with your VUW user): ssh -X username@raapoi.vuw.ac.nz The -X parameter tells SSH to forward any GUI windows to your local machine, this is called X forwarding. Windows SSH Clients Recommended Clients: Git Bash is a great option and is part of the Git for Windows project. MobaXterm is a good option, especially if you require access to GUI applications such as MATLAB or xStata. This also has a built-in SFTP transfer window. File Transfer with SFTP, SCP or rsync \u00b6 There are many file transfer clients available for Mac, Windows and Linux, including but not limited to Free/OpenSource Desktop tools such as Filezilla, Cyberduck, Dolphin and proprietary/licenced offerings such as WinSCP, ExpanDrive, etc One can also use built-in command-line tools on Linux, Mac and Windows (if running Git Bash or MobaXterm). The most common command-line utilities are scp, sftp and rsync In all cases you will need to supply the hostname or IP address of the cluster, see above. You may also need to supply the port (22) and a path. The paths that you will most likely use are your home or your scratch space: /nfs/home/username or /nfs/scratch/username File transfer with cloud tools \u00b6 If you are using cloud storage such as AWS, DropBox, Cloudstor please look at the examples we have in Connecting to Cloud Providers Host Keys \u00b6 An SSH host key identifies the server to your ssh client. They are an important security feature and not something you should just hit ENTER to accept. The fist time an SSH client connects to the server, it displays the servers public key fingerprint. The authenticity of host 'raapoi.vuw.ac.nz (130.195.19.126)' can't be established. ED25519 key fingerprint is SHA256:f+rhB7q5nt/HxcNK3qA8UfSdSJ7J05L1dU4C2fslkxg. This host key is known by the following other names/addresses: C:\\Users\\username/.ssh/known_hosts:109: raapoi Are you sure you want to continue connecting (yes/no/[fingerprint])? Confirm that the finger print on the login server matches the fingerprints shown below and type 'yes'. Old Raapoi 130.195.19.14: ssh-ed25519 255 SHA256:SFQSPRtu5o4cpj/CuS37DXzfrFyalMz1FA2NVmissxo From August 2023 New Raapoi 130.195.19.126: ssh-ed25519 255 SHA256:f+rhB7q5nt/HxcNK3qA8UfSdSJ7J05L1dU4C2fslkxg [!IMPORTANT] If the host key does not match the one stored on your client, you will see a warning. The Raapoi login node was replaced in August 2023, if you had been using the previous login node you can expect to see this warning about the change of host key. Double check that the fingerprint matches one of the above before replacing the key stored in your client. @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY! Someone could be eavesdropping on you right now (man-in-the-middle attack)! It is also possible that a host key has just been changed. The fingerprint for the ED25519 key sent by the remote host is SHA256:f+rhB7q5nt/HxcNK3qA8UfSdSJ7J05L1dU4C2fslkxg. Please contact your system administrator. Add correct host key in C:\\\\Users\\\\username/.ssh/known_hosts to get rid of this message. Offending ED25519 key in C:\\\\Users\\\\username/.ssh/known_hosts:38 Host key for raapoi.vuw.ac.nz has changed and you have requested strict checking. Host key verification failed. To remove an old host key for raapoi.vuw.ac.nz cached on your client run the following: ssh-keygen -R raapoi.vuw.ac.nz ... and you could also run this to remove the IP address(s) for raapoi: ssh-keygen -R 130 .195.19.14 ssh-keygen -R 130 .195.19.126","title":"Accessing the Cluster"},{"location":"accessing_the_cluster/#accessing-the-cluster","text":"To access R\u0101poi, you'll first need to get an account provisioned for you by contacting the CAD research support team with your: Full Name VUW staff username Faculty, School or Institute affiliation. If you don't have a VUW staff account, it may still be possible to be given access - please contact us to determine options. Access is via SSH Hostname: raapoi.vuw.ac.nz IP Address: 130.195.19.14 Port: 22 Username: Your VUW username Password: Your VUW password NOTE: A wired network connection or VPN is required if connecting from campus wifi or from off-campus. Some users have had issues with using the hostname and instead need to use the IP address, eg harrelwe@130.195.19.14 More information on VUW VPN services can be found here . Here is a general overview of SSH https://www.howtogeek.com/311287/how-to-connect-to-an-ssh-server-from-windows-macos-or-linux/ .","title":"Accessing the Cluster"},{"location":"accessing_the_cluster/#ssh-clients","text":"Mac OSX SSH Clients You can use the built-in Terminal.app or you can download iTerm2 or XQuartz. XQuartz is required to be installed if you wish to forward GUI applications (matlab, rstudio, xstata, sas, etc), aka X forwarding. Terminal.app is the default application for command-line interface To login using the built-in Terminal.app on Mac, go to Applications --> Utilities --> Terminal.app Or use Spotlight search (aka Command-Space) iTerm2 is a good replacement for the default Terminal app XQuartz is a Xforwarding application with its own terminal. XQuartz can be used in conjuction with the Terminal.app for GUI apps. NOTE: Mac users should run the following command: sudo defaults write org.macosforge.xquartz.X11 enable_iglx -bool true We have found that this allows some older GUI applications to run with fewer errors. NOTE: Once at the command prompt you can type the following to login (replace \"username\" with your VUW user): ssh -X username@raapoi.vuw.ac.nz The -X parameter tells SSH to forward any GUI windows to your local machine, this is called X forwarding. Windows SSH Clients Recommended Clients: Git Bash is a great option and is part of the Git for Windows project. MobaXterm is a good option, especially if you require access to GUI applications such as MATLAB or xStata. This also has a built-in SFTP transfer window.","title":"SSH Clients"},{"location":"accessing_the_cluster/#file-transfer-with-sftp-scp-or-rsync","text":"There are many file transfer clients available for Mac, Windows and Linux, including but not limited to Free/OpenSource Desktop tools such as Filezilla, Cyberduck, Dolphin and proprietary/licenced offerings such as WinSCP, ExpanDrive, etc One can also use built-in command-line tools on Linux, Mac and Windows (if running Git Bash or MobaXterm). The most common command-line utilities are scp, sftp and rsync In all cases you will need to supply the hostname or IP address of the cluster, see above. You may also need to supply the port (22) and a path. The paths that you will most likely use are your home or your scratch space: /nfs/home/username or /nfs/scratch/username","title":"File Transfer with SFTP, SCP or rsync"},{"location":"accessing_the_cluster/#file-transfer-with-cloud-tools","text":"If you are using cloud storage such as AWS, DropBox, Cloudstor please look at the examples we have in Connecting to Cloud Providers","title":"File transfer with cloud tools"},{"location":"accessing_the_cluster/#host-keys","text":"An SSH host key identifies the server to your ssh client. They are an important security feature and not something you should just hit ENTER to accept. The fist time an SSH client connects to the server, it displays the servers public key fingerprint. The authenticity of host 'raapoi.vuw.ac.nz (130.195.19.126)' can't be established. ED25519 key fingerprint is SHA256:f+rhB7q5nt/HxcNK3qA8UfSdSJ7J05L1dU4C2fslkxg. This host key is known by the following other names/addresses: C:\\Users\\username/.ssh/known_hosts:109: raapoi Are you sure you want to continue connecting (yes/no/[fingerprint])? Confirm that the finger print on the login server matches the fingerprints shown below and type 'yes'. Old Raapoi 130.195.19.14: ssh-ed25519 255 SHA256:SFQSPRtu5o4cpj/CuS37DXzfrFyalMz1FA2NVmissxo From August 2023 New Raapoi 130.195.19.126: ssh-ed25519 255 SHA256:f+rhB7q5nt/HxcNK3qA8UfSdSJ7J05L1dU4C2fslkxg [!IMPORTANT] If the host key does not match the one stored on your client, you will see a warning. The Raapoi login node was replaced in August 2023, if you had been using the previous login node you can expect to see this warning about the change of host key. Double check that the fingerprint matches one of the above before replacing the key stored in your client. @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY! Someone could be eavesdropping on you right now (man-in-the-middle attack)! It is also possible that a host key has just been changed. The fingerprint for the ED25519 key sent by the remote host is SHA256:f+rhB7q5nt/HxcNK3qA8UfSdSJ7J05L1dU4C2fslkxg. Please contact your system administrator. Add correct host key in C:\\\\Users\\\\username/.ssh/known_hosts to get rid of this message. Offending ED25519 key in C:\\\\Users\\\\username/.ssh/known_hosts:38 Host key for raapoi.vuw.ac.nz has changed and you have requested strict checking. Host key verification failed. To remove an old host key for raapoi.vuw.ac.nz cached on your client run the following: ssh-keygen -R raapoi.vuw.ac.nz ... and you could also run this to remove the IP address(s) for raapoi: ssh-keygen -R 130 .195.19.14 ssh-keygen -R 130 .195.19.126","title":"Host Keys"},{"location":"basic_commands/","text":"Basic Commands \u00b6 The vuw Commands \u00b6 In an effort to make using R\u0101poi just a bit easier, CAD staff have created commands to help you view useful information. We call these the vuw commands. This is because all the commands begin with the string vuw . This makes it easier to see the commands available to you. If, at a command prompt you type vuw followed immediately by two TAB keys you will see a list of available commands beginning with vuw . Go ahead and type vuw-TAB-TAB to see for yourself. The commands available as of this update are: vuw-help : Prints this help information vuw-job-report : Provides some summary information about a job vuw-quota : Prints current storage quota and usage vuw-partitions : Prints a list of available partitions and the availability of compute nodes vuw-alljobs : Prints a list of all user jobs vuw-myjobs : Prints a list of your running or pending jobs vuw-job-history : Show jobs finished in last 5 days vuw-job-eff : Show efficiency of your jobs. Use vuw-job-eff --help for more information Linux Commands \u00b6 R\u0101poi is built using the Linux operating system. Access is primarily via command line interface (CLI) as opposed to the graphical user interfaces (GUI) that you are more familiar with (such as those on Windows or Mac) Below are a list of common commands for viewing and managing files and directories (replace the file and directory names with ones you own): ls - This command lists the contents of the current directory * ls -l This is the same command with a flag (-l) which lists the contents with more information, including access permissions * ls -a Same ls command but this time the -a flag which will also list hidden files. Hidden files start with a . (period) * ls -la Stringing flags together cd - This will change your location to a different directory (folder) * cd projects/calctest_proj * Typing cd with no arguments will take you back to your home directory mv - This will move or rename a file * mv project1.txt project2.txt * mv project2.txt projects/calctest_proj/ cp - This allows you to copy file/s and/or directories to defined locations. The cp command works very much like mv , except it copies a file instead of moving it. The general form of the command is cp source destination , for example: cp myfile.txt myfilecopy.txt Further examples and options can be seen here . rm - This will delete a file * rm projects/calctest_proj/projects2.txt * rm -r projects/calctest_proj/code The -r flag recursively removes files and directories mkdir - This will create a new directory * mkdir /nfs/home/myusername/financial To find more detailed information about any command you can use the manpages, eg: man ls Learning the Linux Shell \u00b6 A good tutorial for using linux can be found here: Learning the linux shell . Software Carpentry also provides a good introduction to the shell, including how to work with files and directories .","title":"Basic Commands"},{"location":"basic_commands/#basic-commands","text":"","title":"Basic Commands"},{"location":"basic_commands/#the-vuw-commands","text":"In an effort to make using R\u0101poi just a bit easier, CAD staff have created commands to help you view useful information. We call these the vuw commands. This is because all the commands begin with the string vuw . This makes it easier to see the commands available to you. If, at a command prompt you type vuw followed immediately by two TAB keys you will see a list of available commands beginning with vuw . Go ahead and type vuw-TAB-TAB to see for yourself. The commands available as of this update are: vuw-help : Prints this help information vuw-job-report : Provides some summary information about a job vuw-quota : Prints current storage quota and usage vuw-partitions : Prints a list of available partitions and the availability of compute nodes vuw-alljobs : Prints a list of all user jobs vuw-myjobs : Prints a list of your running or pending jobs vuw-job-history : Show jobs finished in last 5 days vuw-job-eff : Show efficiency of your jobs. Use vuw-job-eff --help for more information","title":"The vuw Commands"},{"location":"basic_commands/#linux-commands","text":"R\u0101poi is built using the Linux operating system. Access is primarily via command line interface (CLI) as opposed to the graphical user interfaces (GUI) that you are more familiar with (such as those on Windows or Mac) Below are a list of common commands for viewing and managing files and directories (replace the file and directory names with ones you own): ls - This command lists the contents of the current directory * ls -l This is the same command with a flag (-l) which lists the contents with more information, including access permissions * ls -a Same ls command but this time the -a flag which will also list hidden files. Hidden files start with a . (period) * ls -la Stringing flags together cd - This will change your location to a different directory (folder) * cd projects/calctest_proj * Typing cd with no arguments will take you back to your home directory mv - This will move or rename a file * mv project1.txt project2.txt * mv project2.txt projects/calctest_proj/ cp - This allows you to copy file/s and/or directories to defined locations. The cp command works very much like mv , except it copies a file instead of moving it. The general form of the command is cp source destination , for example: cp myfile.txt myfilecopy.txt Further examples and options can be seen here . rm - This will delete a file * rm projects/calctest_proj/projects2.txt * rm -r projects/calctest_proj/code The -r flag recursively removes files and directories mkdir - This will create a new directory * mkdir /nfs/home/myusername/financial To find more detailed information about any command you can use the manpages, eg: man ls","title":"Linux Commands"},{"location":"basic_commands/#learning-the-linux-shell","text":"A good tutorial for using linux can be found here: Learning the linux shell . Software Carpentry also provides a good introduction to the shell, including how to work with files and directories .","title":"Learning the Linux Shell"},{"location":"cloud_providers/","text":"Connecting to Cloud Providers \u00b6 AARNET Cloudstor \u00b6 All VUW researchers have access to the AARNET (Australia\u2019s Academic and Research Network) Cloudstor service which provides 1 TB of space to each researcher. To use this service first login and download an appropriate client to your laptop or desktop (or smarthone if you wish): Cloudstor Login NOTE: Within the Cloudstor web login settings you will need to create a Cloudstor Password, this is the password you will use to login on R\u0101poi, it does not use your VUW credentials for the command line login. We suggest setting up an App Password for Raapoi-rcopy rather than a global sync password. This way if your password is compromised you can easily just remove that app password. Setup an App Password by clicky on the settings gear on the top right and finding the App Password link . Once you have setup your cloudstor (aka ownCloud) credentials you can use them to sync data to and from R\u0101poi. For example, if I wanted to sync my project space to Cloudstor I would do the following from R\u0101poi login node: # Use Tmux to keep your session alive if you disconnect. You can reconnect to your Tmux session if you reconnect. See Tmux docs. tmux # Use our new module system module use /home/software/tools/eb_modulefiles/all/Core module load rclone/1.54.1 #check if cloudstor remote is already configured rclone listremotes The above sequence starts a tmux session to allow the transfer to continue even if I disconnect from the cluster (type tmux attach to reconnect) and then loads the rcopy module - which requires the use of our new module system. If you don't already have CloudStor configured as a remote (which you won't if this is your first time using it) follow the instructions on aarnet docs page . Once we have setup rclone to connect to CloudStor, we copy our data. In this case from <my scratch folder>/test to test on CloudStor rclone copy --progress --transfers 8 /nfs/scratch/geldenan/test CloudStor:/test Amazon AWS \u00b6 A feature-rich CLI is available in R\u0101poi. To use it you need to load the appropriate module and its module dependencies: module load amazon/aws/cli Before you proceed you will need to configure your environment with your Access Key ID and Secret Access Key, both of which will be sent to you once your account is created or linked. The command to configure your environment is aws configure You only need to do this once, unless of course you use more than one user/Access Key. Most users can simply click through the region and profile questions (using the default of \"none\"). If you do have a specific region this should be relayed along with your access and secret keys. Once you have the appropriate environment in place and your configuration setup you can use the aws command, plus an appropriate sub-command (s3, emr, rds, dynamodb, etc) and supporting arguments. More information on the CLI can be found here: http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-using.html Transferring Data to/from Amazon (AWS) S3 \u00b6 To transfer data from S3 you first need to setup your AWS connect, instructions for that can be found above. Once that is done you should be able to use the aws commands to copy data to and from your S3 storage. For example if I wanted to copy data from my S3 storage to my project directory I could do the following: tmux module load amazon/aws/cli cd /nfs/scratch/harrelwe/project aws s3 cp s3://mybucket/mydata.dat mydata.dat To copy something to storage simply reverse the file paths, eg. aws s3 cp mydata.dat s3://mybucket/mydata.dat The above starts a tmux session to allow the transfer to continue even if I disconnect from the cluster (type tmux attach to reconnect). I then load the modules necessary to use the AWS commands. I change directory to my project space and use the aws s3 cp command to copy from S3. More information on using aws can be found here: http://docs.aws.amazon.com/cli/latest/reference/s3/index.html#cli-aws-s3 Working with AWS Data Analysis Tools \u00b6 Amazon has a number of data analytics and database services available. Using the command line utilities available in R\u0101poi, researchers can perform work on the eo cluster and transfer data to AWS to perform further analysis with tools such as MapReduce (aka Hadoop), RedShift or Quicksight. A listing of available services and documentation can be found at the following: https://aws.amazon.com/products/analytics/ Google Cloud (gcloud) Connections \u00b6 The Google Cloud SDK is available in R\u0101poi. This includes a command-line interface for connecting to gloud services. To get started, first load the environment module. You can find the path with the module spider command. As of this writing the current version can be loaded thusly: module load google/cloud/sdk/212.0.0 This will give you access to the gcloud command. To setup a connection to your gcloud account use the init sub-command, eg. gcloud init --console-only Follow the instructions to authorize your gcloud account. Once on the Google website, you will be given an authorization code which you will copy/paste back into the R\u0101poi terminal window. Transferring Data to/from Google Cloud (gcloud) \u00b6 To transfer data from gcloud storage you first need to setup your gcloud credentials, instructions for that can be found above. Once that is done you should be able to use the gsutil command to copy data to and from your gcloud storage. For example, if I wanted to copy data from gcloud to my project directory I could do the following: tmux module load google/cloud/sdk/212.0.0 cd /nfs/scratch/harrelwe/project gsutil cp gs://mybucket/mydata.dat mydata.dat To copy something to storage simply reverse the file paths, eg. gsutil cp mydata.dat gs://mybucket/mydata.dat The above starts a tmux session to allow the transfer to continue even if I disconnect from the cluster (type tmux attach to reconnect). I then load the modules necessary to use the gsutil commands. I change directory to my project space and use the gsutil cp command to copy from gcloud. More information on using gcloud can be found here: https://cloud.google.com/sdk/gcloud/ Working with GCloud Data Analysis Tools \u00b6 Google Cloud has a number of data analytics and database services available. Using the gcloud command line utilities available on R\u0101poi, researchers can perform work on the cluster and transfer data to gcloud to perform further analysis with tools such as Dataproc (Hadoop/Spark), BigQuery or Datalab (Visualization) A listing of available services and documentation can be found at the following: https://cloud.google.com/products/ DropBox Cloud Storage \u00b6 NOTE: Dropbox has upload/download limitations and we have found that once your file gets above 50GB in size the transfer will have a better chance of timing out and failing. Configuring your Dropbox account on R\u0101poi Step A: On your local laptop or desktop start your browser and login to your Dropbox account Step B: On R\u0101poi type the following: module load dropbox Step C: Setup account credentials (You should only need to do this once): Run the following command from R\u0101poi dbxcli account You will now see something like the following: Go to https://www.dropbox.com/1/oauth2/authorize?client_id=X12345678&response_type=code&state=state Click \"Allow\" (you might have to log in first). Copy the authorization code. Enter the authorization code here: Step D: Copy the URL link listed in Step C1 and paste it into the web browser that you started in Step A This will provide you with a long access code (aka hash). Now copy that access code and paste it into your R\u0101poi terminal after Step C3 where it is asking for Enter the authorization code here Now hit enter or return. You should see that you are now logged in with your Dropbox credentials Basic Dropbox commands \u00b6 Remember to load the dropbox environment module if you have not already (see module spider for the path) Now type dbx or dbxcli at a prompt. You will see a number of sub-commands, for instance ls, which will list the contents of your Dropbox, eg dbxcli ls Downloading from Dropbox \u00b6 Downloading uses the subcommand called: get. The basic format for get is: dbxcli get fileOnDropbox fileOnRaapoi For instance, if I have a datafile called 2018-financials.csv on Dropbox that I want to copy to my project folder I would type: dbxcli get 2018-financials.csv /nfs/scratch/harrelwe/projects/finance_proj/2018-financials.csv Uploading to Dropbox \u00b6 Uploading is similar to downloading except now we use the subcommand: put. The basic format for put is: dbxcli put fileOnRaapoi fileOnDropbox For example I want to upload a PDF I generated from one of my jobs called final-report.pdf I would type: dbxcli put final-report.pdf final-report.pdf This will upload the PDF and name it the same thing, if I wanted to change the name on Dropbox I could: dbxcli put final-report.pdf analytics-class-final-report.pdf","title":"Connecting to Cloud Providers"},{"location":"cloud_providers/#connecting-to-cloud-providers","text":"","title":"Connecting to Cloud Providers"},{"location":"cloud_providers/#aarnet-cloudstor","text":"All VUW researchers have access to the AARNET (Australia\u2019s Academic and Research Network) Cloudstor service which provides 1 TB of space to each researcher. To use this service first login and download an appropriate client to your laptop or desktop (or smarthone if you wish): Cloudstor Login NOTE: Within the Cloudstor web login settings you will need to create a Cloudstor Password, this is the password you will use to login on R\u0101poi, it does not use your VUW credentials for the command line login. We suggest setting up an App Password for Raapoi-rcopy rather than a global sync password. This way if your password is compromised you can easily just remove that app password. Setup an App Password by clicky on the settings gear on the top right and finding the App Password link . Once you have setup your cloudstor (aka ownCloud) credentials you can use them to sync data to and from R\u0101poi. For example, if I wanted to sync my project space to Cloudstor I would do the following from R\u0101poi login node: # Use Tmux to keep your session alive if you disconnect. You can reconnect to your Tmux session if you reconnect. See Tmux docs. tmux # Use our new module system module use /home/software/tools/eb_modulefiles/all/Core module load rclone/1.54.1 #check if cloudstor remote is already configured rclone listremotes The above sequence starts a tmux session to allow the transfer to continue even if I disconnect from the cluster (type tmux attach to reconnect) and then loads the rcopy module - which requires the use of our new module system. If you don't already have CloudStor configured as a remote (which you won't if this is your first time using it) follow the instructions on aarnet docs page . Once we have setup rclone to connect to CloudStor, we copy our data. In this case from <my scratch folder>/test to test on CloudStor rclone copy --progress --transfers 8 /nfs/scratch/geldenan/test CloudStor:/test","title":"AARNET Cloudstor"},{"location":"cloud_providers/#amazon-aws","text":"A feature-rich CLI is available in R\u0101poi. To use it you need to load the appropriate module and its module dependencies: module load amazon/aws/cli Before you proceed you will need to configure your environment with your Access Key ID and Secret Access Key, both of which will be sent to you once your account is created or linked. The command to configure your environment is aws configure You only need to do this once, unless of course you use more than one user/Access Key. Most users can simply click through the region and profile questions (using the default of \"none\"). If you do have a specific region this should be relayed along with your access and secret keys. Once you have the appropriate environment in place and your configuration setup you can use the aws command, plus an appropriate sub-command (s3, emr, rds, dynamodb, etc) and supporting arguments. More information on the CLI can be found here: http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-using.html","title":"Amazon AWS"},{"location":"cloud_providers/#google-cloud-gcloud-connections","text":"The Google Cloud SDK is available in R\u0101poi. This includes a command-line interface for connecting to gloud services. To get started, first load the environment module. You can find the path with the module spider command. As of this writing the current version can be loaded thusly: module load google/cloud/sdk/212.0.0 This will give you access to the gcloud command. To setup a connection to your gcloud account use the init sub-command, eg. gcloud init --console-only Follow the instructions to authorize your gcloud account. Once on the Google website, you will be given an authorization code which you will copy/paste back into the R\u0101poi terminal window.","title":"Google Cloud (gcloud) Connections"},{"location":"cloud_providers/#dropbox-cloud-storage","text":"NOTE: Dropbox has upload/download limitations and we have found that once your file gets above 50GB in size the transfer will have a better chance of timing out and failing. Configuring your Dropbox account on R\u0101poi Step A: On your local laptop or desktop start your browser and login to your Dropbox account Step B: On R\u0101poi type the following: module load dropbox Step C: Setup account credentials (You should only need to do this once): Run the following command from R\u0101poi dbxcli account You will now see something like the following: Go to https://www.dropbox.com/1/oauth2/authorize?client_id=X12345678&response_type=code&state=state Click \"Allow\" (you might have to log in first). Copy the authorization code. Enter the authorization code here: Step D: Copy the URL link listed in Step C1 and paste it into the web browser that you started in Step A This will provide you with a long access code (aka hash). Now copy that access code and paste it into your R\u0101poi terminal after Step C3 where it is asking for Enter the authorization code here Now hit enter or return. You should see that you are now logged in with your Dropbox credentials","title":"DropBox Cloud Storage"},{"location":"containers/","text":"Using Containers \u00b6 Researchers can use Docker or Singularity containers within the cluster. This is a great way to run difficult-to-compile applications or to share workflows among colleagues. Running an interactive container \u00b6 User can run within a container interactively, this is great for testing code before running a job. Here is an example of running within a docker container that has the blockchain software called BlockSci: module load singularity srun --pty -c 4 --mem=16G bash singularity pull docker://tislaamo/blocksci singularity shell blocksci.simg Once you have typed the singularity shell command you will be within the container and can type the commands available from within the container such as the BlockSci utility blocksci_parser Running a container in batch \u00b6 Running a batch job with containers is similar to running a regular job, but will ultimately depend on how the container was created, so your mileage may vary. Here is an example batch submit script that will run the autometa software that was created in a docker image, lets name the submit file runContainer.sh: #SBATCH -J autometa-job #SBATCH -c 4 #SBATCH --mem=16G #SBATCH --mailtype=BEGIN,END,FAIL #SBATCH --mail-user=myemail@email.net #SBATCH --time=12:00:00 module load singularity singularity pull docker://jasonkwan/autometa:latest singularity exec autometa_latest.sif calculate_read_coverage.py somedata.dat Now to run the file you can: sbatch runContainer.sh Note that singularity shell is primarily for interactive use and singularity exec (or possibly singularity run ) are for executing the applications that were built within the container directly. It is important to know how the container was created to make effective use of the software.","title":"Using Containers"},{"location":"containers/#using-containers","text":"Researchers can use Docker or Singularity containers within the cluster. This is a great way to run difficult-to-compile applications or to share workflows among colleagues.","title":"Using Containers"},{"location":"containers/#running-an-interactive-container","text":"User can run within a container interactively, this is great for testing code before running a job. Here is an example of running within a docker container that has the blockchain software called BlockSci: module load singularity srun --pty -c 4 --mem=16G bash singularity pull docker://tislaamo/blocksci singularity shell blocksci.simg Once you have typed the singularity shell command you will be within the container and can type the commands available from within the container such as the BlockSci utility blocksci_parser","title":"Running an interactive container"},{"location":"containers/#running-a-container-in-batch","text":"Running a batch job with containers is similar to running a regular job, but will ultimately depend on how the container was created, so your mileage may vary. Here is an example batch submit script that will run the autometa software that was created in a docker image, lets name the submit file runContainer.sh: #SBATCH -J autometa-job #SBATCH -c 4 #SBATCH --mem=16G #SBATCH --mailtype=BEGIN,END,FAIL #SBATCH --mail-user=myemail@email.net #SBATCH --time=12:00:00 module load singularity singularity pull docker://jasonkwan/autometa:latest singularity exec autometa_latest.sif calculate_read_coverage.py somedata.dat Now to run the file you can: sbatch runContainer.sh Note that singularity shell is primarily for interactive use and singularity exec (or possibly singularity run ) are for executing the applications that were built within the container directly. It is important to know how the container was created to make effective use of the software.","title":"Running a container in batch"},{"location":"environment/","text":"Enviroment Setup \u00b6 Preparing your environment \u00b6 R\u0101poi has an extensive library of applications and software available. There are numerous programming languages and libraries (R, Julia, Python, lua, OpenMPI, blas, etc) as well as dozens of applications (Matlab, Gaussian, etc). We also keep older versions of software to ensure compatibility. Because of this, R\u0101poi developers use a tool called lmod to allow a user to load a specific version of an application, language or library and start using it for their work. The module command will show you what software is available to load, and will add the software to your environment for immediate use. To show all software available to load type the following: module avail You will see a long list of available modules to load, including a path, eg lua/5.3.5 However, instead of searching through a long list, if you know you want to use lua, you can find the path with the keyword subcommand: module keyword lua If you want to know more about a particular module you can use the whatis or show subcommand. Some modules have this available, for instance: harrelwe@raapoi-master:~$ module show trimmomatic/20190304 ---------------------------------------------------------------------- /home/software/tools/modulefiles/trimmomatic/20190304: ---------------------------------------------------------------------- load(\"java/jdk/1.8.0_121\") setenv(\"TM_HOME\",\"/home/software/apps/trimmomatic/20190304/bin\") module whatis R/CRAN/3.5 R/CRAN/3.5 : Adds the R library path to the pre-built CRAN modules Adding or loading software \u00b6 Once you have found the module path you can load the software: module load lua/5.3.5 After the module loads you can type srun --pty lua at a prompt, or add it to the path of your lua script (the RC team recommends using /usr/bin/env instead of an absolute path). Showing/listing the module environment modifications You can discover what the module will load into your environment you can run module show, for example here is what R adds: module show R/3.5.1 -------------------------------------------------- /home/software/tools/modulefiles/R/3.5.1: -------------------------------------------------- whatis(\"Adds the R language path to your environment \") prepend_path(\"PATH\",\"/home/software/apps/R/3.5.1/bin\") Listing loaded modules \u00b6 To see what modules you have loaded into your environment you can run the command: module list By default you will have the config module loaded (please do not unload that module). For example, here are the modules I have loaded in my environment when I wrote this section: module list Currently Loaded Modules: 1) config 2) tassel/3 3) python/3.7.0 4) python/modules/3.7","title":"Enviroment Setup"},{"location":"environment/#enviroment-setup","text":"","title":"Enviroment Setup"},{"location":"environment/#preparing-your-environment","text":"R\u0101poi has an extensive library of applications and software available. There are numerous programming languages and libraries (R, Julia, Python, lua, OpenMPI, blas, etc) as well as dozens of applications (Matlab, Gaussian, etc). We also keep older versions of software to ensure compatibility. Because of this, R\u0101poi developers use a tool called lmod to allow a user to load a specific version of an application, language or library and start using it for their work. The module command will show you what software is available to load, and will add the software to your environment for immediate use. To show all software available to load type the following: module avail You will see a long list of available modules to load, including a path, eg lua/5.3.5 However, instead of searching through a long list, if you know you want to use lua, you can find the path with the keyword subcommand: module keyword lua If you want to know more about a particular module you can use the whatis or show subcommand. Some modules have this available, for instance: harrelwe@raapoi-master:~$ module show trimmomatic/20190304 ---------------------------------------------------------------------- /home/software/tools/modulefiles/trimmomatic/20190304: ---------------------------------------------------------------------- load(\"java/jdk/1.8.0_121\") setenv(\"TM_HOME\",\"/home/software/apps/trimmomatic/20190304/bin\") module whatis R/CRAN/3.5 R/CRAN/3.5 : Adds the R library path to the pre-built CRAN modules","title":"Preparing your environment"},{"location":"environment/#adding-or-loading-software","text":"Once you have found the module path you can load the software: module load lua/5.3.5 After the module loads you can type srun --pty lua at a prompt, or add it to the path of your lua script (the RC team recommends using /usr/bin/env instead of an absolute path). Showing/listing the module environment modifications You can discover what the module will load into your environment you can run module show, for example here is what R adds: module show R/3.5.1 -------------------------------------------------- /home/software/tools/modulefiles/R/3.5.1: -------------------------------------------------- whatis(\"Adds the R language path to your environment \") prepend_path(\"PATH\",\"/home/software/apps/R/3.5.1/bin\")","title":"Adding or loading software"},{"location":"environment/#listing-loaded-modules","text":"To see what modules you have loaded into your environment you can run the command: module list By default you will have the config module loaded (please do not unload that module). For example, here are the modules I have loaded in my environment when I wrote this section: module list Currently Loaded Modules: 1) config 2) tassel/3 3) python/3.7.0 4) python/modules/3.7","title":"Listing loaded modules"},{"location":"examples/","text":"Examples \u00b6 Simple Bash Example - start here if new to HPC \u00b6 In this example we will run a very simple bash script on the quicktest partition. The bash script is very simple, it just prints the hostname - the node you're running on - and prints the date into a file. It also sleeps for 1 minute - it just does this to give you a chance to see your job in the queue with squeue First lets create a sensible working directory mkdir bash_example cd bash_example We'll use the text editor nano to create our bash script as well as our submission script. In real life, you might find it easier to create your code and submission script on your local machine, then copy them over as nano is not a great editor for large projects. Create and edit our simple bash script - this is our code we will run on the HPC nano test.sh Paste or type the following into the file #!/bin/bash hostname #prints the host name to the terminal date > date_when_job_ran.txt #puts the content of the date command into a txt file sleep 1m # do nothing for 1 minute. Job will still be \"running\" press ctrl-O to save the text in nano, then ctrl-X to exit nano. Using nano again create a file called submit.sh with the following content #!/bin/bash # #SBATCH --job-name=bash_test #SBATCH -o bash_test.out #SBATCH -e bash_test.err # #SBATCH --partition=quicktest # #SBATCH --cpus-per-task=2 #Note: you are always allocated an even number of cpus #SBATCH --mem=1G #SBATCH --time=10:00 bash test.sh #actually run our bash script, using bash If you're familiar with bash scripts, the above is a bit weird. The #SBATCH lines would normally be comments and hence not do anything, but Slurm will read those lines to determine how many resources to provide your job. In this case we ask for the following: quicktest partition (the default - so you don't technically need to ask for it). 1 cpu per task - we have one task, so we're asking for 1 cpu 1 gig of memory. a max runtime of 10 min If your job uses more memory or time than requested, Slurm will immediately kill it. If you use more CPU's than requested - your job will keep running, but your \"cpus\" will be shared bewteen the CPUs you actually requested. So if your job tried to use 10 CPUs but you only asked for one, it'll run extremely slowly - don't do this. Our submit.sh script also names our job bash_test this is what the job will show up as in squeue. We ask for things printed out on the terminal to go to two seperate files. Normal, non error, things that would be printed out on the terminal will be put into the text file bash_test.out . Errors will be printed into the text file bash_test.err Now submit your job to the Slurm queue. sbatch submit.sh #See your job in the queue squeue -u <your_username> #When job is done see the new files ls #look at the content that would have been printed to the terminal if running locally cat bash_test.out # See the content of the file that your bash script created cat date_when_job_ran.txt Simple Python program using virtualenv and pip \u00b6 First we need to create a working directory and move there mkdir python_test cd python_test Next we load the python 3 module and use python 3 to create a python virtualenv. This way we can install pip packages which are not installed on the cluster module load python/3.6.6 python3 -m venv mytest Activate the mytest virtualenv and use pip to install the webcolors package source mytest/bin/activate pip install webcolors Create the file test.py with the following contents using nano import webcolors from random import randint from socket import gethostname colour_list = list ( webcolors . CSS3_HEX_TO_NAMES . items ()) requested_colour = randint ( 0 , len ( colour_list )) colour_name = colour_list [ requested_colour ][ 1 ] print ( \"Random colour name:\" , colour_name , \" on host: \" , gethostname ()) Alternatively download it with wget: wget https://raw.githubusercontent.com/ \\ vuw-research-computing/raapoi-tools/ \\ master/examples/python_venv/test.py Using nano create the submissions script called python_submit.sh with the following content - change me@email.com to your email address. #!/bin/bash # #SBATCH --job-name=python_test #SBATCH -o python_test.out #SBATCH -e python_test.err # #SBATCH --cpus-per-task=2 #Note: you are always allocated an even number of cpus #SBATCH --mem=1G #SBATCH --time=10:00 # #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=me@email.com module load python/3.6.6 source mytest/bin/activate python test.py Alternatively download it with wget wget https://raw.githubusercontent.com/ \\ vuw-research-computing/raapoi-tools/ \\ master/examples/python_venv/python_submit.sh To submit your job to the Slurm scheduler sbatch python_submit.sh Check for your job on the queue with squeue though it might finish very fast. The output files will appear in your working directory. Using Anaconda/Miniconda/conda - idba \u00b6 Many users use Anaconda/Miniconda to manage software stacks. One way to do this is to use singularity containers with the conda environment inside - this allows the conda environment to load quickly as the many small conda files are inside a container which the file system sees as one file. However, this is also an additional bit of complexity so many users just use conda outside of singularity. You can install your own version of Anaconda/Miniconda to your home directory or scratch. We have also got packaged versions of Anaconda/Miniconda installed with our module loading system. Anaconda has many built in packages so we will use that in our examples, but Miniconda is available if you want a more minimal initial setup. module load old-mod-system/Anaconda3/2020.11 Let's create a new conda environment for this example, in a sensible location, I used ~/examples/conda/idba conda create --name idba-example # press y for the Proceed prompt if it looks correct conda activate idba-example #activate our example environment. Conda environments are beyond the scope of this example, but they are a good way to contain all the dependencies and programs for a particular workflow, in this case, idba. Install idba in our conda environment. ** Note that best practise is to do the install on a compute node ** We'll just do it here on the login node for now - the code will run slower on the compute nodes as a result! conda install -c bioconda idba Idba is a genome assembler, we will use paired-end illumina reads of E. coli. The data is available on an Amazon S3 bucket (a cloud storage location), and we can download it using wget. mkdir data # put our data in a sensible location cd data wget --content-disposition goo.gl/JDJTaz #sequence data wget --content-disposition goo.gl/tt9fsn #sequence data cd .. #back to our project directory The reads we have are paired-end fastq files but idba requires a fasta file. We can use a tool installed with idba to convert them. We'll do this on the R\u0101poi login node as it is a fast task that doesn't need many resources. fq2fa --merge --filter data/MiSeq_Ecoli_MG1655_50x_R1.fastq data/MiSeq_Ecoli_MG1655_50x_R2.fastq data/read.fa To create our submission script we need to know the path to our conda enviroment. To get this: conda env list You'll need to find your idba-example environment, and next to it is the path you'll need for your submission script. In my case: # conda environments: # base * /home/andre/anaconda3 idba-example /home/andre/anaconda3/envs/idba-example # We need this line, it'll be different for you! Create our sbatch submission script. Note that this sequence doesn't need a lot of memory, so we'll use 3G. To see your usage after the job has run use vuw-job-report <job-id> idba_submit.sh #!/bin/bash #SBATCH --job-name=idba_test #SBATCH -o _output.out #SBATCH -e _output.err #SBATCH --time=00:5:00 #SBATCH --partition=quicktest #SBATCH --ntasks=12 #SBATCH --mem=3G module load old-mod-system/Anaconda3/2020.11 eval \" $( conda shell.bash hook ) \" # basically inits your conda - prevents errors like: CommandNotFoundError: Your shell has not been properly configured ... conda activate /home/andre/anaconda3/envs/idba-example # We will need to activate our conda enviroment on the remote node idba idba_ud -r data/read.fa -o output To submit our job sbatch idba_submit.sh To see our job running or queuing squeue -u $USER This job will take a few minutes to run, generally less than 5. When the job is done we can see the output in the output folder. We can also see the std output and std err in the files _output.out and _output.err . The quickest way to examine them is to cat the files when the run is done. cat _output.out Loading R packages & running a simple job \u00b6 First login to R\u0101poi and load the R and R/CRAN modules: module load R/4.0.2 module load R/CRAN Then run R on the command line: R Test library existence: > library ( tidyverse ) This should load the package, and give some output like this: \u2500\u2500 Attaching packages \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse 1.3 . 0 \u2500\u2500 \u2714 ggplot2 3.3 . 2 \u2714 purrr 0.3 . 4 \u2714 tibble 3.0 . 1 \u2714 dplyr 1.0 . 0 \u2714 tidyr 1.1 . 0 \u2714 stringr 1.4 . 0 \u2714 readr 1.3 . 1 \u2714 forcats 0.5 . 0 \u2500\u2500 Conflicts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse_conflicts () \u2500\u2500 \u2716 dplyr :: filter () masks stats :: filter () \u2716 dplyr :: lag () masks stats :: lag () (These conflicts are normal and can be ignored.) To quit R, type: > q () Next create a bash submission script called r_submit.sh (or another name of your choice) using your preferred text editor, e.g. nano. #!/bin/bash # #SBATCH --job-name=r_test #SBATCH -o r_test.out #SBATCH -e r_test.err # #SBATCH --cpus-per-task=2 #Note: you are always allocated an even number of cpus #SBATCH --mem=1G #SBATCH --time=10:00 # module load R/4.0.2 module load R/CRAN Rscript mytest.R Save this to the current working directory, and then create another file using your preferred text editor called mytest.R (or another name of your choice) containing the following R commands: library ( tidyverse ) sprintf ( \"Hello World!\" ) then run it with the previously written bash script: sbatch r_submit.sh This submits a task that should execute quickly and create files in the directory from which it was run. Examine r_test.out . You can use an editor like nano, vi or emacs, or you can just cat or less the file to see its contents on the terminal. You should see: \"Hello World\" Matlab GPU example \u00b6 Matlab has various built-in routines which are GPU accelerated. We will run a simple speed comparison between cpu and gpu tasks. In a sensible location create a file called matlab_gpu.m I used ~/examples/matlab/cuda/matlab_gpu.m . % Set an array which will calculate the Eigenvalues of A = rand ( 1000 ); % Copy the Array to the GPU memory - this process takes an erratic amount of time, so we will not time it. Agpu = gpuArray ( A ); tic B=eig(Agpu) ; t1 = toc % Let's compare the time with CPU tic B = eig ( A ); t2 = toc We will also need a Slurm submission script; we'll call this matlab_gpu.sh . Note that we will need to use the new Easybuild module files for our cuda libraries, so make sure to include the module use line module use /home/software/tools/eb_modulefiles/all/Core #!/bin/bash #SBATCH --job-name=matlab-gpu-example #SBATCH --output=out-gpu-example.out #SBATCH --error=out-gpu-example.err #SBATCH --time=00:05:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module use /home/software/tools/eb_modulefiles/all/Core module load matlab/2021a module load fosscuda/2020b matlab -nodisplay -nosplash -nodesktop -r \"run('matlab_gpu.m');exit;\" To submit this job to the Slurm queue sbatch matlab_gpu.sh . This job will take a few minutes to run - this is mostly the Matlab startup time. Examine the queue for your job squeue -u $USER . When your job is done, inspect the output file. You can use an editor like nano, vi or emacs, or you can just cat or less the file to see its contents on the terminal. cat out-gpu-example.out What do you notice about the output? Surely GPUs should be faster than the CPU! It takes time for the GPU to start processing your task, the CPU is able to start the task far more quickly. So for short operations, the CPU can be faster than the GPU - remember to benchmark your code for optimal performance! Just because you can use a GPU for your task doesn't mean it is necessarily faster! To get a better idea of the advantage of the GPU let's increase the size of the array from 1000 to 10000 matlab_gpu.m % Set an array which will calculate the Eigenvalues of A = rand ( 10000 ); % Copy the Array to the GPU memory - this process takes an erratic amount of time, so we will not time it. Agpu = gpuArray ( A ); tic B=eig(Agpu) ; t1 = toc % Let's compare the time with CPU tic B = eig ( A ); t2 = toc To make things fairer for the CPU in this case, we will also allocate half the CPUs on the node to Matlab. Half the CPUs, half the memory and half the GPUs, just to be fair. matlab_gpu.sh #!/bin/bash #SBATCH --job-name=matlab-gpu-example #SBATCH --output=out-gpu-example.out #SBATCH --error=out-gpu-example.err #SBATCH --time=00:05:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=128 #SBATCH --mem=256G module use /home/software/tools/eb_modulefiles/all/Core module load matlab/2021a module load fosscuda/2020b matlab -nodisplay -nosplash -nodesktop -r \"run('matlab_gpu.m');exit;\" The output in my case was: < M A T L A B ( R ) > Copyright 1984 -2021 The MathWorks, Inc. R2021a Update 1 ( 9 .10.0.1649659 ) 64 -bit ( glnxa64 ) April 13 , 2021 To get started, type doc. For product information, visit www.mathworks.com. t1 = 62 .0212 t2 = 223 .0818 So in thise case the GPU was considerably faster. Matlab can do this a bit faster on the CPU if you give it fewer CPUs, the optimum appears to be around 20, but it still takes 177s. Again, optimise your resource requests for your problem, less can sometimes be more, however the GPU easily wins in this case. Job Arrays - running many similar jobs \u00b6 Slurm makes it easy to run many jobs which are similar to each other. This could be one piece of code running over many datasets in parallel or running a set of simulations with a different set of parameters for each run. Simple Bash Job Array example \u00b6 The following code will run the submission script 16 times as resources become available (i.e. they will not neccesarily run at the same time). It will just print out the Slurm array task ID and exit. submit.sh: #!/bin/bash #SBATCH --job-name=test_array #SBATCH --output=out_array_%A_%a.out #SBATCH --error=out_array_%A_%a.err #SBATCH --array=1-16 #SBATCH --time=00:00:20 #SBATCH --partition=parallel #SBATCH --ntasks=1 #SBATCH --mem=1G # Print the task id. echo \"My SLURM_ARRAY_TASK_ID: \" $SLURM_ARRAY_TASK_ID # Add lines here to run your computations. Run the example with the standard sbatch submit.sh A simple R job Array Example \u00b6 As a slightly more practical example the following will run an R script 5 times as resources become available. The R script takes as an input the $SLURM_ARRAY_TASK_ID which then selects a parameter alpha out of a lookup table. This is one way you could run simulations or similar with a set parameters defined in a lookuop table in your code. To make outputs more tidy and to help organisation, instead of dumping all the outputs into the directory with our code and submission script, we will separate the outputs into directories. Dataframes saved from R will be saved to the output/ directory, and all output which would otherwise be printed to the commnd line (stdout and stderr) will be saved to the stdout/ directory. Both of these directories will need to be created before running the script. r_random_alpha.R: # get the arguments supplied to R. # trailingOnly = TRUE gets the user supplied # arguments, and for now we will only get the # first user supplied argument args <- commandArgs ( trailingOnly = TRUE ) inputparam <- args [ 1 ] # a vector with all our parameters. alpha_vec <- c ( 2.5 , 3.3 , 5.1 , 8.2 , 10.9 ) alpha <- alpha_vec [ as.integer ( inputparam )] # Generate a random number between 0 and alpha # store it in dataframe with the coresponding # alpha value randomnum <- runif ( 1 , min = 0 , max = as.double ( alpha )) df <- data.frame ( \"alpha\" = alpha , \"random_num\" = randomnum ) # Save the data frame to a file with the alpha value # Note that the output/ folder will need to be # manually created first! outputname <- paste ( \"output/\" , \"alpha_\" , alpha , \".Rda\" , sep = \"\" ) save ( df , file = outputname ) Next create the submision script. Which we will run on the parallel partition rather than quicktest. r_submit.sh: #!/bin/bash #SBATCH --job-name=test_R_array #SBATCH --output=stdout/array_%A_%a.out #SBATCH --error=stdout/array_%A_%a.err #SBATCH --array=1-5 #SBATCH --time=00:00:20 #SBATCH --partition=parallel #SBATCH --ntasks=1 #SBATCH --mem=1G module load R/CRAN # Print the task id. Rscript r_random_alpha.R $SLURM_ARRAY_TASK_ID Run the jobs with sbatch r_submit.sh Singularity \u00b6 While there are many modules on R\u0101poi, sometimes you might want to install your own packages in your own way. Singularity allows you to do this. If you are familiar with Docker, Singularity is similar, except you can't get root (or sudo) once your container is running on the R\u0101poi. However, you can have sudo rights locally on your own machine, setup your container however you like, then run it without sudo on the cluster. Singularity/Docker container example \u00b6 Singularity allows you to use most (but not all!) docker images on R\u0101poi. On your local machine create the singularity definition file input_args_example.def BootStrap : library From : ubuntu: 16.04 %runscript exec echo \" $@ \" %labels Author Andre This will build an ubuntu 16.04 container that will eventually run on R\u0101poi which runs Centos. This container has a runscript which just echos back any arguments sent to the container when your start it up. Build the container locally with sudo and singularity sudo singularity build inputexample.sif input_args_example.def This will build an image that you can't modify any further and is immediately suitable to run on R\u0101poi Copy this file to R\u0101poi via sftp sftp <username>@raapoi.vuw.ac.nz Create a submit script using singularity on the cluster singularity_submit.sh #!/bin/bash #SBATCH --job-name=singularity_test #SBATCH -o sing_test.out #SBATCH -e sing_test.err #SBATCH --time=00:00:20 #SBATCH --ntasks=1 #SBATCH --mem=1G module load singularity singularity run inputtest.sif \"hello from a container\" Run the script with the usual singularity_submit.sh Singularity/TensorFlow Example \u00b6 tensor.def Bootstrap: docker From: tensorflow/tensorflow:latest-py3 %post apt-get update && apt-get -y install wget build-essential %runscript exec python \" $@ \" compile this locally with sudo and singularity. sudo singularity build tensorflow.sif tensor.def Create a quick tensorflow test code tensortest.py import tensorflow as tf mnist = tf . keras . datasets . mnist ( x_train , y_train ),( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 512 , activation = tf . nn . relu ), tf . keras . layers . Dropout ( 0.2 ), tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax ) ]) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( x_train , y_train , epochs = 5 ) model . evaluate ( x_test , y_test ) Copy your files to R\u0101poi via sftp (or whatever you prefer) sftp <username>@raapoi.vuw.ac.nz cd <where you want to work> put * #put all files in your local directory onto R\u0101poi Lets quickly test the code via an interactive session on a node. Note I find the tensorflow container only runs properly on intel nodes, which we don't have many of at the moment, I'll investigate this further. srun --partition = \"parallel\" --constraint = \"Intel\" --pty bash #now on the remote node - note you might need to wait if nodes are busy module load singularity #load singularity singularity shell tensorflow.sif #now inside the tensorflow container on the remote node python tensortest.py #once that runs, exit the container exit #exit the container exit #exit the interactive session on the node Create a submit script using singularity on the cluster singularity_submit.sh #!/bin/bash #SBATCH --job-name=singularity_test #SBATCH -o sing_test.out #SBATCH -e sing_test.err #SBATCH --time=00:10:00 #SBATCH --partition=parallel #SBATCH --constraint=Intel #SBATCH --ntasks=1 #SBATCH --mem=4G module load singularity #run the container with the runscript defined when we created it singularity run tensorflow.sif tensortest.py Singularity/MaxBin2 Example \u00b6 In a sensible location, either in your home directory or on the scratch: Get the maxbin2 container, there are a few places to get this, but will get the bioconda container as it is more recent than the one referenced on the official maxbin site. module load module load singularity singularity pull docker://quay.io/biocontainers/maxbin2:2.2.6--h14c3975_0 mv maxbin2_2.2.6--h14c3975_0.sif maxbin2_2.2.6.sif #rename for convenience Download some test data mkdir rawdata curl https://downloads.jbei.org/data/microbial_communities/MaxBin/getfile.php?20x.scaffold > rawdata/20x.scaffold curl https://downloads.jbei.org/data/microbial_communities/MaxBin/getfile.php?20x.abund > rawdata/20x.abund Create an output data location mkdir output Create a submit script using singularity on the cluster singularity_submit.sh #!/bin/bash #SBATCH --job-name=maxbin2_test #SBATCH -o sing_test.out #SBATCH -e sing_test.err #SBATCH --time=00:10:00 #SBATCH --partition=parallel #SBATCH --ntasks=4 #SBATCH --mem=4G module load singularity singularity exec maxbin2_2.2.6.sif run_MaxBin.pl -contig rawdata/20x.scaffold -abund rawdata/20x.abund -out output/20x.out -thread 4 Singularity/Sandbox Example \u00b6 This lets you have root inside a container locally and make changes to it. This is really handy for determining how to setuop your container. While you can convert the sandbox container to one you can run on R\u0101poi, I suggest you don't do this . Use the sandbox to figure out how you need to configure your container, what packages to install, config files to change etc. Then create a .def file that contains all the nessesary steps without the need to use the sandbox - this will make your work more reproducable and easier to share with others. example.def BootStrap: library From: ubuntu:16.04 %post apt-get update && apt-get -y install wget build-essential %runscript exec echo \" $@ \" %labels Author Andre Compile this locally with sudo and singularity. We are using the sandbox flag to create a writable container directory ( example/ ) on our local machine where we have sudo rights. sudo singularity build --sandbox example/ example.def Now we can run the container we just built, but with sudo rights inside the container. Your rights outside the container match the rights inside the container, so we need to do this with sudo. sudo singularity shell --writable example/ Inside the container we now have root and can install packages and modify files in the root directories Singularity example:~> apt update Singularity example:~> apt install sqlite Singularity example:~> touch /test.txt #create an empty file in root Singularity example:~> ls / Singularity example:~> exit #exit container To run the container on R\u0101poi we convert it to the default immutable image with build. We might need sudo for this as the prior use of sudo will have created a directory that your usual user can't see every file. sudo singularity build new-example-sif example/ You could now copy the new-example-sif file to R\u0101poi and run it there. However a better workflow is to use this to experiment, to find out what changes you need to make to the image and what packages you need to install. Once you've done that, I suggest starting afresh and putting everything in the.def file . That way when you return to your project in 6 months, or hand it over to someone else, there is a clear record of how the image was built. Singularity/Custom Conda Container - idba example \u00b6 In this example we'll build a singularity container using conda. The example is building a container for idba - a genome assembler. Idba is available in bioconda, but not as a biocontainer. We'll build this container locally to match a local conda environment, then run it on the HPC and do an example assembly. Locally \u00b6 Make sure you have conda setup on your local machine, anaconda and miniconda are good choices. Create a new conda environment and install idba conda create --name idba conda install -c bioconda idba Export your conda environment, we will use this to build the container. conda env export > environment.yml We will use a singularity definition, basing our build on a docker miniconda image. There is a bunch of stuff in this file to make sure the conda environment is in the path. From stackoverflow idba.def Bootstrap: docker From: continuumio/miniconda3 %files environment.yml %environment PATH=/opt/conda/envs/$(head -1 environment.yml | cut -d' ' -f2)/bin:$PATH %post echo \". /opt/conda/etc/profile.d/conda.sh\" >> ~/.bashrc echo \"source activate $(head -1 environment.yml | cut -d' ' -f2)\" > ~/.bashrc /opt/conda/bin/conda env create -f environment.yml %runscript exec \"$@\" Build the image sudo singularity build idba.img idba.def Now copy the idba.img and environment.yml (technically the environment file is not needed, but not having it creates a warning) to somewhere sensible on R\u0101poi. On R\u0101poi \u00b6 Create a data directory, so we can separate our inputs and outputs. Download a paired end illumina read of Ecoli from S3 with wget. The data comes from the Illumina public data library mkdir data cd data wget --content-disposition goo.gl/JDJTaz #sequence data wget --content-disposition goo.gl/tt9fsn #sequence data cd .. #back to our project directory The reads we have are paired end fastq files but idba requires a fasta file. We can use a tool built into our container to convert them. We'll do this on the R\u0101poi login node as it is a fast task that doesn't need many resources. module load singularity singularity exec fq2fa --merge --filter data/MiSeq_Ecoli_MG1655_50x_R1.fastq data/MiSeq_Ecoli_MG1655_50x_R2.fastq data/read.fa Create our sbatch submission script. Note that this sequence doesn't need a lot of memory, so we'll use 1G. To see your usage after the job has run use vuw-job-report <job-id> idba_submit.sh #!/bin/bash #SBATCH --job-name=idba_test #SBATCH -o output.out #SBATCH -e output.err #SBATCH --time=00:10:00 #SBATCH --partition=quicktest #SBATCH --ntasks=12 #SBATCH --mem=1G module load singularity singularity exec idba.img idba idba_ud -r data/read.fa -o output Now we can submit our script to the queue with sbatch idba_submit.sh","title":"Examples"},{"location":"examples/#examples","text":"","title":"Examples"},{"location":"examples/#simple-bash-example-start-here-if-new-to-hpc","text":"In this example we will run a very simple bash script on the quicktest partition. The bash script is very simple, it just prints the hostname - the node you're running on - and prints the date into a file. It also sleeps for 1 minute - it just does this to give you a chance to see your job in the queue with squeue First lets create a sensible working directory mkdir bash_example cd bash_example We'll use the text editor nano to create our bash script as well as our submission script. In real life, you might find it easier to create your code and submission script on your local machine, then copy them over as nano is not a great editor for large projects. Create and edit our simple bash script - this is our code we will run on the HPC nano test.sh Paste or type the following into the file #!/bin/bash hostname #prints the host name to the terminal date > date_when_job_ran.txt #puts the content of the date command into a txt file sleep 1m # do nothing for 1 minute. Job will still be \"running\" press ctrl-O to save the text in nano, then ctrl-X to exit nano. Using nano again create a file called submit.sh with the following content #!/bin/bash # #SBATCH --job-name=bash_test #SBATCH -o bash_test.out #SBATCH -e bash_test.err # #SBATCH --partition=quicktest # #SBATCH --cpus-per-task=2 #Note: you are always allocated an even number of cpus #SBATCH --mem=1G #SBATCH --time=10:00 bash test.sh #actually run our bash script, using bash If you're familiar with bash scripts, the above is a bit weird. The #SBATCH lines would normally be comments and hence not do anything, but Slurm will read those lines to determine how many resources to provide your job. In this case we ask for the following: quicktest partition (the default - so you don't technically need to ask for it). 1 cpu per task - we have one task, so we're asking for 1 cpu 1 gig of memory. a max runtime of 10 min If your job uses more memory or time than requested, Slurm will immediately kill it. If you use more CPU's than requested - your job will keep running, but your \"cpus\" will be shared bewteen the CPUs you actually requested. So if your job tried to use 10 CPUs but you only asked for one, it'll run extremely slowly - don't do this. Our submit.sh script also names our job bash_test this is what the job will show up as in squeue. We ask for things printed out on the terminal to go to two seperate files. Normal, non error, things that would be printed out on the terminal will be put into the text file bash_test.out . Errors will be printed into the text file bash_test.err Now submit your job to the Slurm queue. sbatch submit.sh #See your job in the queue squeue -u <your_username> #When job is done see the new files ls #look at the content that would have been printed to the terminal if running locally cat bash_test.out # See the content of the file that your bash script created cat date_when_job_ran.txt","title":"Simple Bash Example - start here if new to HPC"},{"location":"examples/#simple-python-program-using-virtualenv-and-pip","text":"First we need to create a working directory and move there mkdir python_test cd python_test Next we load the python 3 module and use python 3 to create a python virtualenv. This way we can install pip packages which are not installed on the cluster module load python/3.6.6 python3 -m venv mytest Activate the mytest virtualenv and use pip to install the webcolors package source mytest/bin/activate pip install webcolors Create the file test.py with the following contents using nano import webcolors from random import randint from socket import gethostname colour_list = list ( webcolors . CSS3_HEX_TO_NAMES . items ()) requested_colour = randint ( 0 , len ( colour_list )) colour_name = colour_list [ requested_colour ][ 1 ] print ( \"Random colour name:\" , colour_name , \" on host: \" , gethostname ()) Alternatively download it with wget: wget https://raw.githubusercontent.com/ \\ vuw-research-computing/raapoi-tools/ \\ master/examples/python_venv/test.py Using nano create the submissions script called python_submit.sh with the following content - change me@email.com to your email address. #!/bin/bash # #SBATCH --job-name=python_test #SBATCH -o python_test.out #SBATCH -e python_test.err # #SBATCH --cpus-per-task=2 #Note: you are always allocated an even number of cpus #SBATCH --mem=1G #SBATCH --time=10:00 # #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=me@email.com module load python/3.6.6 source mytest/bin/activate python test.py Alternatively download it with wget wget https://raw.githubusercontent.com/ \\ vuw-research-computing/raapoi-tools/ \\ master/examples/python_venv/python_submit.sh To submit your job to the Slurm scheduler sbatch python_submit.sh Check for your job on the queue with squeue though it might finish very fast. The output files will appear in your working directory.","title":"Simple Python program using virtualenv and pip"},{"location":"examples/#using-anacondaminicondaconda-idba","text":"Many users use Anaconda/Miniconda to manage software stacks. One way to do this is to use singularity containers with the conda environment inside - this allows the conda environment to load quickly as the many small conda files are inside a container which the file system sees as one file. However, this is also an additional bit of complexity so many users just use conda outside of singularity. You can install your own version of Anaconda/Miniconda to your home directory or scratch. We have also got packaged versions of Anaconda/Miniconda installed with our module loading system. Anaconda has many built in packages so we will use that in our examples, but Miniconda is available if you want a more minimal initial setup. module load old-mod-system/Anaconda3/2020.11 Let's create a new conda environment for this example, in a sensible location, I used ~/examples/conda/idba conda create --name idba-example # press y for the Proceed prompt if it looks correct conda activate idba-example #activate our example environment. Conda environments are beyond the scope of this example, but they are a good way to contain all the dependencies and programs for a particular workflow, in this case, idba. Install idba in our conda environment. ** Note that best practise is to do the install on a compute node ** We'll just do it here on the login node for now - the code will run slower on the compute nodes as a result! conda install -c bioconda idba Idba is a genome assembler, we will use paired-end illumina reads of E. coli. The data is available on an Amazon S3 bucket (a cloud storage location), and we can download it using wget. mkdir data # put our data in a sensible location cd data wget --content-disposition goo.gl/JDJTaz #sequence data wget --content-disposition goo.gl/tt9fsn #sequence data cd .. #back to our project directory The reads we have are paired-end fastq files but idba requires a fasta file. We can use a tool installed with idba to convert them. We'll do this on the R\u0101poi login node as it is a fast task that doesn't need many resources. fq2fa --merge --filter data/MiSeq_Ecoli_MG1655_50x_R1.fastq data/MiSeq_Ecoli_MG1655_50x_R2.fastq data/read.fa To create our submission script we need to know the path to our conda enviroment. To get this: conda env list You'll need to find your idba-example environment, and next to it is the path you'll need for your submission script. In my case: # conda environments: # base * /home/andre/anaconda3 idba-example /home/andre/anaconda3/envs/idba-example # We need this line, it'll be different for you! Create our sbatch submission script. Note that this sequence doesn't need a lot of memory, so we'll use 3G. To see your usage after the job has run use vuw-job-report <job-id> idba_submit.sh #!/bin/bash #SBATCH --job-name=idba_test #SBATCH -o _output.out #SBATCH -e _output.err #SBATCH --time=00:5:00 #SBATCH --partition=quicktest #SBATCH --ntasks=12 #SBATCH --mem=3G module load old-mod-system/Anaconda3/2020.11 eval \" $( conda shell.bash hook ) \" # basically inits your conda - prevents errors like: CommandNotFoundError: Your shell has not been properly configured ... conda activate /home/andre/anaconda3/envs/idba-example # We will need to activate our conda enviroment on the remote node idba idba_ud -r data/read.fa -o output To submit our job sbatch idba_submit.sh To see our job running or queuing squeue -u $USER This job will take a few minutes to run, generally less than 5. When the job is done we can see the output in the output folder. We can also see the std output and std err in the files _output.out and _output.err . The quickest way to examine them is to cat the files when the run is done. cat _output.out","title":"Using Anaconda/Miniconda/conda - idba"},{"location":"examples/#loading-r-packages-running-a-simple-job","text":"First login to R\u0101poi and load the R and R/CRAN modules: module load R/4.0.2 module load R/CRAN Then run R on the command line: R Test library existence: > library ( tidyverse ) This should load the package, and give some output like this: \u2500\u2500 Attaching packages \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse 1.3 . 0 \u2500\u2500 \u2714 ggplot2 3.3 . 2 \u2714 purrr 0.3 . 4 \u2714 tibble 3.0 . 1 \u2714 dplyr 1.0 . 0 \u2714 tidyr 1.1 . 0 \u2714 stringr 1.4 . 0 \u2714 readr 1.3 . 1 \u2714 forcats 0.5 . 0 \u2500\u2500 Conflicts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse_conflicts () \u2500\u2500 \u2716 dplyr :: filter () masks stats :: filter () \u2716 dplyr :: lag () masks stats :: lag () (These conflicts are normal and can be ignored.) To quit R, type: > q () Next create a bash submission script called r_submit.sh (or another name of your choice) using your preferred text editor, e.g. nano. #!/bin/bash # #SBATCH --job-name=r_test #SBATCH -o r_test.out #SBATCH -e r_test.err # #SBATCH --cpus-per-task=2 #Note: you are always allocated an even number of cpus #SBATCH --mem=1G #SBATCH --time=10:00 # module load R/4.0.2 module load R/CRAN Rscript mytest.R Save this to the current working directory, and then create another file using your preferred text editor called mytest.R (or another name of your choice) containing the following R commands: library ( tidyverse ) sprintf ( \"Hello World!\" ) then run it with the previously written bash script: sbatch r_submit.sh This submits a task that should execute quickly and create files in the directory from which it was run. Examine r_test.out . You can use an editor like nano, vi or emacs, or you can just cat or less the file to see its contents on the terminal. You should see: \"Hello World\"","title":"Loading R packages &amp; running a simple job"},{"location":"examples/#matlab-gpu-example","text":"Matlab has various built-in routines which are GPU accelerated. We will run a simple speed comparison between cpu and gpu tasks. In a sensible location create a file called matlab_gpu.m I used ~/examples/matlab/cuda/matlab_gpu.m . % Set an array which will calculate the Eigenvalues of A = rand ( 1000 ); % Copy the Array to the GPU memory - this process takes an erratic amount of time, so we will not time it. Agpu = gpuArray ( A ); tic B=eig(Agpu) ; t1 = toc % Let's compare the time with CPU tic B = eig ( A ); t2 = toc We will also need a Slurm submission script; we'll call this matlab_gpu.sh . Note that we will need to use the new Easybuild module files for our cuda libraries, so make sure to include the module use line module use /home/software/tools/eb_modulefiles/all/Core #!/bin/bash #SBATCH --job-name=matlab-gpu-example #SBATCH --output=out-gpu-example.out #SBATCH --error=out-gpu-example.err #SBATCH --time=00:05:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module use /home/software/tools/eb_modulefiles/all/Core module load matlab/2021a module load fosscuda/2020b matlab -nodisplay -nosplash -nodesktop -r \"run('matlab_gpu.m');exit;\" To submit this job to the Slurm queue sbatch matlab_gpu.sh . This job will take a few minutes to run - this is mostly the Matlab startup time. Examine the queue for your job squeue -u $USER . When your job is done, inspect the output file. You can use an editor like nano, vi or emacs, or you can just cat or less the file to see its contents on the terminal. cat out-gpu-example.out What do you notice about the output? Surely GPUs should be faster than the CPU! It takes time for the GPU to start processing your task, the CPU is able to start the task far more quickly. So for short operations, the CPU can be faster than the GPU - remember to benchmark your code for optimal performance! Just because you can use a GPU for your task doesn't mean it is necessarily faster! To get a better idea of the advantage of the GPU let's increase the size of the array from 1000 to 10000 matlab_gpu.m % Set an array which will calculate the Eigenvalues of A = rand ( 10000 ); % Copy the Array to the GPU memory - this process takes an erratic amount of time, so we will not time it. Agpu = gpuArray ( A ); tic B=eig(Agpu) ; t1 = toc % Let's compare the time with CPU tic B = eig ( A ); t2 = toc To make things fairer for the CPU in this case, we will also allocate half the CPUs on the node to Matlab. Half the CPUs, half the memory and half the GPUs, just to be fair. matlab_gpu.sh #!/bin/bash #SBATCH --job-name=matlab-gpu-example #SBATCH --output=out-gpu-example.out #SBATCH --error=out-gpu-example.err #SBATCH --time=00:05:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=128 #SBATCH --mem=256G module use /home/software/tools/eb_modulefiles/all/Core module load matlab/2021a module load fosscuda/2020b matlab -nodisplay -nosplash -nodesktop -r \"run('matlab_gpu.m');exit;\" The output in my case was: < M A T L A B ( R ) > Copyright 1984 -2021 The MathWorks, Inc. R2021a Update 1 ( 9 .10.0.1649659 ) 64 -bit ( glnxa64 ) April 13 , 2021 To get started, type doc. For product information, visit www.mathworks.com. t1 = 62 .0212 t2 = 223 .0818 So in thise case the GPU was considerably faster. Matlab can do this a bit faster on the CPU if you give it fewer CPUs, the optimum appears to be around 20, but it still takes 177s. Again, optimise your resource requests for your problem, less can sometimes be more, however the GPU easily wins in this case.","title":"Matlab GPU example"},{"location":"examples/#job-arrays-running-many-similar-jobs","text":"Slurm makes it easy to run many jobs which are similar to each other. This could be one piece of code running over many datasets in parallel or running a set of simulations with a different set of parameters for each run.","title":"Job Arrays - running many similar jobs"},{"location":"examples/#simple-bash-job-array-example","text":"The following code will run the submission script 16 times as resources become available (i.e. they will not neccesarily run at the same time). It will just print out the Slurm array task ID and exit. submit.sh: #!/bin/bash #SBATCH --job-name=test_array #SBATCH --output=out_array_%A_%a.out #SBATCH --error=out_array_%A_%a.err #SBATCH --array=1-16 #SBATCH --time=00:00:20 #SBATCH --partition=parallel #SBATCH --ntasks=1 #SBATCH --mem=1G # Print the task id. echo \"My SLURM_ARRAY_TASK_ID: \" $SLURM_ARRAY_TASK_ID # Add lines here to run your computations. Run the example with the standard sbatch submit.sh","title":"Simple Bash Job Array example"},{"location":"examples/#a-simple-r-job-array-example","text":"As a slightly more practical example the following will run an R script 5 times as resources become available. The R script takes as an input the $SLURM_ARRAY_TASK_ID which then selects a parameter alpha out of a lookup table. This is one way you could run simulations or similar with a set parameters defined in a lookuop table in your code. To make outputs more tidy and to help organisation, instead of dumping all the outputs into the directory with our code and submission script, we will separate the outputs into directories. Dataframes saved from R will be saved to the output/ directory, and all output which would otherwise be printed to the commnd line (stdout and stderr) will be saved to the stdout/ directory. Both of these directories will need to be created before running the script. r_random_alpha.R: # get the arguments supplied to R. # trailingOnly = TRUE gets the user supplied # arguments, and for now we will only get the # first user supplied argument args <- commandArgs ( trailingOnly = TRUE ) inputparam <- args [ 1 ] # a vector with all our parameters. alpha_vec <- c ( 2.5 , 3.3 , 5.1 , 8.2 , 10.9 ) alpha <- alpha_vec [ as.integer ( inputparam )] # Generate a random number between 0 and alpha # store it in dataframe with the coresponding # alpha value randomnum <- runif ( 1 , min = 0 , max = as.double ( alpha )) df <- data.frame ( \"alpha\" = alpha , \"random_num\" = randomnum ) # Save the data frame to a file with the alpha value # Note that the output/ folder will need to be # manually created first! outputname <- paste ( \"output/\" , \"alpha_\" , alpha , \".Rda\" , sep = \"\" ) save ( df , file = outputname ) Next create the submision script. Which we will run on the parallel partition rather than quicktest. r_submit.sh: #!/bin/bash #SBATCH --job-name=test_R_array #SBATCH --output=stdout/array_%A_%a.out #SBATCH --error=stdout/array_%A_%a.err #SBATCH --array=1-5 #SBATCH --time=00:00:20 #SBATCH --partition=parallel #SBATCH --ntasks=1 #SBATCH --mem=1G module load R/CRAN # Print the task id. Rscript r_random_alpha.R $SLURM_ARRAY_TASK_ID Run the jobs with sbatch r_submit.sh","title":"A simple R job Array Example"},{"location":"examples/#singularity","text":"While there are many modules on R\u0101poi, sometimes you might want to install your own packages in your own way. Singularity allows you to do this. If you are familiar with Docker, Singularity is similar, except you can't get root (or sudo) once your container is running on the R\u0101poi. However, you can have sudo rights locally on your own machine, setup your container however you like, then run it without sudo on the cluster.","title":"Singularity"},{"location":"examples/#singularitydocker-container-example","text":"Singularity allows you to use most (but not all!) docker images on R\u0101poi. On your local machine create the singularity definition file input_args_example.def BootStrap : library From : ubuntu: 16.04 %runscript exec echo \" $@ \" %labels Author Andre This will build an ubuntu 16.04 container that will eventually run on R\u0101poi which runs Centos. This container has a runscript which just echos back any arguments sent to the container when your start it up. Build the container locally with sudo and singularity sudo singularity build inputexample.sif input_args_example.def This will build an image that you can't modify any further and is immediately suitable to run on R\u0101poi Copy this file to R\u0101poi via sftp sftp <username>@raapoi.vuw.ac.nz Create a submit script using singularity on the cluster singularity_submit.sh #!/bin/bash #SBATCH --job-name=singularity_test #SBATCH -o sing_test.out #SBATCH -e sing_test.err #SBATCH --time=00:00:20 #SBATCH --ntasks=1 #SBATCH --mem=1G module load singularity singularity run inputtest.sif \"hello from a container\" Run the script with the usual singularity_submit.sh","title":"Singularity/Docker container example"},{"location":"examples/#singularitytensorflow-example","text":"tensor.def Bootstrap: docker From: tensorflow/tensorflow:latest-py3 %post apt-get update && apt-get -y install wget build-essential %runscript exec python \" $@ \" compile this locally with sudo and singularity. sudo singularity build tensorflow.sif tensor.def Create a quick tensorflow test code tensortest.py import tensorflow as tf mnist = tf . keras . datasets . mnist ( x_train , y_train ),( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 512 , activation = tf . nn . relu ), tf . keras . layers . Dropout ( 0.2 ), tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax ) ]) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( x_train , y_train , epochs = 5 ) model . evaluate ( x_test , y_test ) Copy your files to R\u0101poi via sftp (or whatever you prefer) sftp <username>@raapoi.vuw.ac.nz cd <where you want to work> put * #put all files in your local directory onto R\u0101poi Lets quickly test the code via an interactive session on a node. Note I find the tensorflow container only runs properly on intel nodes, which we don't have many of at the moment, I'll investigate this further. srun --partition = \"parallel\" --constraint = \"Intel\" --pty bash #now on the remote node - note you might need to wait if nodes are busy module load singularity #load singularity singularity shell tensorflow.sif #now inside the tensorflow container on the remote node python tensortest.py #once that runs, exit the container exit #exit the container exit #exit the interactive session on the node Create a submit script using singularity on the cluster singularity_submit.sh #!/bin/bash #SBATCH --job-name=singularity_test #SBATCH -o sing_test.out #SBATCH -e sing_test.err #SBATCH --time=00:10:00 #SBATCH --partition=parallel #SBATCH --constraint=Intel #SBATCH --ntasks=1 #SBATCH --mem=4G module load singularity #run the container with the runscript defined when we created it singularity run tensorflow.sif tensortest.py","title":"Singularity/TensorFlow Example"},{"location":"examples/#singularitymaxbin2-example","text":"In a sensible location, either in your home directory or on the scratch: Get the maxbin2 container, there are a few places to get this, but will get the bioconda container as it is more recent than the one referenced on the official maxbin site. module load module load singularity singularity pull docker://quay.io/biocontainers/maxbin2:2.2.6--h14c3975_0 mv maxbin2_2.2.6--h14c3975_0.sif maxbin2_2.2.6.sif #rename for convenience Download some test data mkdir rawdata curl https://downloads.jbei.org/data/microbial_communities/MaxBin/getfile.php?20x.scaffold > rawdata/20x.scaffold curl https://downloads.jbei.org/data/microbial_communities/MaxBin/getfile.php?20x.abund > rawdata/20x.abund Create an output data location mkdir output Create a submit script using singularity on the cluster singularity_submit.sh #!/bin/bash #SBATCH --job-name=maxbin2_test #SBATCH -o sing_test.out #SBATCH -e sing_test.err #SBATCH --time=00:10:00 #SBATCH --partition=parallel #SBATCH --ntasks=4 #SBATCH --mem=4G module load singularity singularity exec maxbin2_2.2.6.sif run_MaxBin.pl -contig rawdata/20x.scaffold -abund rawdata/20x.abund -out output/20x.out -thread 4","title":"Singularity/MaxBin2 Example"},{"location":"examples/#singularitysandbox-example","text":"This lets you have root inside a container locally and make changes to it. This is really handy for determining how to setuop your container. While you can convert the sandbox container to one you can run on R\u0101poi, I suggest you don't do this . Use the sandbox to figure out how you need to configure your container, what packages to install, config files to change etc. Then create a .def file that contains all the nessesary steps without the need to use the sandbox - this will make your work more reproducable and easier to share with others. example.def BootStrap: library From: ubuntu:16.04 %post apt-get update && apt-get -y install wget build-essential %runscript exec echo \" $@ \" %labels Author Andre Compile this locally with sudo and singularity. We are using the sandbox flag to create a writable container directory ( example/ ) on our local machine where we have sudo rights. sudo singularity build --sandbox example/ example.def Now we can run the container we just built, but with sudo rights inside the container. Your rights outside the container match the rights inside the container, so we need to do this with sudo. sudo singularity shell --writable example/ Inside the container we now have root and can install packages and modify files in the root directories Singularity example:~> apt update Singularity example:~> apt install sqlite Singularity example:~> touch /test.txt #create an empty file in root Singularity example:~> ls / Singularity example:~> exit #exit container To run the container on R\u0101poi we convert it to the default immutable image with build. We might need sudo for this as the prior use of sudo will have created a directory that your usual user can't see every file. sudo singularity build new-example-sif example/ You could now copy the new-example-sif file to R\u0101poi and run it there. However a better workflow is to use this to experiment, to find out what changes you need to make to the image and what packages you need to install. Once you've done that, I suggest starting afresh and putting everything in the.def file . That way when you return to your project in 6 months, or hand it over to someone else, there is a clear record of how the image was built.","title":"Singularity/Sandbox Example"},{"location":"examples/#singularitycustom-conda-container-idba-example","text":"In this example we'll build a singularity container using conda. The example is building a container for idba - a genome assembler. Idba is available in bioconda, but not as a biocontainer. We'll build this container locally to match a local conda environment, then run it on the HPC and do an example assembly.","title":"Singularity/Custom Conda Container - idba example"},{"location":"faq/","text":"Frequently Asked Questions \u00b6 I don't want to interfere with other people work, what does it mean \"Other users are prevented from using resources you request, even if you don't use them\"? The system is shared, you will very rarely have full use of a node. You need to be careful to request resources, leaving the extra space available to others. For example, say you submitted a job to bigmem which asked for 800GB of ram and 10 CPUs. Your job would end up on the node with 1000GB ram as only that one would fit it - if your job actually only used 300GB of ram - the extra 500GB of ram you requested would be \"wasted\" no one else could use it. So, another user with a job requesting 600GB of ram would have to wait for your job to end even if there was space for it to run alongside yours. The same issue occurs with CPU requests. It can be very hard to accurately estimate memory and cpu needs before running your job. If your job has a short run time (less than ~10 hours), you can just request more than you need and check the memory usage afterward to guide further jobs. If your job has a long run time (several days), you should run a test job with a short runtime (a few hours) to estimate your needs first. Allocating tasks, threads and MPI processes - or how to parse the meanings of ntasks , cpus-per-task and nodes in my sbatch scripts From the C.E.C.I hpc docs you use mpi and do not care about where those cores are distributed: --ntasks=16 you want to launch 16 independent processes (no communication): --ntasks=16 you want those cores to spread across distinct nodes: --ntasks=16 --ntasks-per-node=1 or --ntasks=16 --nodes=16 you want 16 processes to spread across 8 nodes to have two processes per node: --ntasks=16 --ntasks-per-node=2 you want 16 processes to stay on the same node: --ntasks=16 --ntasks-per-node=16 you want one process that can use 16 cores for multithreading: - -ntasks=1 --cpus-per-task=16 you want 4 processes that can use 4 cores each for multithreading: --ntasks=4 --cpus-per-task=4","title":"FAQ"},{"location":"faq/#frequently-asked-questions","text":"I don't want to interfere with other people work, what does it mean \"Other users are prevented from using resources you request, even if you don't use them\"? The system is shared, you will very rarely have full use of a node. You need to be careful to request resources, leaving the extra space available to others. For example, say you submitted a job to bigmem which asked for 800GB of ram and 10 CPUs. Your job would end up on the node with 1000GB ram as only that one would fit it - if your job actually only used 300GB of ram - the extra 500GB of ram you requested would be \"wasted\" no one else could use it. So, another user with a job requesting 600GB of ram would have to wait for your job to end even if there was space for it to run alongside yours. The same issue occurs with CPU requests. It can be very hard to accurately estimate memory and cpu needs before running your job. If your job has a short run time (less than ~10 hours), you can just request more than you need and check the memory usage afterward to guide further jobs. If your job has a long run time (several days), you should run a test job with a short runtime (a few hours) to estimate your needs first. Allocating tasks, threads and MPI processes - or how to parse the meanings of ntasks , cpus-per-task and nodes in my sbatch scripts From the C.E.C.I hpc docs you use mpi and do not care about where those cores are distributed: --ntasks=16 you want to launch 16 independent processes (no communication): --ntasks=16 you want those cores to spread across distinct nodes: --ntasks=16 --ntasks-per-node=1 or --ntasks=16 --nodes=16 you want 16 processes to spread across 8 nodes to have two processes per node: --ntasks=16 --ntasks-per-node=2 you want 16 processes to stay on the same node: --ntasks=16 --ntasks-per-node=16 you want one process that can use 16 cores for multithreading: - -ntasks=1 --cpus-per-task=16 you want 4 processes that can use 4 cores each for multithreading: --ntasks=4 --cpus-per-task=4","title":"Frequently Asked Questions"},{"location":"ganglia/","text":"Visit here to get real-time metrics and history of R\u0101poi's utilisation. Ganglia example:","title":"Ganglia"},{"location":"hpclayout/","text":"HPC layout \u00b6 Note! Understanding the R\u0101poi hardware layout is not critical for most users! It is useful for users running big parallel MPI jobs and may be of interest to others. To a first approximation a High Performance Computer (HPC) is a collection of large computers or servers (nodes) that are connected together. There will also be some attached storage. Rather than logging into the system and immediately running your program or code, it is organised into a job and submitted to a scheduler that takes your job and runs it on one of the nodes that has enough free resources (cpu and memory) to meet your job request. Most of the time you will be sharing a node with other users. It is important to try and not over request resources as requested resources are kept in reserve for you and not available to others, even if you don't use them. This is particularly important when requesting a lot of resources or running array jobs which can use up a lot of the HPCs resources. Hardware \u00b6 On R\u0101poi, the node you login into and submit your jobs to is called raapoi-master . The computers/servers making up the nodes are of several types, covered in partitions . Most of the processors in R\u0101poi are in the parallel AMD nodes such as AMD01n01, AMD01n02 etc. Figures 1-4 show more details of these nodes. Figure 1: Example of some of the computers making up R\u0101poi. This is the front panel in Rack 4 in the Datacentre - highlighted is one of the AMD chassis, which have 4 nodes each. Figure 2: Example of some of the computers making up R\u0101poi. This is the back in Rack 4 in the Datacentre. Here you can clearly see the 4 nodes in each chassis of the parallel partition Figure 3: An AMD compute node, one of 4 in a chassis. The 2 black rectangles are the processor heatsinks, on each side are the ram modules. Each ram module is 32GB for a total of 512GB. On the lower left, the green circuit board is the the InfiniBand network card. Opposite that, in black, is the 1.7TB NvMe storage we use as fast /tmp space. Figure 4: One of the CPUs with the heatsink removed. At 115.00 x 165.00 mm, it is physically much larger than the processor in a desktop Each AMD node has 2 of these 7702 processors. Each processor has 64Cores/128Threads (with SMT - symmetric multi-threading - enabled) for a total of 128Cores/256Threads per node. Network \u00b6 On R\u0101poi the nodes are connected to each other in two ways - via 10G ethernet and via 52G infiniband. Most of the time you can ignore this, but it is important for interconnected jobs running across multiple nodes like weather simulations. In figure 5 we can see the network layout of R\u0101poi from the perspective of the Login node. This is the node you ssh into, via the VUW intranet - either from a locally wired connection or via the VPN. The nodes are organised into groups mostly aligning with the partition the node is in. Ethernet \u00b6 The dashed lines indicate an Ethernet connection, all nodes are connected via ethernet at either 1G or 10G depending on the node. Most of the intel nodes are only connected at 1G due to their age. The newer nodes are all 10G connected. The ethernet connection can also reach out into the wider internet for downloading updates, datasets etc. Infiniband \u00b6 Many nodes are also connected by a solid line indicating an Infiniband network connection. This connection is faster than the ethernet connection but more importantly lower latency than the ethernet connection. This helps with large interconnected (eg MPI) jobs running across multiple nodes. The latency of the interprocess communication carried over the Infiniband link can have a dramatic affect on large scale calculations which for instance need to communicate grid boundary conditions across the nodes Where infiniband is available, the scratch storage and BeeGFS storage is transmitted over the link as the latency helps with IO performance. classDiagram Parallel_AMD -- Login Parallel_AMD .. Login Quicktest -- Login Quicktest .. Login Highmem .. Login GPU .. Login Login .. Internet Login .. Scratch Login -- Scratch Login .. BeeGFS Login -- BeeGFS class Internet { vuw intranet wider internet } class Scratch { 100 TB raapoi_fs01 } class BeeGFS { 100TB across Bee01 Bee02 Bee03 } class Login { raapoi-master } class Parallel_AMD { amd01n01 amd01n02 amd01n03 amd01n04 - amd02n01 amd02n02 amd02n03 amd02n04 - amd03n01 amd03n02 amd03n03 amd03n04 - amd04n01 amd04n02 amd04n03 amd04n04 - amd05n01 amd05n02 amd05n03 amd05n04 - amd06n01 amd06n02 amd06n03 amd06n04 - amd07n01 amd07n02 amd07n03 amd07n04 } class Quicktest{ itl02n01 itl02n02 itl02n03 itl02n04 - itl03n01 itl03n02 } class Highmem{ high01 high02 high03 high04 } class GPU{ gpu01 gpu02 gpu03 } Figure 5: Logical HPC layout from the perspective of the login node. Solid lines indicate ethernet connections, dashed Infiniband Looking at the HPC from the perspective of the ethernet and infiniband networks. The nodes in Figure 6 and 7 are the same as before, but we're just using the group container label to simplify the diagram. classDiagram Parallel_AMD .. Ethernet Quicktest .. Ethernet Highmem .. Ethernet GPU .. Ethernet Ethernet .. Internet Ethernet .. Login Ethernet .. Scratch Ethernet .. BeeGFS class Ethernet{ 1-10Gig Connects to the wider internet } class Scratch { } class BeeGFS { } class Login { } class Parallel_AMD { } class Quicktest{ } class Highmem{ } class GPU{ } Figure 6: Logical HPC layout from the perspective of the ethernet connections. Node layout is the same as in Figure 5, but only the group headings have been retained. classDiagram Parallel_AMD -- Infiniband Quicktest -- Infiniband Infiniband -- Login Infiniband -- Scratch Infiniband -- BeeGFS class Infiniband{ 56Gb/s Low latency } class Scratch { } class BeeGFS { } class Login { } class Parallel_AMD { } class Quicktest{ } class Highmem{ } class GPU{ } Figure 7: Logical HPC layout from the perspective of the Infiniband connections. Note that not all nodes are connected via the infiniband link! Node layout is the same as in Figure 5, but only the group headings have been retained. The Infiniband nodes are connected to one of two SX6036 Infiniband switches. The intel and quicktest and login nodes are connected to one switch. Everything else is connected to the the other. The switches are broadly interconnected, but there is as small latency penalty for crossing the switch.","title":"HPC Hardware Layout"},{"location":"hpclayout/#hpc-layout","text":"Note! Understanding the R\u0101poi hardware layout is not critical for most users! It is useful for users running big parallel MPI jobs and may be of interest to others. To a first approximation a High Performance Computer (HPC) is a collection of large computers or servers (nodes) that are connected together. There will also be some attached storage. Rather than logging into the system and immediately running your program or code, it is organised into a job and submitted to a scheduler that takes your job and runs it on one of the nodes that has enough free resources (cpu and memory) to meet your job request. Most of the time you will be sharing a node with other users. It is important to try and not over request resources as requested resources are kept in reserve for you and not available to others, even if you don't use them. This is particularly important when requesting a lot of resources or running array jobs which can use up a lot of the HPCs resources.","title":"HPC layout"},{"location":"hpclayout/#hardware","text":"On R\u0101poi, the node you login into and submit your jobs to is called raapoi-master . The computers/servers making up the nodes are of several types, covered in partitions . Most of the processors in R\u0101poi are in the parallel AMD nodes such as AMD01n01, AMD01n02 etc. Figures 1-4 show more details of these nodes. Figure 1: Example of some of the computers making up R\u0101poi. This is the front panel in Rack 4 in the Datacentre - highlighted is one of the AMD chassis, which have 4 nodes each. Figure 2: Example of some of the computers making up R\u0101poi. This is the back in Rack 4 in the Datacentre. Here you can clearly see the 4 nodes in each chassis of the parallel partition Figure 3: An AMD compute node, one of 4 in a chassis. The 2 black rectangles are the processor heatsinks, on each side are the ram modules. Each ram module is 32GB for a total of 512GB. On the lower left, the green circuit board is the the InfiniBand network card. Opposite that, in black, is the 1.7TB NvMe storage we use as fast /tmp space. Figure 4: One of the CPUs with the heatsink removed. At 115.00 x 165.00 mm, it is physically much larger than the processor in a desktop Each AMD node has 2 of these 7702 processors. Each processor has 64Cores/128Threads (with SMT - symmetric multi-threading - enabled) for a total of 128Cores/256Threads per node.","title":"Hardware"},{"location":"hpclayout/#network","text":"On R\u0101poi the nodes are connected to each other in two ways - via 10G ethernet and via 52G infiniband. Most of the time you can ignore this, but it is important for interconnected jobs running across multiple nodes like weather simulations. In figure 5 we can see the network layout of R\u0101poi from the perspective of the Login node. This is the node you ssh into, via the VUW intranet - either from a locally wired connection or via the VPN. The nodes are organised into groups mostly aligning with the partition the node is in.","title":"Network"},{"location":"hpclayout/#ethernet","text":"The dashed lines indicate an Ethernet connection, all nodes are connected via ethernet at either 1G or 10G depending on the node. Most of the intel nodes are only connected at 1G due to their age. The newer nodes are all 10G connected. The ethernet connection can also reach out into the wider internet for downloading updates, datasets etc.","title":"Ethernet"},{"location":"hpclayout/#infiniband","text":"Many nodes are also connected by a solid line indicating an Infiniband network connection. This connection is faster than the ethernet connection but more importantly lower latency than the ethernet connection. This helps with large interconnected (eg MPI) jobs running across multiple nodes. The latency of the interprocess communication carried over the Infiniband link can have a dramatic affect on large scale calculations which for instance need to communicate grid boundary conditions across the nodes Where infiniband is available, the scratch storage and BeeGFS storage is transmitted over the link as the latency helps with IO performance. classDiagram Parallel_AMD -- Login Parallel_AMD .. Login Quicktest -- Login Quicktest .. Login Highmem .. Login GPU .. Login Login .. Internet Login .. Scratch Login -- Scratch Login .. BeeGFS Login -- BeeGFS class Internet { vuw intranet wider internet } class Scratch { 100 TB raapoi_fs01 } class BeeGFS { 100TB across Bee01 Bee02 Bee03 } class Login { raapoi-master } class Parallel_AMD { amd01n01 amd01n02 amd01n03 amd01n04 - amd02n01 amd02n02 amd02n03 amd02n04 - amd03n01 amd03n02 amd03n03 amd03n04 - amd04n01 amd04n02 amd04n03 amd04n04 - amd05n01 amd05n02 amd05n03 amd05n04 - amd06n01 amd06n02 amd06n03 amd06n04 - amd07n01 amd07n02 amd07n03 amd07n04 } class Quicktest{ itl02n01 itl02n02 itl02n03 itl02n04 - itl03n01 itl03n02 } class Highmem{ high01 high02 high03 high04 } class GPU{ gpu01 gpu02 gpu03 } Figure 5: Logical HPC layout from the perspective of the login node. Solid lines indicate ethernet connections, dashed Infiniband Looking at the HPC from the perspective of the ethernet and infiniband networks. The nodes in Figure 6 and 7 are the same as before, but we're just using the group container label to simplify the diagram. classDiagram Parallel_AMD .. Ethernet Quicktest .. Ethernet Highmem .. Ethernet GPU .. Ethernet Ethernet .. Internet Ethernet .. Login Ethernet .. Scratch Ethernet .. BeeGFS class Ethernet{ 1-10Gig Connects to the wider internet } class Scratch { } class BeeGFS { } class Login { } class Parallel_AMD { } class Quicktest{ } class Highmem{ } class GPU{ } Figure 6: Logical HPC layout from the perspective of the ethernet connections. Node layout is the same as in Figure 5, but only the group headings have been retained. classDiagram Parallel_AMD -- Infiniband Quicktest -- Infiniband Infiniband -- Login Infiniband -- Scratch Infiniband -- BeeGFS class Infiniband{ 56Gb/s Low latency } class Scratch { } class BeeGFS { } class Login { } class Parallel_AMD { } class Quicktest{ } class Highmem{ } class GPU{ } Figure 7: Logical HPC layout from the perspective of the Infiniband connections. Note that not all nodes are connected via the infiniband link! Node layout is the same as in Figure 5, but only the group headings have been retained. The Infiniband nodes are connected to one of two SX6036 Infiniband switches. The intel and quicktest and login nodes are connected to one switch. Everything else is connected to the the other. The switches are broadly interconnected, but there is as small latency penalty for crossing the switch.","title":"Infiniband"},{"location":"managing_jobs/","text":"Managing Jobs \u00b6 Cancelling a Job \u00b6 To cancel a job, first find the jobID, you can use the vuw-myjobs (or squeue ) command to see a list of your jobs, including jobIDs. Once you have that you can use the scancel command, eg scancel 236789 To cancel all of your jobs you can use the -u flag followed by your username: scancel -u harrelwe Viewing Job information \u00b6 Job History \u00b6 If you want to get a quick view of all the jobs completed within the last 5 days you can use the vuw-job-history command, for example: $ vuw-job-history MY JOBS WITHIN LAST 5 days JobID State JobName MaxVMSize CPUTime ------------ ---------- ---------- ---------- ---------- 2645 COMPLETED bash 00:00:22 2645.extern COMPLETED extern 0.15G 00:00:22 2645.0 COMPLETED bash 0.22G 00:00:20 2734 COMPLETED bash 00:07:40 2734.extern COMPLETED extern 0.15G 00:07:40 2734.0 COMPLETED bash 0.22G 00:07:40 Job Reports \u00b6 To view a report of your past jobs you can run vuw-job-report : $ vuw-job-report 162711 JOB REPORT FOR JOB 162711 JobName Nodes ReqMem UsedMem(GB) ReqCPUs CPUTime State Completed test-schro 1 64Gn 24 00:02.513 COMPLETED 2019-05-28T16:17:10 batch 1 64Gn 0.15G 24 00:00.210 COMPLETED 2019-05-28T16:17:10 extern 1 64Gn 0.15G 24 00:00.002 COMPLETED 2019-05-28T16:17:10 NOTE: In this example you see that I requested 64 GigaBytes of memory but only used 0.15 GB. This means that 63 GB of memory went unused, which was a waste of resources. You can also get a report of your completed jobs using the sacct command. For example if I wanted to get a report on how much memory my job used I could do the following: sacct --units=G --format=\"MaxVMSize\" -j 2156 MaxVMSize will report the maximum virtual memory (RAM plus swap space) used by my job in GigBytes ( --units=G ) -j 2156 shows the information for job ID 2156 type man sacct at a prompt in engaging to see the documentation on the sacct command Viewing jobs in the Queue \u00b6 To view your running jobs you can type vuw-myjobs eg: $ vuw-myjobs JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 7921967 quicktest bash harrelwe R 0:12 1 c03n01 As you can see I have a single job running on the node c03n01 on the quicktest partition You can see all the jobs in the queues by running the vuw-alljobs command. This will produce a very long list of jobs if the cluster is busy. Job Queuing (aka Why isn't my job running?) \u00b6 When a partition is busy, jobs will be placed in a queue. You can observe this in the vuw-myjobs and vuw-alljobs commands. The STATE of your job will be PENDING, this means it is waiting for resources or your job has been re-prioritized to allow other users access to run their jobs (this is called fair-share queueing). The resource manager will list a reason the job is pending, these reasons can include: Priority - Your job priority has been reduced to allow other users access to the cluster. If no other user with normal priority is also pending then your job will start once resources are available. Possible reasons why your priority has been lowered can include: the number of jobs you have run in the past 24-48 hours; the duration of the job and the amount of resources requested. The Slurm manager uses fair-share queuing to ensure the best use of the cluster. You can google fair-share queuing if you want to know more Resources - There are insufficient resources to start your job. Some combination of CPU, Memory, Time or other specialized resource are unavailable. Once resources are freed up your job will begin to run. Time: If you request more time than the max run-time of a partition, your job will be queued indefinitely (in other words: it will never run). Your time request must be less than or equal to the Partition Max Run-Time. Also if a special reservation is placed on the cluster, for instance prior to a scheduled maintenance, this too will reduce the available time to run your job. You can see Max Run-Time for our partitions described in this document. CAD or ITS Staff will alert all users prior to any scheduled maintenance and advise them of the time restrictions. QOSGrpCPULimit - This is a Quality of Service configuration to limit the number of CPUs per user. The QOSMax is the maximum that can be requested for any single job. If a user requests more CPUs than the QOSMax for a single job then the job will not run. If the user requests more than QOSMax in 2 or more jobs then the subsequent jobs will queue until the users running jobs complete. PartitionTimeLimit - This means you have requested more time than the maximum runtime of the partition. This document contains information about the different partitions, including max run-time. Typing vuw-partitions will also show the max run-time for the partitions available to you. ReqNodeNotAvail - 99% of the time you will receive this code if you have asked for too much time. This frequently occurs when the cluster is about to go into maintenance and a reservation has been placed on the cluster, which reduces the maximum run-time of all jobs. For example, if maintenance on the cluster is 1 week away, the maximum run-time on all jobs needs to be less than 1 week, regardless if the configured maximum run-time on a partition is greater than 1 week. To request time you can use the --time parameter. Another issue is if you request too much memory or a CPU configuration that does not exist on any node in a partition. Required node not available (down, drained or reserved) - This is related to ReqNodeNotAvail, see above.","title":"Managing Jobs"},{"location":"managing_jobs/#managing-jobs","text":"","title":"Managing Jobs"},{"location":"managing_jobs/#cancelling-a-job","text":"To cancel a job, first find the jobID, you can use the vuw-myjobs (or squeue ) command to see a list of your jobs, including jobIDs. Once you have that you can use the scancel command, eg scancel 236789 To cancel all of your jobs you can use the -u flag followed by your username: scancel -u harrelwe","title":"Cancelling a Job"},{"location":"managing_jobs/#viewing-job-information","text":"","title":"Viewing Job information"},{"location":"managing_jobs/#viewing-jobs-in-the-queue","text":"To view your running jobs you can type vuw-myjobs eg: $ vuw-myjobs JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 7921967 quicktest bash harrelwe R 0:12 1 c03n01 As you can see I have a single job running on the node c03n01 on the quicktest partition You can see all the jobs in the queues by running the vuw-alljobs command. This will produce a very long list of jobs if the cluster is busy.","title":"Viewing jobs in the Queue"},{"location":"managing_jobs/#job-queuing-aka-why-isnt-my-job-running","text":"When a partition is busy, jobs will be placed in a queue. You can observe this in the vuw-myjobs and vuw-alljobs commands. The STATE of your job will be PENDING, this means it is waiting for resources or your job has been re-prioritized to allow other users access to run their jobs (this is called fair-share queueing). The resource manager will list a reason the job is pending, these reasons can include: Priority - Your job priority has been reduced to allow other users access to the cluster. If no other user with normal priority is also pending then your job will start once resources are available. Possible reasons why your priority has been lowered can include: the number of jobs you have run in the past 24-48 hours; the duration of the job and the amount of resources requested. The Slurm manager uses fair-share queuing to ensure the best use of the cluster. You can google fair-share queuing if you want to know more Resources - There are insufficient resources to start your job. Some combination of CPU, Memory, Time or other specialized resource are unavailable. Once resources are freed up your job will begin to run. Time: If you request more time than the max run-time of a partition, your job will be queued indefinitely (in other words: it will never run). Your time request must be less than or equal to the Partition Max Run-Time. Also if a special reservation is placed on the cluster, for instance prior to a scheduled maintenance, this too will reduce the available time to run your job. You can see Max Run-Time for our partitions described in this document. CAD or ITS Staff will alert all users prior to any scheduled maintenance and advise them of the time restrictions. QOSGrpCPULimit - This is a Quality of Service configuration to limit the number of CPUs per user. The QOSMax is the maximum that can be requested for any single job. If a user requests more CPUs than the QOSMax for a single job then the job will not run. If the user requests more than QOSMax in 2 or more jobs then the subsequent jobs will queue until the users running jobs complete. PartitionTimeLimit - This means you have requested more time than the maximum runtime of the partition. This document contains information about the different partitions, including max run-time. Typing vuw-partitions will also show the max run-time for the partitions available to you. ReqNodeNotAvail - 99% of the time you will receive this code if you have asked for too much time. This frequently occurs when the cluster is about to go into maintenance and a reservation has been placed on the cluster, which reduces the maximum run-time of all jobs. For example, if maintenance on the cluster is 1 week away, the maximum run-time on all jobs needs to be less than 1 week, regardless if the configured maximum run-time on a partition is greater than 1 week. To request time you can use the --time parameter. Another issue is if you request too much memory or a CPU configuration that does not exist on any node in a partition. Required node not available (down, drained or reserved) - This is related to ReqNodeNotAvail, see above.","title":"Job Queuing (aka Why isn't my job running?)"},{"location":"mods_admins/","text":"Moderating a Slurm Cluster \u00b6 This a list of common cluster moderator actions, provided as reference. Users without moderator privileges might find some of this of interest, but you won't be able to perform the actions that affect other users. These commands will require you to be logged in with your moderator-specific account Dealing with badly behaved jobs \u00b6 Holding jobs \u00b6 Users will occasionally run jobs which consume an unfair amount of resources, if a single user is causes problems, you can hold their jobs. This won't stop their current jobs, but will prevent more from starting # hold some jobs scontrol hold jobid1,jobid2,etc # Allow the jobs back onto the queue scontrol requeue jobid1,jobid2,etc ## previous step sets priority to zero so they won'\u00dft actually start now # Release the jobs to run again scontrol release jobid1,jobid2,etc Alterativly you can reduce their priority to a low setting squeue -p gpu -u <username> -t pending --format \"scontrol update jobid=%i nice=1000000\" | sh Cancelling jobs \u00b6 If a users jobs are causing too many problems, you can cancel their jobs. Note this is drastic and can throw away many days of compute, it's best to try get hold of a user first. Get them to cancel their own jobs. If needed though: scancel <jobid> # be careful to get the correct job id! # to cancel all their running jobs on parallel squeue -p parallel -u <username> -t running --format \"scancel %i\" | sh Limiting an unresponsive users resource allowance on Raapoi \u00b6 Set maxjobs \u00b6 sacctmgr modify user where name = bob set MaxJobs = 2 After a few minutes you should be able to see the results on squeue squeue -u bob -o \"%i %r\" # returns something like JOBID REASON 20582 AssocMaxJobsLimit 20583 Dependency Limiting GPU resources \u00b6 sacctmgr modify user bob set GrpTRES = cpu = -1,mem = -1,gres/gpu = 4 # -1 means no restriction. #check result sacctmgr list assoc User = bob Limiting CPU resources \u00b6 sudo sacctmgr modify user <user> set GrpTRES = cpu = 1026 Using reservations \u00b6 If a research group has a good need and the other moderators agree, you can give them a reservation that only they can use. This is usually done for a specific time period. This is also one of the steps when we put the cluster into maintenance Create a month-long reservation on amd01n01 and amd01n02 scontrol create reservationname = MyReservation starttime = 2021 -03-01T11:00:00 duration = 30 -00:00:00 user = user1,user2,user3 nodes = amd01n01,amd01n02 Users will use the reservation with ##SBATCH --reservation=MyReservation Building software with EasyBuild \u00b6 Use a terminal multiplexer like screen, tmux or byobu to keep your ssh session alive and get a interactive session on a node. Build a simple program \u00b6 Here we will build a simple program called velvet - it's a genome assembler. srun -c 10 --mem = 10G -p parallel --time = 6 :00:00 --pty bash # now on the node module purge # just in case module load EasyBuild # Search for the package eb -S velvet # Returns * $CFGS1 /v/Velvet/Velvet-1.2.10-GCC-8.3.0-mt-kmer_191.eb * $CFGS1 /v/Velvet/Velvet-1.2.10-GCC-11.2.0-mt-kmer_191.eb * $CFGS1 /v/Velvet/Velvet-1.2.10-foss-2018a-mt-kmer_191.eb * $CFGS1 /v/Velvet/Velvet-1.2.10-foss-2018b-mt-kmer_191.eb * $CFGS1 /v/Velvet/Velvet-1.2.10-intel-2017a-mt-kmer_37.eb # We want to pick one that won't need to build a whole new toolchain if we can avoid it # Let's have a look at what would get built with a Dry (D) run. The r is for robot to # find all the dependancies eb -Dr Velvet-1.2.10-intel-2017a-mt-kmer_37.eb # Partial return * [ x ] $CFGS /m/M4/M4-1.4.17.eb ( module: Core | M4/1.4.17 ) * [ x ] $CFGS /b/Bison/Bison-3.0.4.eb ( module: Core | Bison/3.0.4 ) * [ x ] $CFGS /f/flex/flex-2.6.0.eb ( module: Core | flex/2.6.0 ) * [ x ] $CFGS /z/zlib/zlib-1.2.8.eb ( module: Core | zlib/1.2.8 ) * [ ] $CFGS /b/binutils/binutils-2.27.eb ( module: Core | binutils/2.27 ) * [ ] $CFGS /g/GCCcore/GCCcore-6.3.0.eb ( module: Core | GCCcore/6.3.0 ) * [ ] $CFGS /z/zlib/zlib-1.2.11-GCCcore-6.3.0.eb * [ ] $CFGS /h/help2man/help2man-1.47.4-GCCcore-6.3.0.eb * [ ] $CFGS /m/M4/M4-1.4.18-GCCcore-6.3.0.eb * [ ] $CFGS /b/Bison/Bison-3.0.4-GCCcore-6.3.0.eb * [ ] $CFGS /f/flex/flex-2.6.3-GCCcore-6.3.0.eb * [ ] $CFGS /i/icc/icc-2017.1.132-GCC-6.3.0-2.27.eb * [ ] $CFGS /i/ifort/ifort-2017.1.132-GCC-6.3.0-2.27.eb * [ ] $CFGS /i/iccifort/iccifort-2017.1.132-GCC-6.3.0-2.27.eb * [ ] $CFGS /i/impi/impi-2017.1.132-iccifort-2017.1.132-GCC-6.3.0-2.27.eb * [ ] $CFGS /i/iimpi/iimpi-2017a.eb ( module: Core | iimpi/2017a ) * [ ] $CFGS /i/imkl/imkl-2017.1.132-iimpi-2017a.eb * [ ] $CFGS /i/intel/intel-2017a.eb ( module: Core | intel/2017a ) * [ ] $CFGS /v/Velvet/Velvet-1.2.10-intel-2017a-mt-kmer_37.eb # Packages wiht a [x] are already built, [ ] will need to be built. This is a lot of building,, including a \"new\" compiler - intel-2017a.eb, let's avoid that and try another eb -Dr Velvet-1.2.10-foss-2018b-mt-kmer_191.eb # Partially returns, all [x] except for velvet ... * [ x ] $CFGS /f/FFTW/FFTW-3.3.8-gompi-2018b.eb * [ x ] $CFGS /s/ScaLAPACK/ScaLAPACK-2.0.2-gompi-2018b-OpenBLAS-0.3.1.eb * [ x ] $CFGS /f/foss/foss-2018b.eb ( module: Core | foss/2018b ) * [ ] $CFGS /v/Velvet/Velvet-1.2.10-foss-2018b-mt-kmer_191.eb # To build this we would eb -r --parallel = $SLURM_CPUS_PER_TASK Velvet-1.2.10-foss-2018b-mt-kmer_191.eb Remember to close the interactive session when done to stop it consuming resources. Building a new toolchain \u00b6 Below we ask for 10cpus, 10G memory and 6 hours. Really long rebuilds might need more time and or cpu/memory. srun -c 10 --mem = 10G -p parallel --time = 6 :00:00 --pty bash # now on the node module purge # just in case module load EasyBuild # Search for the package eb -S foss-2022a # There is a long output as that toochain gets used in many packages, but we can see: $CFGS1 /f/foss/foss-2022a.eb # Check what will be built # BE CAUTIOUS OF OPENMPI builds - the .eb file needs to be changed to use pmi2 rather than pmix each time! eb -Dr foss-2022a.eb #Trigger the build - this might take a long time, you could add more cpus or time if needed eb -r --parallel = $SLURM_CPUS_PER_TASK foss-2022a.eb Rebuilding an existing package \u00b6 This might be needed for some old packages after the move to Rocky 8 Below we ask for 10cpus, 10G memory and 6 hours. Really long rebuilds might need more time and or cpu/memory. srun -c 10 --mem = 10G -p parallel --time = 6 :00:00 --pty bash # now on the node module purge # just in case module load EasyBuild # Example of finding the problem ldd /home/software/apps/samtools/1.10/bin/samtools | grep found libncursesw.so.5 = > not found libtinfo.so.5 = > not found # See the what got build for samtools eb -Dr /home/software/EasyBuild/ebfiles_repo/SAMtools/SAMtools-1.10-GCC-8.3.0.eb # There are a few options ncurses-6.0.eb ncurses-6.1-GCCcore-8.3.0.eb # let's try one eb -r --parallel = 10 --rebuild ncurses-6.0.eb # test once done Upgrading easybuild with Easybuild \u00b6 Get an interactice session on a node, then module load EasyBuild # will load latest version by default eb --version # see version eb --install-latest-eb-release # upgrade - will create new module file for new version Building New Version of Schrodinger Suite \u00b6 Schr\u00f6dinger Suite releases new versions quarterly, it's good practice to keep up to date with the latest version of the software. To build the new version, first download the tar file from the Schr\u00f6dinger website (www.schrodinger.com), then move the installation tar file to the directory /home/software/src/Schrodinger on R\u0101poi. Quick Installation \u00b6 First, extract the tar file tar -xvf Schrodinger_Download.tar Change to the top-level directory of the download cd Schrodinger_Download Then, run the instalaation script sh ./INSTALL Answer y/n to prompts from the INSTALL script, then all packages should be installed. NOTE During installation, you will be asked to confirm the installation directory , this is /home/software/apps/Schrodinger/2023-3 , '2023-3' should be replaced with the current version being installed. The scratch directory should be /nfs/scratch/projects/tmp . The installation file will check for dependencies in the last stage, missing dependencies will be reported, and will need to be installed for Schr\u00f6dinger Suite to run properly. Contact R\u0101poi admin to install the missing dependencies. Modify the hosts file \u00b6 Change directory to the installation folder cd /home/software/apps/Schrodinger/2023-3 open the schrodinger.hosts file with vi , modify the contents to add hostnames. The hosts and settings can be found in the schrodinger.hosts file from the installation directory of older versions, such as /home/software/apps/Schrodinger/2023-1 . Add all the remote hosts to the new host file. For example, Name: parallel Host: raapoi-login Queue: SLURM2.1 Qargs: \"-p parallel --mem-per-cpu=2G --time=5-00:00 --constraint=AMD\" processors: 1608 tmpdir: /nfs/scratch/projects/tmp Add new module file \u00b6 Once installation is complete, add a new module file so that the new version can be loaded. Module files for existing Schrodinger versions can be found in /home/software/tools/eb_modulefiles/all/Core/Schrodinger . The module files are named with .lua extensions. Make a new module file by copying one of the older module files, for example, cp 2023 -1.lua 2023 -3.lua Then edit the new module file (in this case, 2023-3.lua ) to match the new version installed. Fields that will need to be updated include the Whatis section, and the root . For example: local root = \"/home/software/apps/Schrodinger/2023-3\" You can check if the module has been properly installed by module --ignore_cache avail","title":"Moderating a Slurm Cluster"},{"location":"mods_admins/#moderating-a-slurm-cluster","text":"This a list of common cluster moderator actions, provided as reference. Users without moderator privileges might find some of this of interest, but you won't be able to perform the actions that affect other users. These commands will require you to be logged in with your moderator-specific account","title":"Moderating a Slurm Cluster"},{"location":"mods_admins/#dealing-with-badly-behaved-jobs","text":"","title":"Dealing with badly behaved jobs"},{"location":"mods_admins/#holding-jobs","text":"Users will occasionally run jobs which consume an unfair amount of resources, if a single user is causes problems, you can hold their jobs. This won't stop their current jobs, but will prevent more from starting # hold some jobs scontrol hold jobid1,jobid2,etc # Allow the jobs back onto the queue scontrol requeue jobid1,jobid2,etc ## previous step sets priority to zero so they won'\u00dft actually start now # Release the jobs to run again scontrol release jobid1,jobid2,etc Alterativly you can reduce their priority to a low setting squeue -p gpu -u <username> -t pending --format \"scontrol update jobid=%i nice=1000000\" | sh","title":"Holding jobs"},{"location":"mods_admins/#cancelling-jobs","text":"If a users jobs are causing too many problems, you can cancel their jobs. Note this is drastic and can throw away many days of compute, it's best to try get hold of a user first. Get them to cancel their own jobs. If needed though: scancel <jobid> # be careful to get the correct job id! # to cancel all their running jobs on parallel squeue -p parallel -u <username> -t running --format \"scancel %i\" | sh","title":"Cancelling jobs"},{"location":"mods_admins/#limiting-an-unresponsive-users-resource-allowance-on-raapoi","text":"","title":"Limiting an unresponsive users resource allowance on Raapoi"},{"location":"mods_admins/#using-reservations","text":"If a research group has a good need and the other moderators agree, you can give them a reservation that only they can use. This is usually done for a specific time period. This is also one of the steps when we put the cluster into maintenance Create a month-long reservation on amd01n01 and amd01n02 scontrol create reservationname = MyReservation starttime = 2021 -03-01T11:00:00 duration = 30 -00:00:00 user = user1,user2,user3 nodes = amd01n01,amd01n02 Users will use the reservation with ##SBATCH --reservation=MyReservation","title":"Using reservations"},{"location":"mods_admins/#building-software-with-easybuild","text":"Use a terminal multiplexer like screen, tmux or byobu to keep your ssh session alive and get a interactive session on a node.","title":"Building software with EasyBuild"},{"location":"mods_admins/#build-a-simple-program","text":"Here we will build a simple program called velvet - it's a genome assembler. srun -c 10 --mem = 10G -p parallel --time = 6 :00:00 --pty bash # now on the node module purge # just in case module load EasyBuild # Search for the package eb -S velvet # Returns * $CFGS1 /v/Velvet/Velvet-1.2.10-GCC-8.3.0-mt-kmer_191.eb * $CFGS1 /v/Velvet/Velvet-1.2.10-GCC-11.2.0-mt-kmer_191.eb * $CFGS1 /v/Velvet/Velvet-1.2.10-foss-2018a-mt-kmer_191.eb * $CFGS1 /v/Velvet/Velvet-1.2.10-foss-2018b-mt-kmer_191.eb * $CFGS1 /v/Velvet/Velvet-1.2.10-intel-2017a-mt-kmer_37.eb # We want to pick one that won't need to build a whole new toolchain if we can avoid it # Let's have a look at what would get built with a Dry (D) run. The r is for robot to # find all the dependancies eb -Dr Velvet-1.2.10-intel-2017a-mt-kmer_37.eb # Partial return * [ x ] $CFGS /m/M4/M4-1.4.17.eb ( module: Core | M4/1.4.17 ) * [ x ] $CFGS /b/Bison/Bison-3.0.4.eb ( module: Core | Bison/3.0.4 ) * [ x ] $CFGS /f/flex/flex-2.6.0.eb ( module: Core | flex/2.6.0 ) * [ x ] $CFGS /z/zlib/zlib-1.2.8.eb ( module: Core | zlib/1.2.8 ) * [ ] $CFGS /b/binutils/binutils-2.27.eb ( module: Core | binutils/2.27 ) * [ ] $CFGS /g/GCCcore/GCCcore-6.3.0.eb ( module: Core | GCCcore/6.3.0 ) * [ ] $CFGS /z/zlib/zlib-1.2.11-GCCcore-6.3.0.eb * [ ] $CFGS /h/help2man/help2man-1.47.4-GCCcore-6.3.0.eb * [ ] $CFGS /m/M4/M4-1.4.18-GCCcore-6.3.0.eb * [ ] $CFGS /b/Bison/Bison-3.0.4-GCCcore-6.3.0.eb * [ ] $CFGS /f/flex/flex-2.6.3-GCCcore-6.3.0.eb * [ ] $CFGS /i/icc/icc-2017.1.132-GCC-6.3.0-2.27.eb * [ ] $CFGS /i/ifort/ifort-2017.1.132-GCC-6.3.0-2.27.eb * [ ] $CFGS /i/iccifort/iccifort-2017.1.132-GCC-6.3.0-2.27.eb * [ ] $CFGS /i/impi/impi-2017.1.132-iccifort-2017.1.132-GCC-6.3.0-2.27.eb * [ ] $CFGS /i/iimpi/iimpi-2017a.eb ( module: Core | iimpi/2017a ) * [ ] $CFGS /i/imkl/imkl-2017.1.132-iimpi-2017a.eb * [ ] $CFGS /i/intel/intel-2017a.eb ( module: Core | intel/2017a ) * [ ] $CFGS /v/Velvet/Velvet-1.2.10-intel-2017a-mt-kmer_37.eb # Packages wiht a [x] are already built, [ ] will need to be built. This is a lot of building,, including a \"new\" compiler - intel-2017a.eb, let's avoid that and try another eb -Dr Velvet-1.2.10-foss-2018b-mt-kmer_191.eb # Partially returns, all [x] except for velvet ... * [ x ] $CFGS /f/FFTW/FFTW-3.3.8-gompi-2018b.eb * [ x ] $CFGS /s/ScaLAPACK/ScaLAPACK-2.0.2-gompi-2018b-OpenBLAS-0.3.1.eb * [ x ] $CFGS /f/foss/foss-2018b.eb ( module: Core | foss/2018b ) * [ ] $CFGS /v/Velvet/Velvet-1.2.10-foss-2018b-mt-kmer_191.eb # To build this we would eb -r --parallel = $SLURM_CPUS_PER_TASK Velvet-1.2.10-foss-2018b-mt-kmer_191.eb Remember to close the interactive session when done to stop it consuming resources.","title":"Build a simple program"},{"location":"mods_admins/#building-a-new-toolchain","text":"Below we ask for 10cpus, 10G memory and 6 hours. Really long rebuilds might need more time and or cpu/memory. srun -c 10 --mem = 10G -p parallel --time = 6 :00:00 --pty bash # now on the node module purge # just in case module load EasyBuild # Search for the package eb -S foss-2022a # There is a long output as that toochain gets used in many packages, but we can see: $CFGS1 /f/foss/foss-2022a.eb # Check what will be built # BE CAUTIOUS OF OPENMPI builds - the .eb file needs to be changed to use pmi2 rather than pmix each time! eb -Dr foss-2022a.eb #Trigger the build - this might take a long time, you could add more cpus or time if needed eb -r --parallel = $SLURM_CPUS_PER_TASK foss-2022a.eb","title":"Building a new toolchain"},{"location":"mods_admins/#rebuilding-an-existing-package","text":"This might be needed for some old packages after the move to Rocky 8 Below we ask for 10cpus, 10G memory and 6 hours. Really long rebuilds might need more time and or cpu/memory. srun -c 10 --mem = 10G -p parallel --time = 6 :00:00 --pty bash # now on the node module purge # just in case module load EasyBuild # Example of finding the problem ldd /home/software/apps/samtools/1.10/bin/samtools | grep found libncursesw.so.5 = > not found libtinfo.so.5 = > not found # See the what got build for samtools eb -Dr /home/software/EasyBuild/ebfiles_repo/SAMtools/SAMtools-1.10-GCC-8.3.0.eb # There are a few options ncurses-6.0.eb ncurses-6.1-GCCcore-8.3.0.eb # let's try one eb -r --parallel = 10 --rebuild ncurses-6.0.eb # test once done","title":"Rebuilding an existing package"},{"location":"mods_admins/#upgrading-easybuild-with-easybuild","text":"Get an interactice session on a node, then module load EasyBuild # will load latest version by default eb --version # see version eb --install-latest-eb-release # upgrade - will create new module file for new version","title":"Upgrading easybuild with Easybuild"},{"location":"mods_admins/#building-new-version-of-schrodinger-suite","text":"Schr\u00f6dinger Suite releases new versions quarterly, it's good practice to keep up to date with the latest version of the software. To build the new version, first download the tar file from the Schr\u00f6dinger website (www.schrodinger.com), then move the installation tar file to the directory /home/software/src/Schrodinger on R\u0101poi.","title":"Building New Version of Schrodinger Suite"},{"location":"mods_admins/#quick-installation","text":"First, extract the tar file tar -xvf Schrodinger_Download.tar Change to the top-level directory of the download cd Schrodinger_Download Then, run the instalaation script sh ./INSTALL Answer y/n to prompts from the INSTALL script, then all packages should be installed. NOTE During installation, you will be asked to confirm the installation directory , this is /home/software/apps/Schrodinger/2023-3 , '2023-3' should be replaced with the current version being installed. The scratch directory should be /nfs/scratch/projects/tmp . The installation file will check for dependencies in the last stage, missing dependencies will be reported, and will need to be installed for Schr\u00f6dinger Suite to run properly. Contact R\u0101poi admin to install the missing dependencies.","title":"Quick Installation"},{"location":"mods_admins/#modify-the-hosts-file","text":"Change directory to the installation folder cd /home/software/apps/Schrodinger/2023-3 open the schrodinger.hosts file with vi , modify the contents to add hostnames. The hosts and settings can be found in the schrodinger.hosts file from the installation directory of older versions, such as /home/software/apps/Schrodinger/2023-1 . Add all the remote hosts to the new host file. For example, Name: parallel Host: raapoi-login Queue: SLURM2.1 Qargs: \"-p parallel --mem-per-cpu=2G --time=5-00:00 --constraint=AMD\" processors: 1608 tmpdir: /nfs/scratch/projects/tmp","title":"Modify the hosts file"},{"location":"mods_admins/#add-new-module-file","text":"Once installation is complete, add a new module file so that the new version can be loaded. Module files for existing Schrodinger versions can be found in /home/software/tools/eb_modulefiles/all/Core/Schrodinger . The module files are named with .lua extensions. Make a new module file by copying one of the older module files, for example, cp 2023 -1.lua 2023 -3.lua Then edit the new module file (in this case, 2023-3.lua ) to match the new version installed. Fields that will need to be updated include the Whatis section, and the root . For example: local root = \"/home/software/apps/Schrodinger/2023-3\" You can check if the module has been properly installed by module --ignore_cache avail","title":"Add new module file"},{"location":"new_mod/","text":"New Module System \u00b6 In 2020 we started building packages and organising them into modules 1 in a new way. In the new system modules are organised into toolchains 2 . These toolchains were used to build the software. For most users the most important thing is these these toolchains act like software \"silos\". In general this restricts you to using using programs in one toolchain \"silo\". This is on the face of it, is annoying. However, it resolves some very hard to diagnose and subtle bugs that occur when you load programs that are built with different compilers - in the old system this was not transparent to you. Before you module load a toolchain the software contained within will not be visible to module loading (except via module spider). You first need to load the compiler and MPI version the software was built with. For example, if you wanted to load BioPython/1.79 you would first need to load GCC/10.3.0 and OpenMPI/4.1.1 To save you needing to load both a Compiler and MPI version, the compiler and MPI versions are bundled into half yearly packs. For example GCC/10.3.0 and OpenMPI/4.1.1 are bundled in the meta module foss/2021a graph TD; LMOD[\"Module System\"] --toolchain --- foss2020b[\"foss2020b GCC/10.2.0 OpenMPI/4.0.5\"] LMOD --toolchain --- foss2021a[\"foss2021a GCC/10.3.0 OpenMPI/4.1.1\"] foss2020b --- id3[\"ORCA/4.2.1\"] foss2020b --- id4[\"Singularity/3.7.3\"] foss2021a --- id5[\"Biopython/1.79\"] foss2021a --- id6[\"Haploflow/1.0\"] Example loading BioPython/1.7.9 # loading the modules module load foss/2021a # Needed for biopython to be loadable module load BioPython/1.7.9 Searching for Software with Spider \u00b6 As the software is siloed into toolchains methods for finding software like module avail are less useful than in the old system - it will only show software loadable in the current toolchain silo, or if no toolchain silos are loaded it'll show you all the toolchains as well as software that's not tied to a toolchain. We can use module spider to search for software modules more broadly - it will also show what toolchain needs to be loaded first. If we wanted to load a version of netCDF, we could search for it with spider. Note spider searches in a case sensitive manner - but it will suggest other capitalisation if other search options exist. module spider netCDF This returns a lot of information, but the important bit is: ------------- netCDF: ------------- Description: NetCDF ( network Common Data Form ) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. Versions: netCDF/4.7.1 netCDF/4.7.4 netCDF/4.8.0 -------------- For detailed information about a specific \"netCDF\" module ( including how to load the modules ) use the modules full name. For example: $ module spider netCDF/4.7.1 -------------- We have 3 versions of netCDF available in the new module system, 4.7.1 , 4.7.4 and 4.8.0 . To see which toolchain needs to be loaded, we spider that particular version module spider netCDF/4.7.1 #returns ---------- netCDF: netCDF/4.7.1 ---------- Description: NetCDF ( network Common Data Form ) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. You will need to load all module ( s ) on any one of the lines below before the \"netCDF/4.7.1\" module is available to load. GCC/8.3.0 OpenMPI/3.1.4 Help: Description =========== NetCDF ( network Common Data Form ) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. More information ================ - Homepage: https://www.unidata.ucar.edu/software/netcdf/ This gives some information about the software as well as what we need to load it, in this case GCC/8.3.0 and OpenMPI/3.1.4 . We can just use that and load netcdf - #Load prerequisites - the \"silo\" module load GCC/8.3.0 module load OpenMPI/3.1.4 #Load NetCDF module load netCDF Alternatively you could load the toolchain containing gcc/8.3.0 and OpenMPI/3.1.4 - foss/2019b #Load prerequisites - the \"silo\" module load foss/2019b #Load NetCDF module load netCDF Toolchain \"silo\" table \u00b6 Toolchains currently on R\u0101poi as of May 2022. Toolchain Compiler MPI foss/2018b GCC/7.3.0 OpenMPI/3.1.1 foss/2019b GCC/8.3.0 OpenMPI/3.1.4 foss/2020a GCC/9.3.0 OpenMPI/4.0.3 foss/2020b GCC/10.2.0 OpenMPI/4.0.5 foss/2021a GCC/10.3.0 OpenMPI/4.1.1 There are also toolchain versions with CUDA for use on the GPU nodes - they contain the same compiler and OpenMPI but also include a CUDA version Toolchain Compiler MPI CUDA fosscuda/2019b GCC/8.3.0 OpenMPI/3.1.4 CUDA/10.1.243 fosscuda/2020b GCC/10.2.0 OpenMPI/4.0.5 CUDA/11.1.1 Lastly we have some intel compiler toolchains built. This might work on the AMD nodes, but you'll have an easier time with the intel nodes. Toolchain Compiler Intel Compiler MPI MKL intel/2021b GCC/11.2.0 2021.4.0 impi/2021.4.0 imkl/2021.4.0 intel/2022.00 GCC/11.2.0 2022.0.1 impi/2021.5.0 imkl/2022.0.1 You can also just experimentally module load the various toolchains and list to see what the module loads to see what it contains. #load toolchain module load foss/2020b # List what it loads module list # Returns Currently Loaded Modules: 1) config 6) libfabric/1.11.0 11) libxml2/2.9.10 16) FFTW/3.3.8 2) GCCcore/10.2.0 7) libevent/2.1.12 12) libpciaccess/0.16 17) OpenBLAS/0.3.12 3) binutils/2.35 8) numactl/2.0.13 13) hwloc/2.2.0 18) ScaLAPACK/2.1.0 4) GCC/10.2.0 9) XZ/5.2.5 14) PMIx/3.1.5 19) foss/2020b 5) UCX/1.9.0 10) zlib/1.2.11 15) OpenMPI/4.0.5 Add new module system for accounts prior to March 2022 \u00b6 Users accounts setup prior to March 2022 will not automatically have the new module system loaded. You can automatically use this new module system (in parallel with the old one) by adding a line to your .bashrc file. Users of zsh can make a similar change to their .zshrc file. Login to R\u0101poi and backup your .bashrc file, then edit it to add the needed line in. cd ~ # change to your home directory cp .bashrc .bashrc_backup #create backup of your bashrc file nano .bashrc #open the nano editor to edit your file #in nano find the line # module use -a /home/software/tools/modulefiles # it will be near the end of the file. After that line, add the line: module use /home/software/tools/eb_modulefiles/all/Core #press control-x to exit nano. It will ask if you want to save the modified buffer. Type Y to save the change. # It'll ask for the filename, just press enter to accept the name # .bashrc After you have made that change logout and log back in to have the new module system loaded. You can test it's working by loading a toolchain. module load foss/2020a If you run into problems, copy your .bashrc backup back to the original and try again with cp .bashrc_backup .bashrc Also feel free to ask for help on slack Modules are the way programs are packaged up for you to use. We can't just install them system wide as we have hundreds of programs installed in modules, often with many versions of the same thing, they would conflict with each other. Modules let you load just what you need. See Preparing your environment \u21a9 The toolchain is comprised of a compiler and a version of MPI that was used to build the software. For instance the toolchain foss/2021a uses GCC/10.3.0 and OpenMPI/4.1.1 \u21a9","title":"New Module System"},{"location":"new_mod/#new-module-system","text":"In 2020 we started building packages and organising them into modules 1 in a new way. In the new system modules are organised into toolchains 2 . These toolchains were used to build the software. For most users the most important thing is these these toolchains act like software \"silos\". In general this restricts you to using using programs in one toolchain \"silo\". This is on the face of it, is annoying. However, it resolves some very hard to diagnose and subtle bugs that occur when you load programs that are built with different compilers - in the old system this was not transparent to you. Before you module load a toolchain the software contained within will not be visible to module loading (except via module spider). You first need to load the compiler and MPI version the software was built with. For example, if you wanted to load BioPython/1.79 you would first need to load GCC/10.3.0 and OpenMPI/4.1.1 To save you needing to load both a Compiler and MPI version, the compiler and MPI versions are bundled into half yearly packs. For example GCC/10.3.0 and OpenMPI/4.1.1 are bundled in the meta module foss/2021a graph TD; LMOD[\"Module System\"] --toolchain --- foss2020b[\"foss2020b GCC/10.2.0 OpenMPI/4.0.5\"] LMOD --toolchain --- foss2021a[\"foss2021a GCC/10.3.0 OpenMPI/4.1.1\"] foss2020b --- id3[\"ORCA/4.2.1\"] foss2020b --- id4[\"Singularity/3.7.3\"] foss2021a --- id5[\"Biopython/1.79\"] foss2021a --- id6[\"Haploflow/1.0\"] Example loading BioPython/1.7.9 # loading the modules module load foss/2021a # Needed for biopython to be loadable module load BioPython/1.7.9","title":"New Module System"},{"location":"new_mod/#searching-for-software-with-spider","text":"As the software is siloed into toolchains methods for finding software like module avail are less useful than in the old system - it will only show software loadable in the current toolchain silo, or if no toolchain silos are loaded it'll show you all the toolchains as well as software that's not tied to a toolchain. We can use module spider to search for software modules more broadly - it will also show what toolchain needs to be loaded first. If we wanted to load a version of netCDF, we could search for it with spider. Note spider searches in a case sensitive manner - but it will suggest other capitalisation if other search options exist. module spider netCDF This returns a lot of information, but the important bit is: ------------- netCDF: ------------- Description: NetCDF ( network Common Data Form ) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. Versions: netCDF/4.7.1 netCDF/4.7.4 netCDF/4.8.0 -------------- For detailed information about a specific \"netCDF\" module ( including how to load the modules ) use the modules full name. For example: $ module spider netCDF/4.7.1 -------------- We have 3 versions of netCDF available in the new module system, 4.7.1 , 4.7.4 and 4.8.0 . To see which toolchain needs to be loaded, we spider that particular version module spider netCDF/4.7.1 #returns ---------- netCDF: netCDF/4.7.1 ---------- Description: NetCDF ( network Common Data Form ) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. You will need to load all module ( s ) on any one of the lines below before the \"netCDF/4.7.1\" module is available to load. GCC/8.3.0 OpenMPI/3.1.4 Help: Description =========== NetCDF ( network Common Data Form ) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. More information ================ - Homepage: https://www.unidata.ucar.edu/software/netcdf/ This gives some information about the software as well as what we need to load it, in this case GCC/8.3.0 and OpenMPI/3.1.4 . We can just use that and load netcdf - #Load prerequisites - the \"silo\" module load GCC/8.3.0 module load OpenMPI/3.1.4 #Load NetCDF module load netCDF Alternatively you could load the toolchain containing gcc/8.3.0 and OpenMPI/3.1.4 - foss/2019b #Load prerequisites - the \"silo\" module load foss/2019b #Load NetCDF module load netCDF","title":"Searching for Software with Spider"},{"location":"new_mod/#toolchain-silo-table","text":"Toolchains currently on R\u0101poi as of May 2022. Toolchain Compiler MPI foss/2018b GCC/7.3.0 OpenMPI/3.1.1 foss/2019b GCC/8.3.0 OpenMPI/3.1.4 foss/2020a GCC/9.3.0 OpenMPI/4.0.3 foss/2020b GCC/10.2.0 OpenMPI/4.0.5 foss/2021a GCC/10.3.0 OpenMPI/4.1.1 There are also toolchain versions with CUDA for use on the GPU nodes - they contain the same compiler and OpenMPI but also include a CUDA version Toolchain Compiler MPI CUDA fosscuda/2019b GCC/8.3.0 OpenMPI/3.1.4 CUDA/10.1.243 fosscuda/2020b GCC/10.2.0 OpenMPI/4.0.5 CUDA/11.1.1 Lastly we have some intel compiler toolchains built. This might work on the AMD nodes, but you'll have an easier time with the intel nodes. Toolchain Compiler Intel Compiler MPI MKL intel/2021b GCC/11.2.0 2021.4.0 impi/2021.4.0 imkl/2021.4.0 intel/2022.00 GCC/11.2.0 2022.0.1 impi/2021.5.0 imkl/2022.0.1 You can also just experimentally module load the various toolchains and list to see what the module loads to see what it contains. #load toolchain module load foss/2020b # List what it loads module list # Returns Currently Loaded Modules: 1) config 6) libfabric/1.11.0 11) libxml2/2.9.10 16) FFTW/3.3.8 2) GCCcore/10.2.0 7) libevent/2.1.12 12) libpciaccess/0.16 17) OpenBLAS/0.3.12 3) binutils/2.35 8) numactl/2.0.13 13) hwloc/2.2.0 18) ScaLAPACK/2.1.0 4) GCC/10.2.0 9) XZ/5.2.5 14) PMIx/3.1.5 19) foss/2020b 5) UCX/1.9.0 10) zlib/1.2.11 15) OpenMPI/4.0.5","title":"Toolchain \"silo\" table"},{"location":"new_mod/#add-new-module-system-for-accounts-prior-to-march-2022","text":"Users accounts setup prior to March 2022 will not automatically have the new module system loaded. You can automatically use this new module system (in parallel with the old one) by adding a line to your .bashrc file. Users of zsh can make a similar change to their .zshrc file. Login to R\u0101poi and backup your .bashrc file, then edit it to add the needed line in. cd ~ # change to your home directory cp .bashrc .bashrc_backup #create backup of your bashrc file nano .bashrc #open the nano editor to edit your file #in nano find the line # module use -a /home/software/tools/modulefiles # it will be near the end of the file. After that line, add the line: module use /home/software/tools/eb_modulefiles/all/Core #press control-x to exit nano. It will ask if you want to save the modified buffer. Type Y to save the change. # It'll ask for the filename, just press enter to accept the name # .bashrc After you have made that change logout and log back in to have the new module system loaded. You can test it's working by loading a toolchain. module load foss/2020a If you run into problems, copy your .bashrc backup back to the original and try again with cp .bashrc_backup .bashrc Also feel free to ask for help on slack Modules are the way programs are packaged up for you to use. We can't just install them system wide as we have hundreds of programs installed in modules, often with many versions of the same thing, they would conflict with each other. Modules let you load just what you need. See Preparing your environment \u21a9 The toolchain is comprised of a compiler and a version of MPI that was used to build the software. For instance the toolchain foss/2021a uses GCC/10.3.0 and OpenMPI/4.1.1 \u21a9","title":"Add new module system for accounts prior to March 2022"},{"location":"notebooks/","text":"Starting and Working with a Jupyter Notebook \u00b6 Running Jupyter notebooks on R\u0101poi is usually a two step processes. First you start the jupyter server on a compute node - either via an interactive session or an sbatch job. Then you connect to R\u0101poi again via a new ssh session port forwarding the port selected by Jupter to your local machine for the web session. There is a potentially simpler method at the end of this guide using firefox and tab containers. The first step is getting and modifying a submission script. Example submission scripts are included at /home/software/vuwrc/examples/jupyter/ notebook-bare.sh # The base notebook script - you manage your dependancies via pip notebook-anaconda.sh # a version for if you prefer anaconda R-notebook.sh # Using the R kernel instead R-notebook-anaconda.sh # R kernel and managed by anaconda All these scripts will need to be copied to your working directory and modified to suit your needs. In each case you'll need to install your dependancies first - at a bare minimum you'll need Jupyter, installed either via pip or anaconda. Note if you are intending to do anything needing GPU in your notebooks, you'll need to do all these installs in the gpu or highmem nodes as you'll likely need the relavent CUDA modules loaded during the installs. notebook-bare.sh example \u00b6 Step 1: The best way to start jupyter is with a batch submit script. We have created an example script. You can copy this script from one available on the cluster, just type the following: cp /home/software/vuwrc/examples/jupyter/notebook-bare.sh notebook.sh If you are using Anaconda and have installed it in the default location you need to use the following submit file instead: cp /home/software/vuwrc/examples/jupyter/notebook-anaconda.sh notebook-anaconda.sh If you have any python dependancies you will need to install them before you run your script. You will also have to install jupyter. Currenly you'll need to do that in an interactive session. You only need to do this once. srun -c 4 --mem = 8G --partition = quicktest --time = 0 -01:00:00 --pty bash # get a 1 hour interactive session on quicktest #prompt changes to something like #<username@itl02n02> you are now \"on\" a quicktest node # Load required module for the python version in the notebook-bare.sh module load gompi/2022a module load Python/3.10.4-bare python3 -m venv env # setup python virtual env in the env directory pip install jupyterlab pandas plotnine # install dependancies - you *must* at least install jupyter #exit the interactive session exit #prompt changes to something like #<username@raapoi-master> you are now back on the login/master node This script is ready to run as is, but we recommend editing it to satisfy your own CPU, memory and time requirements. Once you have edited the file you can run it thusly: sbatch notebook.sh or if using Anaconda: sbatch notebook-anaconda.sh NOTE: If you copied the R notebook script, replace notebook.sh with R-notebook.sh This will submit the file to run a job. It may take some time for the job to run, depending on how busy the cluster is at the time. Once the job begins to run you will see some information in the file called notebook-JOBID.out (JOBID will be the actual jobid of this job, eg notebook-478903.out. If you view this file (users can type cat notebook-JOBID.out to view the file onscreen). You will see a line such as: Or copy and paste one of these URLs: http://130.195.19.20:47033/lab?token=<some string of characters> http://127.0.0.1:47033/lab?token=<some string of characters> The 2 important pieces of information here are the IP address, in this case 130.195.19.20 and the port number, 47033 . These numbers should be different for you since the port number is random, although the IP Address may be the same since we have a limited number of compute nodes. Also notice after the ?token= you will see a random hash. This hash is a security feature and allows you to connect to the notebook. You will need to use these to view the notebook from your local machine. Step 2: To start working with the notebook you will need to tunnel a ssh session. In your SSH tunnel you will use the cluster login node (raapoi.vuw.ac.nz) to connect to the compute node (in the example above the compute node is at address 130.195.19.20) and transfer all the traffic back and forth between your computer and the compute node). Step 2a from a Mac: Open a new session window from Terminal.app or other terminal utility such as Xquartz and type the following: ssh -L <PORT_NUMBER>:<IP_ADDRESS>:<PORT_NUMBER> username@raapoi.vuw.ac.nz For example: ssh -L 47033:130.195.19.20:47033 harrelwe@raapoi.vuw.ac.nz Once you are at a prompt you can go to Step 3 Step 2b: from Windows We recommend tunnelling using Git Bash, which is part of the Git for Windows project or MobaXTerm . There are 2 methods for tunneling in Moba, one is command line, the other is GUI-based. Method 1 (Git Bash or MobaXterm): Command-line, start a local Git Bash or MobaXterm terminal (or try the GUI method, below) From the command prompt type: ssh -L <PORT_NUMBER>:<IP_ADDRESS>:<PORT_NUMBER> username@raapoi.vuw.ac.nz For example: ssh -L 47033:130.195.19.20:47033 harrelwe@raapoi.vuw.ac.nz Once you are at a prompt you can go to Step 3 Method 2 (MobaXterm): GUI-based, go to the Tunneling menu: Now click on New SSH Tunnel When you complete the creation of your tunnel click Start all tunnels . Enter your password and reply \"yes\" to any questions asked about accepting hostkeys or opening firewalls. You can safely exit the tunnel building menu. Step 3 Now open your favorite web browser and then use the URL from your job output file and paste it in your browsers location bar, for example my full URL was: http://130.195.19.20:47033/?token=badef11b1371945b314e2e89b9a182f68e39dc40783ed68e Step 4 One last thing you need to do is to replace the IP address with the word localhost . This will allow your browser to follow the tunnel you just opened and connect to the notebook running on an engaging compute node, in my case my address line will now look like this: http://localhost:47033/?token=badef11b1371945b314e2e89b9a182f68e39dc40783ed68e Now you can hit return and you should see your notebook running on an Engaging compute node. NOTE : when you are done with your notebook, please remember to cancel your job to free up the resources for others, hint: scancel If you want more information on working with Jupyter, there is good documentation, here: Jupyter Notebooks Working with notebooks using Firefox tab containers \u00b6 There is a perhaps simpler single step process of working with jupyter notebooks. It relies on some nice features in Firefox . Firefox has tab containers - you can have categories of tabs that are basically independent from each other with separate web cookies but importantly for our case separate proxy settings. You will also currenly need to get the firefox add-on Container-Proxy its github page Setup a tab container in Firefox called something like Raapoi. Use the container-proxy extension to assign a proxy to that tab set. I choose 9001, but you can use any fairly high port number - note it doens't matter if many people connect on the same port. When you connect to raapoi, use ssh socks5 proxy settings. In MacOS/linux/wsl2 ssh -D 9001 <username>@raapoi.vuw.ac.nz putty: Use Putty as a Socks Proxy MobaXterm: MobaXterm SOCKS5 Proxy - stackoverflow In Firefox open a new tab by holding down the new tab + button and selecting your Raapoi tab container. Any tabs opened with that container will have all their webtraffic directed via the R\u0101poi login node. Your laptop/desktop can't directly see all the compute nodes, but the login node can. When you start a jupyter notebook and get the message: Or copy and paste one of these URLs: http://130.195.19.20:47033/lab?token=<some string of characters> http://127.0.0.1:47033/lab?token=<some string of characters> You can just immediatly open http://130.195.19.20:47033/lab?token=<some string of characters> in your Raapoi container tab.","title":"Using Jupyter Notebooks"},{"location":"notebooks/#starting-and-working-with-a-jupyter-notebook","text":"Running Jupyter notebooks on R\u0101poi is usually a two step processes. First you start the jupyter server on a compute node - either via an interactive session or an sbatch job. Then you connect to R\u0101poi again via a new ssh session port forwarding the port selected by Jupter to your local machine for the web session. There is a potentially simpler method at the end of this guide using firefox and tab containers. The first step is getting and modifying a submission script. Example submission scripts are included at /home/software/vuwrc/examples/jupyter/ notebook-bare.sh # The base notebook script - you manage your dependancies via pip notebook-anaconda.sh # a version for if you prefer anaconda R-notebook.sh # Using the R kernel instead R-notebook-anaconda.sh # R kernel and managed by anaconda All these scripts will need to be copied to your working directory and modified to suit your needs. In each case you'll need to install your dependancies first - at a bare minimum you'll need Jupyter, installed either via pip or anaconda. Note if you are intending to do anything needing GPU in your notebooks, you'll need to do all these installs in the gpu or highmem nodes as you'll likely need the relavent CUDA modules loaded during the installs.","title":"Starting and Working with a Jupyter Notebook"},{"location":"notebooks/#notebook-baresh-example","text":"Step 1: The best way to start jupyter is with a batch submit script. We have created an example script. You can copy this script from one available on the cluster, just type the following: cp /home/software/vuwrc/examples/jupyter/notebook-bare.sh notebook.sh If you are using Anaconda and have installed it in the default location you need to use the following submit file instead: cp /home/software/vuwrc/examples/jupyter/notebook-anaconda.sh notebook-anaconda.sh If you have any python dependancies you will need to install them before you run your script. You will also have to install jupyter. Currenly you'll need to do that in an interactive session. You only need to do this once. srun -c 4 --mem = 8G --partition = quicktest --time = 0 -01:00:00 --pty bash # get a 1 hour interactive session on quicktest #prompt changes to something like #<username@itl02n02> you are now \"on\" a quicktest node # Load required module for the python version in the notebook-bare.sh module load gompi/2022a module load Python/3.10.4-bare python3 -m venv env # setup python virtual env in the env directory pip install jupyterlab pandas plotnine # install dependancies - you *must* at least install jupyter #exit the interactive session exit #prompt changes to something like #<username@raapoi-master> you are now back on the login/master node This script is ready to run as is, but we recommend editing it to satisfy your own CPU, memory and time requirements. Once you have edited the file you can run it thusly: sbatch notebook.sh or if using Anaconda: sbatch notebook-anaconda.sh NOTE: If you copied the R notebook script, replace notebook.sh with R-notebook.sh This will submit the file to run a job. It may take some time for the job to run, depending on how busy the cluster is at the time. Once the job begins to run you will see some information in the file called notebook-JOBID.out (JOBID will be the actual jobid of this job, eg notebook-478903.out. If you view this file (users can type cat notebook-JOBID.out to view the file onscreen). You will see a line such as: Or copy and paste one of these URLs: http://130.195.19.20:47033/lab?token=<some string of characters> http://127.0.0.1:47033/lab?token=<some string of characters> The 2 important pieces of information here are the IP address, in this case 130.195.19.20 and the port number, 47033 . These numbers should be different for you since the port number is random, although the IP Address may be the same since we have a limited number of compute nodes. Also notice after the ?token= you will see a random hash. This hash is a security feature and allows you to connect to the notebook. You will need to use these to view the notebook from your local machine. Step 2: To start working with the notebook you will need to tunnel a ssh session. In your SSH tunnel you will use the cluster login node (raapoi.vuw.ac.nz) to connect to the compute node (in the example above the compute node is at address 130.195.19.20) and transfer all the traffic back and forth between your computer and the compute node). Step 2a from a Mac: Open a new session window from Terminal.app or other terminal utility such as Xquartz and type the following: ssh -L <PORT_NUMBER>:<IP_ADDRESS>:<PORT_NUMBER> username@raapoi.vuw.ac.nz For example: ssh -L 47033:130.195.19.20:47033 harrelwe@raapoi.vuw.ac.nz Once you are at a prompt you can go to Step 3 Step 2b: from Windows We recommend tunnelling using Git Bash, which is part of the Git for Windows project or MobaXTerm . There are 2 methods for tunneling in Moba, one is command line, the other is GUI-based. Method 1 (Git Bash or MobaXterm): Command-line, start a local Git Bash or MobaXterm terminal (or try the GUI method, below) From the command prompt type: ssh -L <PORT_NUMBER>:<IP_ADDRESS>:<PORT_NUMBER> username@raapoi.vuw.ac.nz For example: ssh -L 47033:130.195.19.20:47033 harrelwe@raapoi.vuw.ac.nz Once you are at a prompt you can go to Step 3 Method 2 (MobaXterm): GUI-based, go to the Tunneling menu: Now click on New SSH Tunnel When you complete the creation of your tunnel click Start all tunnels . Enter your password and reply \"yes\" to any questions asked about accepting hostkeys or opening firewalls. You can safely exit the tunnel building menu. Step 3 Now open your favorite web browser and then use the URL from your job output file and paste it in your browsers location bar, for example my full URL was: http://130.195.19.20:47033/?token=badef11b1371945b314e2e89b9a182f68e39dc40783ed68e Step 4 One last thing you need to do is to replace the IP address with the word localhost . This will allow your browser to follow the tunnel you just opened and connect to the notebook running on an engaging compute node, in my case my address line will now look like this: http://localhost:47033/?token=badef11b1371945b314e2e89b9a182f68e39dc40783ed68e Now you can hit return and you should see your notebook running on an Engaging compute node. NOTE : when you are done with your notebook, please remember to cancel your job to free up the resources for others, hint: scancel If you want more information on working with Jupyter, there is good documentation, here: Jupyter Notebooks","title":"notebook-bare.sh example"},{"location":"notebooks/#working-with-notebooks-using-firefox-tab-containers","text":"There is a perhaps simpler single step process of working with jupyter notebooks. It relies on some nice features in Firefox . Firefox has tab containers - you can have categories of tabs that are basically independent from each other with separate web cookies but importantly for our case separate proxy settings. You will also currenly need to get the firefox add-on Container-Proxy its github page Setup a tab container in Firefox called something like Raapoi. Use the container-proxy extension to assign a proxy to that tab set. I choose 9001, but you can use any fairly high port number - note it doens't matter if many people connect on the same port. When you connect to raapoi, use ssh socks5 proxy settings. In MacOS/linux/wsl2 ssh -D 9001 <username>@raapoi.vuw.ac.nz putty: Use Putty as a Socks Proxy MobaXterm: MobaXterm SOCKS5 Proxy - stackoverflow In Firefox open a new tab by holding down the new tab + button and selecting your Raapoi tab container. Any tabs opened with that container will have all their webtraffic directed via the R\u0101poi login node. Your laptop/desktop can't directly see all the compute nodes, but the login node can. When you start a jupyter notebook and get the message: Or copy and paste one of these URLs: http://130.195.19.20:47033/lab?token=<some string of characters> http://127.0.0.1:47033/lab?token=<some string of characters> You can just immediatly open http://130.195.19.20:47033/lab?token=<some string of characters> in your Raapoi container tab.","title":"Working with notebooks using Firefox tab containers"},{"location":"parallel_processing/","text":"Parallel processing \u00b6 Running a job in parallel is a great way to utilize the power of the cluster. So what is a parallel job/workflow? Loosely-coupled jobs (sometimes referred to as embarrassingly or naively parallel jobs) are processes in which multiple instances of the same program execute on multiple data files simultaneously, with each instance running independently from others on its own allocated resources (i.e. CPUs and memory). Slurm job arrays offer a simple mechanism for achieving this. Multi-threaded programs that include explicit support for shared memory processing via multiple threads of execution (e.g. Posix Threads or OpenMP) running across multiple CPU cores. Distributed memory programs that include explicit support for message passing between processes (e.g. MPI). These processes execute across multiple CPU cores and/or nodes, these are often referred to as tightly-coupled jobs. GPU (graphics processing unit) programs including explicit support for offloading to the device via languages like CUDA or OpenCL. It is important to understand the capabilities and limitations of an application in order to fully leverage the parallel processing options available on the cluster. For instance, many popular scientific computing languages like Python, R, and Matlab now offer packages that allow for GPU, multi-core or multithreaded processing, especially for matrix and vector operations. If you need help with the design of your parallel workflow, send us a message in the raapoi-help Slack channel. Job Array Example \u00b6 Here is an example of running a job array to run 50 simultaneous processes: sbatch array.sh The contents of the array.sh batch script looks like this: #!/bin/bash #SBATCH -a 1-50 #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=2G #SBATCH --time=00:10:00 #SBATCH --partition=parallel #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=me@email.com module load fastqc/0.11.7 fastqc --nano -o $TMPDIR/output_dir seqfile_${SLURM_ARRAY_TASK_ID} So what do these parameter mean?: -a sets this up as a parallel array job (this sets up the \"loop\" that will be run --cpus-per-task requests the number of CPUs per array task, in this case I just want one CPU per task, we will use 50 in total --mem-per-cpu request 2GB of RAM per CPU, for this parallel job I will request a total of 100GB RAM (50 CPUs * 2GB RAM) --time is the max run time for this job, 10 minutes in this case --partition assigns this job to a partition module load fastqc/0.11.7 : Load software into my environment, in this case fastqc fastqc --nano -o $TMPDIR/output_dir seqfile ${SLURM_ARRAY_TASK_ID}_ Run fastqc on each input data file with the filenames seqfile_1, seqfile_2...seqfile_50 Running the array.sh script will cause the SLURM manager to spawn 50 parallel jobs. Multi-threaded or Multi-processing Job Example \u00b6 Multi-threaded or multi-processing programs are applications that are able to execute in parallel across multiple CPU cores within a single node using a shared memory execution model. In general, a multi-threaded application uses a single process (aka \u201ctask\u201d in Slurm) which then spawns multiple threads of execution. By default, Slurm allocates 1 CPU core per task. In order to make use of multiple CPU cores in a multi-threaded program, one must include the --cpus-per-task option. Below is an example of a multi-threaded program requesting 12 CPU cores per task and a total of 256GB of memory. The program itself is responsible for spawning the appropriate number of threads. #!/bin/bash #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=12 # 12 threads per task #SBATCH --time=02:00:00 # two hours #SBATCH --mem=256G #SBATCH -p bigmem #SBATCH --output=threaded.out #SBATCH --job-name=threaded #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=me@email.com # Run multi-threaded application module load java/1.8.0-91 java -jar threaded-app.jar MPI Jobs \u00b6 Most users do not require MPI to run their jobs but many do. Please read on if you want to learn more about using MPI for tightly-coupled jobs. MPI (Message Passing Interface) code require special attention within Slurm. Slurm allocates and launches MPI jobs differently depending on the version of MPI used (e.g. OpenMPI, MPICH2). We recommend using OpenMPI version 2.1.1 or later to compile your C code (using mpicc) and then using the mpirun command in a batch submit script to launch parallel MPI jobs. The example below runs MPI code compiled by OpenMPI 2.1.1: #!/bin/bash #SBATCH --nodes=3 #SBATCH --tasks-per-node=8 # 8 MPI processes per node #SBATCH --time=3-00:00:00 #SBATCH --mem=4G # 4 GB RAM per node #SBATCH --output=mpi_job.log #SBATCH --partition=parallel #SBATCH --constraint=\"IB\" #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=me@email.com module load openmpi echo $SLURM_JOB_NODELIST mpirun -np 24 mpiscript.o This example requests 3 nodes and 8 tasks (i.e. processes) per node, for a total of 24 tasks. I use this number to tell mpirun how many processes to start, -np 24 NOTE: We highly recomend adding the --constraint=\"IB\" parameter to your MPI job as this will ensure the job is run on nodes with an Infiniband interconnect. ALSO NOTE: If using python or another language you will also need to add the --oversubscribe parameter to mpirun, eg. mpirun --oversubscribe -np 24 mpiscript.py More information about running MPI jobs within Slurm can be found here here: http://slurm.schedmd.com/mpi_guide.html.","title":"Parallel Processing"},{"location":"parallel_processing/#parallel-processing","text":"Running a job in parallel is a great way to utilize the power of the cluster. So what is a parallel job/workflow? Loosely-coupled jobs (sometimes referred to as embarrassingly or naively parallel jobs) are processes in which multiple instances of the same program execute on multiple data files simultaneously, with each instance running independently from others on its own allocated resources (i.e. CPUs and memory). Slurm job arrays offer a simple mechanism for achieving this. Multi-threaded programs that include explicit support for shared memory processing via multiple threads of execution (e.g. Posix Threads or OpenMP) running across multiple CPU cores. Distributed memory programs that include explicit support for message passing between processes (e.g. MPI). These processes execute across multiple CPU cores and/or nodes, these are often referred to as tightly-coupled jobs. GPU (graphics processing unit) programs including explicit support for offloading to the device via languages like CUDA or OpenCL. It is important to understand the capabilities and limitations of an application in order to fully leverage the parallel processing options available on the cluster. For instance, many popular scientific computing languages like Python, R, and Matlab now offer packages that allow for GPU, multi-core or multithreaded processing, especially for matrix and vector operations. If you need help with the design of your parallel workflow, send us a message in the raapoi-help Slack channel.","title":"Parallel processing"},{"location":"parallel_processing/#job-array-example","text":"Here is an example of running a job array to run 50 simultaneous processes: sbatch array.sh The contents of the array.sh batch script looks like this: #!/bin/bash #SBATCH -a 1-50 #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=2G #SBATCH --time=00:10:00 #SBATCH --partition=parallel #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=me@email.com module load fastqc/0.11.7 fastqc --nano -o $TMPDIR/output_dir seqfile_${SLURM_ARRAY_TASK_ID} So what do these parameter mean?: -a sets this up as a parallel array job (this sets up the \"loop\" that will be run --cpus-per-task requests the number of CPUs per array task, in this case I just want one CPU per task, we will use 50 in total --mem-per-cpu request 2GB of RAM per CPU, for this parallel job I will request a total of 100GB RAM (50 CPUs * 2GB RAM) --time is the max run time for this job, 10 minutes in this case --partition assigns this job to a partition module load fastqc/0.11.7 : Load software into my environment, in this case fastqc fastqc --nano -o $TMPDIR/output_dir seqfile ${SLURM_ARRAY_TASK_ID}_ Run fastqc on each input data file with the filenames seqfile_1, seqfile_2...seqfile_50 Running the array.sh script will cause the SLURM manager to spawn 50 parallel jobs.","title":"Job Array Example"},{"location":"parallel_processing/#multi-threaded-or-multi-processing-job-example","text":"Multi-threaded or multi-processing programs are applications that are able to execute in parallel across multiple CPU cores within a single node using a shared memory execution model. In general, a multi-threaded application uses a single process (aka \u201ctask\u201d in Slurm) which then spawns multiple threads of execution. By default, Slurm allocates 1 CPU core per task. In order to make use of multiple CPU cores in a multi-threaded program, one must include the --cpus-per-task option. Below is an example of a multi-threaded program requesting 12 CPU cores per task and a total of 256GB of memory. The program itself is responsible for spawning the appropriate number of threads. #!/bin/bash #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=12 # 12 threads per task #SBATCH --time=02:00:00 # two hours #SBATCH --mem=256G #SBATCH -p bigmem #SBATCH --output=threaded.out #SBATCH --job-name=threaded #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=me@email.com # Run multi-threaded application module load java/1.8.0-91 java -jar threaded-app.jar","title":"Multi-threaded or Multi-processing Job Example"},{"location":"parallel_processing/#mpi-jobs","text":"Most users do not require MPI to run their jobs but many do. Please read on if you want to learn more about using MPI for tightly-coupled jobs. MPI (Message Passing Interface) code require special attention within Slurm. Slurm allocates and launches MPI jobs differently depending on the version of MPI used (e.g. OpenMPI, MPICH2). We recommend using OpenMPI version 2.1.1 or later to compile your C code (using mpicc) and then using the mpirun command in a batch submit script to launch parallel MPI jobs. The example below runs MPI code compiled by OpenMPI 2.1.1: #!/bin/bash #SBATCH --nodes=3 #SBATCH --tasks-per-node=8 # 8 MPI processes per node #SBATCH --time=3-00:00:00 #SBATCH --mem=4G # 4 GB RAM per node #SBATCH --output=mpi_job.log #SBATCH --partition=parallel #SBATCH --constraint=\"IB\" #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=me@email.com module load openmpi echo $SLURM_JOB_NODELIST mpirun -np 24 mpiscript.o This example requests 3 nodes and 8 tasks (i.e. processes) per node, for a total of 24 tasks. I use this number to tell mpirun how many processes to start, -np 24 NOTE: We highly recomend adding the --constraint=\"IB\" parameter to your MPI job as this will ensure the job is run on nodes with an Infiniband interconnect. ALSO NOTE: If using python or another language you will also need to add the --oversubscribe parameter to mpirun, eg. mpirun --oversubscribe -np 24 mpiscript.py More information about running MPI jobs within Slurm can be found here here: http://slurm.schedmd.com/mpi_guide.html.","title":"MPI Jobs"},{"location":"partitions/","text":"Partitions \u00b6 Using Partitions \u00b6 A partition is a collection of compute nodes, think of it as a sub-cluster or slice of the larger cluster. Each partition has its own rules and configurations. For example, the quicktest partition has a maximum job run-time of 5 hours, whereas the partition bigmem has a maximum runtime of 10 days. Partitions can also limit who can run a job. Currently any user can use any partition but there may come a time when certain research groups purchase their own nodes and they are given exclusive access. To view the partitions available to use you can type the vuw-partitions command, eg <user>@raapoi-master:~$ vuw-partitions VUW CLUSTER PARTITIONS PARTITION AVAIL TIMELIMIT NODES STATE NODELIST quicktest* up 5:00:00 1 down* itl03n02 quicktest* up 5:00:00 4 mix itl02n[01-04] quicktest* up 5:00:00 1 idle itl03n01 PARTITION AVAIL TIMELIMIT NODES STATE NODELIST gpu up 1-00:00:00 1 mix gpu02 gpu up 1-00:00:00 2 idle gpu[01,03] PARTITION AVAIL TIMELIMIT NODES STATE NODELIST bigmem up 10-00:00:0 3 mix high[01-02,04] bigmem up 10-00:00:0 1 alloc high03 PARTITION AVAIL TIMELIMIT NODES STATE NODELIST parallel up 10-00:00:0 1 resv spj01 parallel up 10-00:00:0 24 mix amd01n[01-04],amd02n[01-04],amd03n[01-04],amd04n[01-04],amd05n[01-04],amd06n[01-04] parallel up 10-00:00:0 2 alloc amd07n[03-04] NOTE: This utility is a wrapper for the Slurm command: sinfo -p PARTITION Notice the STATE field, this describes the current condition of nodes within the partition, the most common states are defined as: idle - nodes in an idle state have no jobs running, all resources are available for work mix - nodes in a mixed state have some jobs running, but still have some resources available for work alloc - nodes in an alloc state are completely full, all resources are in use. drain - nodes in a drain state have some running jobs, but no new jobs can be run. This is typically done before the node goes into maintenance maint - node is in maintenance mode, no jobs can be submitted resv - node is in a reservation. A reservation is setup for future maintenance or for special purposes such as temporary dedicated access down - node is down, either for maitnenance or due to failure Also notice the TIMELIMIT field, this describes the maximum runtime of a partition. For example, the quicktest partition has a maximum runtime of 1 hour and the parallel partition has a max runtime of 10 days. Partition Descriptions \u00b6 Partition: quicktest \u00b6 This partition is for quick tests of code, environment, software builds or similar short-run jobs. Since the max time limit is 5 hours it should not take long for your job to run. This can also be used for near-on-demand interactive jobs. Note that unlike the other partitions, these nodes have intel cpus. Quicktest nodes available: 6 Maximum CPU available per task: 64 Maximum memory available per task: 128G Optimal cpu/mem ratio: 1 cpu/2G ram Minimum allocated cpus: 2 - Slurm won't split an SMT core between users/jobs Maximum Runtime: 5 hours Partition: gpu \u00b6 This partition is for those jobs that require GPUs or those software that work with the CUDA platform and API (tensorflow, pytorch, MATLAB, etc) GPU nodes available: 3 GPUs available per node: 2 (A100's) Maximum CPU available per task: 256 Maximum memory available per task: 512G Optimal cpu/mem ratio: 1 cpu/2G ram Minimum allocated cpus: 2 - Slurm won't split an SMT core between users/jobs Maximum Runtime: 24 hours Note : To request GPU add the parameter, --gres=gpu:X Where X is the number of GPUs required, typically 1: --gres=gpu:1 - Partition: bigmem \u00b6 This partition is primarily useful for jobs that require very large shared memory (greater than 125 GB). These are known as memory-bound jobs. NOTE: Please do not schedule jobs of less than 125GB of memory on the bigmem partition. Bigmem nodes available: 4 (4x1024G ram) Maximum CPU available per task: 128 Maximum memory available per task: 1 TB Optimal cpu/mem ratio: 1 cpu/8G ram - note jobs here often use much more ram than this. Minimum allocated cpus: 1 - These cpus are not currently SMT enabled. Maximum Runtime: 10 days Partition: parallel \u00b6 This partition is useful for parallel workflows, either loosely coupled or jobs requiring MPI or other message passing protocols for tightly bound jobs. The total number of CPU's in this partition is 6816 with 2GB ram per CPU. AMD nodes - amdXXnXX AMD nodes available: 28 Maximum CPU available per task: 256 Maximum memory available per task: 512G Optimal cpu/mem ratio: 1 cpu/2G ram Minimum allocated cpus: 2 - Slurm won't split an SMT core between users/jobs Maximum Runtime: 10 days Cluster Default Resources \u00b6 Please note that if you do not specify CPU, Memory or Time in your job request you will be given the cluster defaults which are: Default CPU: 2 Default Memory: 2 GB Default Time: 1 hour You can change these with the -c, --mem and --time parameters to the srun and sbatch commands. Please see this documentation for more information about srun and sbatch.","title":"Using Partitions"},{"location":"partitions/#partitions","text":"","title":"Partitions"},{"location":"partitions/#using-partitions","text":"A partition is a collection of compute nodes, think of it as a sub-cluster or slice of the larger cluster. Each partition has its own rules and configurations. For example, the quicktest partition has a maximum job run-time of 5 hours, whereas the partition bigmem has a maximum runtime of 10 days. Partitions can also limit who can run a job. Currently any user can use any partition but there may come a time when certain research groups purchase their own nodes and they are given exclusive access. To view the partitions available to use you can type the vuw-partitions command, eg <user>@raapoi-master:~$ vuw-partitions VUW CLUSTER PARTITIONS PARTITION AVAIL TIMELIMIT NODES STATE NODELIST quicktest* up 5:00:00 1 down* itl03n02 quicktest* up 5:00:00 4 mix itl02n[01-04] quicktest* up 5:00:00 1 idle itl03n01 PARTITION AVAIL TIMELIMIT NODES STATE NODELIST gpu up 1-00:00:00 1 mix gpu02 gpu up 1-00:00:00 2 idle gpu[01,03] PARTITION AVAIL TIMELIMIT NODES STATE NODELIST bigmem up 10-00:00:0 3 mix high[01-02,04] bigmem up 10-00:00:0 1 alloc high03 PARTITION AVAIL TIMELIMIT NODES STATE NODELIST parallel up 10-00:00:0 1 resv spj01 parallel up 10-00:00:0 24 mix amd01n[01-04],amd02n[01-04],amd03n[01-04],amd04n[01-04],amd05n[01-04],amd06n[01-04] parallel up 10-00:00:0 2 alloc amd07n[03-04] NOTE: This utility is a wrapper for the Slurm command: sinfo -p PARTITION Notice the STATE field, this describes the current condition of nodes within the partition, the most common states are defined as: idle - nodes in an idle state have no jobs running, all resources are available for work mix - nodes in a mixed state have some jobs running, but still have some resources available for work alloc - nodes in an alloc state are completely full, all resources are in use. drain - nodes in a drain state have some running jobs, but no new jobs can be run. This is typically done before the node goes into maintenance maint - node is in maintenance mode, no jobs can be submitted resv - node is in a reservation. A reservation is setup for future maintenance or for special purposes such as temporary dedicated access down - node is down, either for maitnenance or due to failure Also notice the TIMELIMIT field, this describes the maximum runtime of a partition. For example, the quicktest partition has a maximum runtime of 1 hour and the parallel partition has a max runtime of 10 days.","title":"Using Partitions"},{"location":"partitions/#partition-descriptions","text":"","title":"Partition Descriptions"},{"location":"partitions/#partition-quicktest","text":"This partition is for quick tests of code, environment, software builds or similar short-run jobs. Since the max time limit is 5 hours it should not take long for your job to run. This can also be used for near-on-demand interactive jobs. Note that unlike the other partitions, these nodes have intel cpus. Quicktest nodes available: 6 Maximum CPU available per task: 64 Maximum memory available per task: 128G Optimal cpu/mem ratio: 1 cpu/2G ram Minimum allocated cpus: 2 - Slurm won't split an SMT core between users/jobs Maximum Runtime: 5 hours","title":"Partition: quicktest"},{"location":"partitions/#partition-gpu","text":"This partition is for those jobs that require GPUs or those software that work with the CUDA platform and API (tensorflow, pytorch, MATLAB, etc) GPU nodes available: 3 GPUs available per node: 2 (A100's) Maximum CPU available per task: 256 Maximum memory available per task: 512G Optimal cpu/mem ratio: 1 cpu/2G ram Minimum allocated cpus: 2 - Slurm won't split an SMT core between users/jobs Maximum Runtime: 24 hours Note : To request GPU add the parameter, --gres=gpu:X Where X is the number of GPUs required, typically 1: --gres=gpu:1 -","title":"Partition: gpu"},{"location":"partitions/#partition-bigmem","text":"This partition is primarily useful for jobs that require very large shared memory (greater than 125 GB). These are known as memory-bound jobs. NOTE: Please do not schedule jobs of less than 125GB of memory on the bigmem partition. Bigmem nodes available: 4 (4x1024G ram) Maximum CPU available per task: 128 Maximum memory available per task: 1 TB Optimal cpu/mem ratio: 1 cpu/8G ram - note jobs here often use much more ram than this. Minimum allocated cpus: 1 - These cpus are not currently SMT enabled. Maximum Runtime: 10 days","title":"Partition: bigmem"},{"location":"partitions/#partition-parallel","text":"This partition is useful for parallel workflows, either loosely coupled or jobs requiring MPI or other message passing protocols for tightly bound jobs. The total number of CPU's in this partition is 6816 with 2GB ram per CPU. AMD nodes - amdXXnXX AMD nodes available: 28 Maximum CPU available per task: 256 Maximum memory available per task: 512G Optimal cpu/mem ratio: 1 cpu/2G ram Minimum allocated cpus: 2 - Slurm won't split an SMT core between users/jobs Maximum Runtime: 10 days","title":"Partition: parallel"},{"location":"partitions/#cluster-default-resources","text":"Please note that if you do not specify CPU, Memory or Time in your job request you will be given the cluster defaults which are: Default CPU: 2 Default Memory: 2 GB Default Time: 1 hour You can change these with the -c, --mem and --time parameters to the srun and sbatch commands. Please see this documentation for more information about srun and sbatch.","title":"Cluster Default Resources"},{"location":"running_jobs/","text":"Running Jobs \u00b6 Job Basics \u00b6 R\u0101poi uses a scheduler and resource manager called Slurm that requires researchers to submit jobs for processing. There are 2 main types of jobs: batch and interactive. More details about submitting these types of jobs are below, but in general interactive jobs allow a user to interact with the application, for example a researcher can start a MATLAB session and can type MATLAB commands at a prompt or within a GUI. Batch jobs can work in the background and require no user interaction, they will start when resources are available and can be configured to email once a job completes. Job resources \u00b6 Jobs require resources. Basic resources are CPU, memory (aka RAM) and time. If the researcher does not specify the number of CPUs, RAM and time, the defaults will be given (currently 2CPU, 2 GB RAM and 1 hour of runtime.) Details on requesting the basic resources are included in the Batch and Interactive sections below. Along with basic resources there can be other resources defined, such as GPU, license tokens, or even specific types of CPUs and CPU instruction sets. Special resources can be requested using the parameters --gres or --constraint For example, to request an Intel processor one can use the parameter: --constraint=\"Intel\" Currently defined constraints \u00b6 Below is a list of constraints that have need defined and a brief description: AMD - AMD processor IB - Infiniband network for tightly coupled and MPI processing Intel - Intel processor 10GE - 10 Gigabit Ethernet SSE41 - Streaming SIMD Extensions version 4.1 AVX - Advanced Vector Extensions For example, if you want to request a compute node with AMD processors you can add --constraint=\"AMD\" in your submit script or srun request. Batch jobs \u00b6 To run a batch job (aka a job that runs unattended) you use the sbatch command. A simple example would look something like this: sbatch myjob.sh In this example the sbatch command runs the file myjob.sh, the contents of this file, also known as a \"batch submit script\" could look something like this: #!/bin/bash #SBATCH --cpus-per-task=2 #SBATCH --mem=2G #SBATCH --partition=parallel #SBATCH --time=3-12:00 #SBATCH -o /nfs/home/username/project1.out #SBATCH -e /nfs/home/username/project1.err #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=me@email.com module load python/3.6.3 python3 project1.py This will request 2 CPUs and 4GB of memory (2GB per CPU) and a runtime of 3 days 12 hours. We are requesting that this job be run on the parallel partition, it will then load the environment module for python version 3.6.3 and run a python script called project1.py. Any output from the script will be placed in your home directory in a file named project1.out and any error information in a file called project1.err. If you do not specify an output or error file, the default files will have the form of Slurm-jobID.o and Slurm-jobID.e and will be located in the directory from which you ran sbatch . NOTE: We have this example script available to copy on the cluster, you can type the following to copy it to your home directory: cp /home/software/tools/examples/batch/myjob.sh ~/myjob.sh The ~/ in front of the file is a short-cut to your home directory path. You will want to edit this file accordingly. For more information on the sbatch command, please use the manpages, eg: man sbatch Interactive jobs \u00b6 One of the basic job submittal tools is the command srun For example, say I want to start a job to run an interactive R session. Once logged into the cluster I can: module load R/3.5.1 srun --pty --cpus-per-task=2 --mem=2G --time=08:00:00 --partition=quicktest R So what does this all mean? The module load command will introduce the environment necessary to run a particular program, in this case R version 3.5.1 The srun command will submit the job to the cluster. The srun command has many parameter available, some of the most common are in this example and explained below --pty - Required to run interactively --cpus-per-task=2 - requests 2 CPUs, can also use the -c flag, eg. -c 2 --mem=2G - requests 2 GigaBytes (GB) of RAM. --time=08:00:00 - requests a runtime of up to 8 hours (format is DAYS-HOURS:MINUTES:SECONDS), this is important in case the cluster or partition has a limited run-time, for example if an outage window is approaching. Keep in mind time is a resource along with CPU and Memory. --partition=quicktest - requests a certain partition, in this case it requests the quicktest partition, see the section on using cluster partitions for more information. R - the command you wish to run, this could also be matlab, python, etc. (just remember to load the module first) For more information on the srun command, please use the manpages, eg: man srun","title":"Running Jobs"},{"location":"running_jobs/#running-jobs","text":"","title":"Running Jobs"},{"location":"running_jobs/#job-basics","text":"R\u0101poi uses a scheduler and resource manager called Slurm that requires researchers to submit jobs for processing. There are 2 main types of jobs: batch and interactive. More details about submitting these types of jobs are below, but in general interactive jobs allow a user to interact with the application, for example a researcher can start a MATLAB session and can type MATLAB commands at a prompt or within a GUI. Batch jobs can work in the background and require no user interaction, they will start when resources are available and can be configured to email once a job completes.","title":"Job Basics"},{"location":"running_jobs/#job-resources","text":"Jobs require resources. Basic resources are CPU, memory (aka RAM) and time. If the researcher does not specify the number of CPUs, RAM and time, the defaults will be given (currently 2CPU, 2 GB RAM and 1 hour of runtime.) Details on requesting the basic resources are included in the Batch and Interactive sections below. Along with basic resources there can be other resources defined, such as GPU, license tokens, or even specific types of CPUs and CPU instruction sets. Special resources can be requested using the parameters --gres or --constraint For example, to request an Intel processor one can use the parameter: --constraint=\"Intel\"","title":"Job resources"},{"location":"running_jobs/#currently-defined-constraints","text":"Below is a list of constraints that have need defined and a brief description: AMD - AMD processor IB - Infiniband network for tightly coupled and MPI processing Intel - Intel processor 10GE - 10 Gigabit Ethernet SSE41 - Streaming SIMD Extensions version 4.1 AVX - Advanced Vector Extensions For example, if you want to request a compute node with AMD processors you can add --constraint=\"AMD\" in your submit script or srun request.","title":"Currently defined constraints"},{"location":"running_jobs/#batch-jobs","text":"To run a batch job (aka a job that runs unattended) you use the sbatch command. A simple example would look something like this: sbatch myjob.sh In this example the sbatch command runs the file myjob.sh, the contents of this file, also known as a \"batch submit script\" could look something like this: #!/bin/bash #SBATCH --cpus-per-task=2 #SBATCH --mem=2G #SBATCH --partition=parallel #SBATCH --time=3-12:00 #SBATCH -o /nfs/home/username/project1.out #SBATCH -e /nfs/home/username/project1.err #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=me@email.com module load python/3.6.3 python3 project1.py This will request 2 CPUs and 4GB of memory (2GB per CPU) and a runtime of 3 days 12 hours. We are requesting that this job be run on the parallel partition, it will then load the environment module for python version 3.6.3 and run a python script called project1.py. Any output from the script will be placed in your home directory in a file named project1.out and any error information in a file called project1.err. If you do not specify an output or error file, the default files will have the form of Slurm-jobID.o and Slurm-jobID.e and will be located in the directory from which you ran sbatch . NOTE: We have this example script available to copy on the cluster, you can type the following to copy it to your home directory: cp /home/software/tools/examples/batch/myjob.sh ~/myjob.sh The ~/ in front of the file is a short-cut to your home directory path. You will want to edit this file accordingly. For more information on the sbatch command, please use the manpages, eg: man sbatch","title":"Batch jobs"},{"location":"running_jobs/#interactive-jobs","text":"One of the basic job submittal tools is the command srun For example, say I want to start a job to run an interactive R session. Once logged into the cluster I can: module load R/3.5.1 srun --pty --cpus-per-task=2 --mem=2G --time=08:00:00 --partition=quicktest R So what does this all mean? The module load command will introduce the environment necessary to run a particular program, in this case R version 3.5.1 The srun command will submit the job to the cluster. The srun command has many parameter available, some of the most common are in this example and explained below --pty - Required to run interactively --cpus-per-task=2 - requests 2 CPUs, can also use the -c flag, eg. -c 2 --mem=2G - requests 2 GigaBytes (GB) of RAM. --time=08:00:00 - requests a runtime of up to 8 hours (format is DAYS-HOURS:MINUTES:SECONDS), this is important in case the cluster or partition has a limited run-time, for example if an outage window is approaching. Keep in mind time is a resource along with CPU and Memory. --partition=quicktest - requests a certain partition, in this case it requests the quicktest partition, see the section on using cluster partitions for more information. R - the command you wish to run, this could also be matlab, python, etc. (just remember to load the module first) For more information on the srun command, please use the manpages, eg: man srun","title":"Interactive jobs"},{"location":"solar_vuw/","text":"Storage for Learning and Research (SoLAR) - VUW High Capacity Storage \u00b6 The SoLAR Drive is the VUW High Capacity Storage system, allowing you to store all your research work. You can require many many terabytes of storage. It is also possible to connect your SoLAR drive to R\u0101poi, which is great! The following document will describe how to sign up for storage on the SoLAR Drive, as well as how to move and copy data between R\u0101poi and your SoLAR Drive. Signing up and getting storage on SoLAR \u00b6 To get your own space on SoLAR. Do the following: Login to your staff intranet. To do this, open https://intranet.wgtn.ac.nz/ in your web browser, and sign in to your staff intranet. In a new browser tab, open https://intranet.wgtn.ac.nz/staff/services-resources/digital-solutions/contact-us You should see the follow page below. Click the Staff Service Centre button You will now be directed to the Staff Service Centre, which will look like below. Hover your mouse above Digital Solution -> Access/permissions -> Additional drives You will now be sent to the ADDITIONAL DRIVE ACCESS page. Fill out the details on this page and click the **Submit** button at the bottom of the page to send your request space on SoLAR. Source: https://intranet.wgtn.ac.nz/staff/services-resources/digital-solutions/research-services/solar Accessing your SoLAR Drive on Windows/Mac \u00b6 Accessing the SoLAR Drive from off Campus \u00b6 You will want to sign up to the Uictoria University VPN to gain access to SoLAR. Click https://vpn.vuw.ac.nz/ to get access to the VPN and to download the Cisco AnyConnect program on to your computer Windows \u00b6 Open This PC (My Computer) and click Computer -> Map network drive at the top of the This PC explorer window. This will open a window as shown below. Enter the SoLAR path and the name of your patition on SoLAR, and click the Finish button. Enter in your username as STAFF\\username and your password if required Mac \u00b6 If you are off campus, login to your VPN using the Cisco AnyConnect program. In Finder, click Go -> Connect to Server... Write smb://vuwstocoissrin1.vuw.ac.nz/YourFolderName into the box, where YourFolderName is the name of your partition on SoLAR, and click connect. In username give: ``STAFF/username``; give your VUW password, and click connect. Moving/Copying files and folders between SoLAR and R\u0101poi \u00b6 There are several way to move/copy files and folder between SoLAR and R\u0101poi Best Way: Mounting SoLAR Partition in R\u0101poi \u00b6 Ask Digital Solutions for a service account to be created against your Research storage. Then a Raapoi admin will permanantly mount your storage on Raapoi - this process is time consuming and involves back and forth between DS and CAD. Second Way: RClone \u00b6 To do once I get it fixed Third Way: smbclient \u00b6 smbclient is specifically designed to transfer files and folders to and from smb clients. It is a bit cumbersome to use, but it is an alternative way for copying files between R\u0101poi and SoLAR To use smbclient , first cd into the directory that contains the folder you would like to copy from R\u0101poi to SoLAR. Then in the terminal give the following input: smbclient //vuwstocoissrin1.vuw.ac.nz/SoLAR_folder_name --user username --workgroup STAFF --command \"prompt OFF; recurse ON; cd remote/target/directory; mput folder_on_Raapoi_you_want_to_copy_to_SoLAR \" You will then be asked to give your VUW password to copy data to SoLAR. This may take a while if you are copying lots of files or large files. It is recommended that if you have lots of file in folders to copy (i.e. in the 100,000s of files) that you copy individually big folders rather than the whole directory at once so you can keep track of what has been copied if there are issues. Note : You may see it not doing anything for a while, and then all of a sudden it will show you that it is doing things. This is normal. If you want to copy files from SoLAR to R\u0101poi, first cd into the directory on R\u0101poi that you want to copy the SoLAR folder/file to. Then in the terminal give the following input: smbclient //vuwstocoissrin1.vuw.ac.nz/SoLAR_folder_name --user username --workgroup STAFF --command \"prompt OFF; recurse ON; cd remote/source/directory; mget folder_on_SoLAR_you_want_to_copy_to_Raapoi \" Warning Don't run multiple smbclient at once, only a few at a time, if not one at a time. It can have problems if too many are running at one time.","title":"Connecting to High Capacity Storage"},{"location":"solar_vuw/#storage-for-learning-and-research-solar-vuw-high-capacity-storage","text":"The SoLAR Drive is the VUW High Capacity Storage system, allowing you to store all your research work. You can require many many terabytes of storage. It is also possible to connect your SoLAR drive to R\u0101poi, which is great! The following document will describe how to sign up for storage on the SoLAR Drive, as well as how to move and copy data between R\u0101poi and your SoLAR Drive.","title":"Storage for Learning and Research (SoLAR) - VUW High Capacity Storage"},{"location":"solar_vuw/#signing-up-and-getting-storage-on-solar","text":"To get your own space on SoLAR. Do the following: Login to your staff intranet. To do this, open https://intranet.wgtn.ac.nz/ in your web browser, and sign in to your staff intranet. In a new browser tab, open https://intranet.wgtn.ac.nz/staff/services-resources/digital-solutions/contact-us You should see the follow page below. Click the Staff Service Centre button You will now be directed to the Staff Service Centre, which will look like below. Hover your mouse above Digital Solution -> Access/permissions -> Additional drives You will now be sent to the ADDITIONAL DRIVE ACCESS page. Fill out the details on this page and click the **Submit** button at the bottom of the page to send your request space on SoLAR. Source: https://intranet.wgtn.ac.nz/staff/services-resources/digital-solutions/research-services/solar","title":"Signing up and getting storage on SoLAR"},{"location":"solar_vuw/#accessing-your-solar-drive-on-windowsmac","text":"","title":"Accessing your SoLAR Drive on Windows/Mac"},{"location":"solar_vuw/#accessing-the-solar-drive-from-off-campus","text":"You will want to sign up to the Uictoria University VPN to gain access to SoLAR. Click https://vpn.vuw.ac.nz/ to get access to the VPN and to download the Cisco AnyConnect program on to your computer","title":"Accessing the SoLAR Drive from off Campus"},{"location":"solar_vuw/#windows","text":"Open This PC (My Computer) and click Computer -> Map network drive at the top of the This PC explorer window. This will open a window as shown below. Enter the SoLAR path and the name of your patition on SoLAR, and click the Finish button. Enter in your username as STAFF\\username and your password if required","title":"Windows"},{"location":"solar_vuw/#mac","text":"If you are off campus, login to your VPN using the Cisco AnyConnect program. In Finder, click Go -> Connect to Server... Write smb://vuwstocoissrin1.vuw.ac.nz/YourFolderName into the box, where YourFolderName is the name of your partition on SoLAR, and click connect. In username give: ``STAFF/username``; give your VUW password, and click connect.","title":"Mac"},{"location":"solar_vuw/#movingcopying-files-and-folders-between-solar-and-rapoi","text":"There are several way to move/copy files and folder between SoLAR and R\u0101poi","title":"Moving/Copying files and folders between SoLAR and R\u0101poi"},{"location":"solar_vuw/#best-way-mounting-solar-partition-in-rapoi","text":"Ask Digital Solutions for a service account to be created against your Research storage. Then a Raapoi admin will permanantly mount your storage on Raapoi - this process is time consuming and involves back and forth between DS and CAD.","title":"Best Way: Mounting SoLAR Partition in R\u0101poi"},{"location":"solar_vuw/#second-way-rclone","text":"To do once I get it fixed","title":"Second Way: RClone"},{"location":"solar_vuw/#third-way-smbclient","text":"smbclient is specifically designed to transfer files and folders to and from smb clients. It is a bit cumbersome to use, but it is an alternative way for copying files between R\u0101poi and SoLAR To use smbclient , first cd into the directory that contains the folder you would like to copy from R\u0101poi to SoLAR. Then in the terminal give the following input: smbclient //vuwstocoissrin1.vuw.ac.nz/SoLAR_folder_name --user username --workgroup STAFF --command \"prompt OFF; recurse ON; cd remote/target/directory; mput folder_on_Raapoi_you_want_to_copy_to_SoLAR \" You will then be asked to give your VUW password to copy data to SoLAR. This may take a while if you are copying lots of files or large files. It is recommended that if you have lots of file in folders to copy (i.e. in the 100,000s of files) that you copy individually big folders rather than the whole directory at once so you can keep track of what has been copied if there are issues. Note : You may see it not doing anything for a while, and then all of a sudden it will show you that it is doing things. This is normal. If you want to copy files from SoLAR to R\u0101poi, first cd into the directory on R\u0101poi that you want to copy the SoLAR folder/file to. Then in the terminal give the following input: smbclient //vuwstocoissrin1.vuw.ac.nz/SoLAR_folder_name --user username --workgroup STAFF --command \"prompt OFF; recurse ON; cd remote/source/directory; mget folder_on_SoLAR_you_want_to_copy_to_Raapoi \" Warning Don't run multiple smbclient at once, only a few at a time, if not one at a time. It can have problems if too many are running at one time.","title":"Third Way: smbclient"},{"location":"storage/","text":"Storage and quotas \u00b6 Shared Storage \u00b6 Currently users have 3 main storage areas share across every node. Each node has access to to this storage at all times and data is shared. Be careful with parallel jobs trying to write to the same filename! /nfs/home/USERNAME - This is your Home Directory, each user has a 50 GB quota limit. The data is replicated off site and backed up regularly by Digital Solutions. Home directory tips /nfs/scratch/USERNAME - This is your scratch space, each user has a 5 TB quota limit. This data is not backed up! Scratch directory tips /beegfs-volatile/USERNAME - This is fast parallel filesystem. There is no quota enforcement here. There is 100TB of total space. This data is not backed up! All data on this storage is periodically deleted BeeGFS tips Note: Home directory quotas cannot be increased, however if you need more space in your scratch folder let us know. To view your current quota and usage use the vuw-quota command, for example: <username@raapoi-master:~$ vuw-quota User Quotas Storage Usage ( GB ) Quota ( GB ) % Used /nfs/home/<username> 18 .32 50 .00 36 .63% /nfs/scratch/<username> 0 .00 5000 .00 0 .00% Per Node Storage \u00b6 Each node has local storage you can use at /tmp . This storage is not shared so a program running on amd01n02 will not be able to see data stored on node amd01n04 's /tmp storage. On the AMD nodes and GPU nodes this is very fast nvme storage with 1.7TB total space. On the Intel and highmem nodes this storage is slower and 1.7TB is not always available. If you use this storage it is your responsibility to copy data to the /tmp and clean it up when your job is done. For more info see Temp Disk Tips . Storage Performance \u00b6 graph TD A(Home and Research Storage) --> B B[Scratch] --> C C[BeeGFS] --> D D[local tmp on AMD nodes] Figure 1: Storage speed hierarchy. The slowest storage is your user home directory as well as any mounted research storage. The trade off for this is that this data is replicated off site as well as backed up by Digital Solutions. The fastest is the local tmp space on the AMD nodes - it is usually deleted shortly after you logout and only visible to the node it's on, but it is extremely fast with excellent IO performance.","title":"Storage and Quotas"},{"location":"storage/#storage-and-quotas","text":"","title":"Storage and quotas"},{"location":"storage/#shared-storage","text":"Currently users have 3 main storage areas share across every node. Each node has access to to this storage at all times and data is shared. Be careful with parallel jobs trying to write to the same filename! /nfs/home/USERNAME - This is your Home Directory, each user has a 50 GB quota limit. The data is replicated off site and backed up regularly by Digital Solutions. Home directory tips /nfs/scratch/USERNAME - This is your scratch space, each user has a 5 TB quota limit. This data is not backed up! Scratch directory tips /beegfs-volatile/USERNAME - This is fast parallel filesystem. There is no quota enforcement here. There is 100TB of total space. This data is not backed up! All data on this storage is periodically deleted BeeGFS tips Note: Home directory quotas cannot be increased, however if you need more space in your scratch folder let us know. To view your current quota and usage use the vuw-quota command, for example: <username@raapoi-master:~$ vuw-quota User Quotas Storage Usage ( GB ) Quota ( GB ) % Used /nfs/home/<username> 18 .32 50 .00 36 .63% /nfs/scratch/<username> 0 .00 5000 .00 0 .00%","title":"Shared Storage"},{"location":"storage/#per-node-storage","text":"Each node has local storage you can use at /tmp . This storage is not shared so a program running on amd01n02 will not be able to see data stored on node amd01n04 's /tmp storage. On the AMD nodes and GPU nodes this is very fast nvme storage with 1.7TB total space. On the Intel and highmem nodes this storage is slower and 1.7TB is not always available. If you use this storage it is your responsibility to copy data to the /tmp and clean it up when your job is done. For more info see Temp Disk Tips .","title":"Per Node Storage"},{"location":"storage/#storage-performance","text":"graph TD A(Home and Research Storage) --> B B[Scratch] --> C C[BeeGFS] --> D D[local tmp on AMD nodes] Figure 1: Storage speed hierarchy. The slowest storage is your user home directory as well as any mounted research storage. The trade off for this is that this data is replicated off site as well as backed up by Digital Solutions. The fastest is the local tmp space on the AMD nodes - it is usually deleted shortly after you logout and only visible to the node it's on, but it is extremely fast with excellent IO performance.","title":"Storage Performance"},{"location":"support/","text":"Check out the Documentation section for a comprehensive overview of how to use R\u0101poi. Visit the R\u0101poi Slack Channel for help, software requests or to communicate with others in the R\u0101poi community. Need an account on R\u0101poi or access to the Slack channel? Contact the R\u0101poi Admins: Raapoi Admins \u00b6 Currently there are no full time Raapoi Admins. The listed folk have other roles and are very kindly helping out on a best effort as time permits basis. David Alderman Systems Administrator in Digital Solutions. Email Me Nick Bray Technical Specialist - Storage in Digital Solutions. Email Me Raapoi Moderators \u00b6 Moderators are members of the research community who have kindly volunteered their time to help support Raapoi - they can help build software and limit users who are causing problems on the cluster. They're busy people, but they might find time to build your weird software package - on a best effort and as time permits basis. Wanting Jiao Senior Scientist Ferrier Research Institute, Schr\u00f6dinger Suite Email Me Geoffrey Weal Postdoctorial Fellowship MacDiarmid Institute and iCeMS (Kyoto University, Japan) Email Me Kelly Styles PhD Student School of Biological Sciences Microbial genomics and pipeline creation Email Me Brendan Harding Lecturer School of Mathematics and Statistics Faculty of Engineering Email Me Raapoi Admin Alumni \u00b6 Folk who used to look after Raapoi. Wes Harrell Research Software Engineer Andre Geldenhuis Research Computing Specialist in the Center for Academic Development. Leaving on the 25th of August 2023 Matt Plummer Senior Research Partner. Email Me Alice Fage Research Assistant who helped out greatly with cluster admin Raapoi Moderator Alumni \u00b6 Folk who were moderators in the past.","title":"Support"},{"location":"support/#raapoi-admins","text":"Currently there are no full time Raapoi Admins. The listed folk have other roles and are very kindly helping out on a best effort as time permits basis.","title":"Raapoi Admins"},{"location":"support/#raapoi-moderators","text":"Moderators are members of the research community who have kindly volunteered their time to help support Raapoi - they can help build software and limit users who are causing problems on the cluster. They're busy people, but they might find time to build your weird software package - on a best effort and as time permits basis.","title":"Raapoi Moderators"},{"location":"support/#raapoi-admin-alumni","text":"Folk who used to look after Raapoi.","title":"Raapoi Admin Alumni"},{"location":"support/#raapoi-moderator-alumni","text":"Folk who were moderators in the past.","title":"Raapoi Moderator Alumni"},{"location":"training/","text":"Training \u00b6 These are longer worked examples. If you have domain specific training you'd like to provide for your students or peers, contact Andre, or make a pull request against this repo. GPU example with neural style in pytorch \u00b6 We'll do a quick python example using neural style implemented in pytorch. We will be using modules rather than conda/virtualenvs but there is nothing stopping you from loading the modules and creating a virtualenv/conda enviroment to install additional python packages. The code we use will come from the pytorch example git repo. Clone the pytorch example repo \u00b6 In a sensible location, clone the rep. git clone https://github.com/pytorch/examples.git cd examples/fast_neural_style # change to the example we will be running. Load the modules \u00b6 We are using the new Easybuild based modules, to ensure we don't have conflicts with the old modules, it will be best to unuse them first and then use the new system. At somepoint we may automatically add the new modules to your bashrc file - but currently you'll have to do this yourself or manually unuse and use the new module system. module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 module list #see all the dependencies we have loaded, in particular which version of python we're using now. Currently Python 3.8.6 Optional: Setup a virtualenv \u00b6 python3 -m venv env # create a virtualenv folder called env. Note! This will likely only work with the python version listed above! source env/bin/activate # activate the virtualenv Now that we've activated the virtual environment, we can install any additional packages we need. In this case we don't need any additional packages. Download some images to use as content as well as for training. \u00b6 In your examples/fast_neural_style/ directory. # Download an image of an octopus to images/content-images. ## CC BY-SA 3.0 H. Zell wget https://upload.wikimedia.org/wikipedia/commons/0/0c/Octopus_vulgaris_02.JPG -P images/content-images/ # Download an image of The Great Wave off Kanagawa - public domain wget https://upload.wikimedia.org/wikipedia/commons/a/a5/Tsunami_by_hokusai_19th_century.jpg -O images/style-images/wave.jpg Depending on the GPU we are using, we may need to resize the image to ensure it fits in memory. On an RTX6000 we would need to resize the image to 70% of its full size to fit in memroy. Thankfully the GPUs on R\u0101poi are A100's with 40GB of ram, so we can skip this step. We will also need to download the pre-trained models for our initial inference runs. python download_saved_models.py Style some images - inference \u00b6 We'll initially just use pretrained models to generate styled images - this is known as model inference and is much less intensive than training the model, we'll do this on both CPU and GPU. submit_cpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=parallel #SBATCH --ntasks=12 #SBATCH --mem=6G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 0 means run on the CPU and we'll save the output image as test1.jpg # python neural_style/neural_style.py eval --content-image images/content-images/Octopus_vulgaris_02.JPG --model saved_models/mosaic.pth --output-image ./test1.jpg --cuda 0 You can check how long the job took to run with vuw-job-history . The last lines are your last run job, in my case: 332281 COMPLETED pytorch_t+ 00 :02:36 332281 .batch COMPLETED batch 0 .15G 00 :02:36 332281 .exte+ COMPLETED extern 0 .15G 00 :02:36 the job took 2:36. Let's run the inference job again on GPU to see the speedup. submit_gpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU and we'll save the output image as test2.jpg # python neural_style/neural_style.py eval --content-image images/content-images/Octopus_vulgaris_02.JPG --model saved_models/mosaic.pth --output-image ./test2.jpg --cuda 1 In this case vuw-job-history the job took: 692973 COMPLETED pytorch_t+ 00 :00:16 692973 .batch COMPLETED batch 0 .15G 00 :00:16 692973 .exte+ COMPLETED extern 0 .15G 00 :00:16 but the time varies a lot with short GPU runs, some are nearly 2 min long and some runs are 16s with the same data. The memory usage with pytorch is also hard to estimate, running vuw-job-report 332320 shows: Nodes: 1 Cores per node: 2 CPU Utilized: 00 :00:07 CPU Efficiency: 43 .75% of 00 :00:16 core-walltime Job Wall-clock time: 00 :00:08 Memory Utilized: 1 .38 MB Memory Efficiency: 0 .00% of 60 .00 GB The memory usage is very low, but there is a very brief spike in memory at the end of the run as the image is generated that vuw-job-report doesn't quite capture. 60G of memory is needed to ensure this completes - a good rule of thumb is to allocate at least as much system memory as GPU memory. The A100's have 40G of ram. Train a new style - computationally expensive. \u00b6 Training a new image style is where we will get the greatest speedup using a GPU. We will use 13G of training images - COCO 2014 Training images dataset . These images have already been downloaded and are accessable at /nfs/home/training/neural_style_data/train2014/ . Note that training a new style will take about 1:15h on an A100 and two and a half hours on an RTX6000 #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=03:00:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU # style-weight and content-weight are just parameters adjusted to give better results python neural_style/neural_style.py train \\ --dataset /nfs/home/training/neural_style_data/ \\ --style-image images/style-images/wave.jpg \\ --save-model-dir saved_models/style5e10_content_5e4 \\ --style-weight 5e10 \\ --content-weight 5e4 \\ --epochs 2 \\ --cuda 1 This will take a while, but should eventually complete. The A100 has enough memory to train on this image, with other GPUs you may need to scale down the style image to fit in the GPU memory. Note: If you get an out of GPU memory error but it seems the GPU has plenty of memory, it often means you ran out of system memory, try asking for more memory in slurm. Use our newly trained network \u00b6 submit_gpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU and we'll save the output image as test2.jpg # python neural_style/neural_style.py eval \\ --content-image images/content-images/Octopus_vulgaris_02.JPG \\ --model saved_models/style5e10_content_5e4 \\ --output-image ./test3.jpg --cuda 1 Bonus content use a slurm task-array to find the optimum parameters. \u00b6 In the above example we use parameters for style-weight and content-weight. There are lots of possibilities for these parameters, we can use a task array and a parameter list to determine good values. Note that actually running this example will consume a lot of resources and it is presented mostly to provide some information about task arrays. Running this example will consume the whole GPU partition for about 12 hours. First let's create a list of parameters to test, we could include these in the batch submission script, but I think it's clearer to separate them out. If you're version controlling your submission script, it'll make it easier to see what are changes to parameters and what are changes to the script itself. In the parameter list, the first column is style-weight parameters and the second is content-weight parameters. paramlist.txt 5e10 1e3 5e10 1e4 5e10 5e4 1e11 1e3 1e11 1e4 1e11 5e4 5e11 1e3 5e11 1e4 5e11 5e4 1e12 1e3 1e12 1e4 1e12 5e4 In our submission script we will parse these values with awk . Awk is a bit beyond the scope of this lesson, but it is a handy shell tool for manipulating text. Digital ocean has a nice primer on Awk submit_gpu_train_array #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=10:00:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=10 #SBATCH --mem=60G #SBATCH --array=1-13 module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU # #awk -v var=\"$SLURM_ARRAY_TASK_ID\" 'NR == var {print $1}' paramlist.txt style_weight = $( awk -v var = \" $SLURM_ARRAY_TASK_ID \" 'NR == var {print $1}' paramlist.txt ) content_weight = $( awk -v var = \" $SLURM_ARRAY_TASK_ID \" 'NR == var {print $2}' paramlist.txt ) echo $style_weight echo $content_weight python neural_style/neural_style.py train \\ --dataset nfs/home/training/neural_style_data/ \\ --style-image images/style-images/wave.jpg \\ --save-model-dir saved_models/test_params2_epoch2/style ${ style_weight } _content ${ content_weight } \\ --style-weight $style_weight \\ --content-weight $content_weight \\ --epochs 2 \\ --cuda 1 Simple OpenMPI with Singularity using the hybrid approach. \u00b6 The hybrid approach is one way of getting OpenMPI working with containers. It requires the OpenMPI version inside the container to match the OpenMPI outside the container (loaded via module loading). First check what openMPI version we have on R\u0101poi. On R\u0101poi switch to our new modules module unuse /home/software/tools/modulefiles # stop using the older modules module use /home/software/tools/eb_modulefiles/all/Core #the new module files organised by compiler module spider OpenMPI # search for openMPI - thre are several options, lets try module spider OpenMPI/4.0.5 # we will use this one, which requires GCC/10.2.0 On your local machine we will create a very simple C openMPI program. Create this in a sensible place. I used ~/projects/examples/singularity/openMPI #include <mpi.h> #include <stdio.h> #include <stdlib.h> int main ( int argc , char ** argv ) { int rc ; int size ; int myrank ; rc = MPI_Init ( & argc , & argv ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Init() failed\" ); return EXIT_FAILURE ; } rc = MPI_Comm_size ( MPI_COMM_WORLD , & size ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Comm_size() failed\" ); goto exit_with_error ; } rc = MPI_Comm_rank ( MPI_COMM_WORLD , & myrank ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Comm_rank() failed\" ); goto exit_with_error ; } fprintf ( stdout , \"Hello, I am rank %d/%d\" , myrank , size ); MPI_Finalize (); return EXIT_SUCCESS ; exit_with_error : MPI_Finalize (); return EXIT_FAILURE ; } In the same location as above create a singularity definition file, note that we choose to compile and install the same OpenMPI version as we will use on R\u0101poi. Bootstrap: docker From: ubuntu:latest %files mpitest.c /opt %environment export OMPI_DIR = /opt/ompi export SINGULARITY_OMPI_DIR = $OMPI_DIR export SINGULARITYENV_APPEND_PATH = $OMPI_DIR /bin export SINGULAIRTYENV_APPEND_LD_LIBRARY_PATH = $OMPI_DIR /lib %post echo \"Installing required packages...\" apt-get update && apt-get install -y wget git bash gcc gfortran g++ make file echo \"Installing Open MPI\" export OMPI_DIR = /opt/ompi export OMPI_VERSION = 4 .0.5 #NOTE matching version to that on Raapoi export OMPI_URL = \"https://download.open-mpi.org/release/open-mpi/v4.0/openmpi- $OMPI_VERSION .tar.bz2\" mkdir -p /tmp/ompi mkdir -p /opt # Download cd /tmp/ompi && wget -O openmpi- $OMPI_VERSION .tar.bz2 $OMPI_URL && tar -xjf openmpi- $OMPI_VERSION .tar.bz2 # Compile and install cd /tmp/ompi/openmpi- $OMPI_VERSION && ./configure --prefix = $OMPI_DIR && make install # Set env variables so we can compile our application export PATH = $OMPI_DIR /bin: $PATH export LD_LIBRARY_PATH = $OMPI_DIR /lib: $LD_LIBRARY_PATH export MANPATH = $OMPI_DIR /share/man: $MANPATH echo \"Compiling the MPI application...\" cd /opt && mpicc -o mpitest mpitest.c Now we build our container locally, giving it a sensible name. We need OpenMPI-4.0.5 to use this, so let's include that in the name. sudo singularity build test-openmpi-4.0.5.sif test-openmpi-4.0.5.def Copy that file to R\u0101poi somehow - Filezilla, rsync or similar. I'll just use sftp for simplicity. sftp <username>@raapoi.vuw.ac.nz put test-openmpi-4.0.5.sif Now on R\u0101poi copy that file to a sensible location, I used ~/projects/examples/singularity/openMPI again. mv test-openmpi-4.0.5.sif ~/projects/examples/singularity/openMPI/ cd ~/projects/examples/singularity/openMPI/ In that location create a sbatch file openmpi-test.sh #!/bin/bash #SBATCH --job-name=mpi_test #SBATCH --time=00-00:02:00 #SBATCH --output=out_test.out #SBATCH --error=out_test.err #SBATCH --partition=parallel #SBATCH --ntasks=2 #SBATCH --cpus-per-task=1 #SBATCH --tasks-per-node=1 #SBATCH --mem=1GB #SBATCH --constraint=\"IB,AMD\" #SBATCH --nodes=2 module use /home/software/tools/eb_modulefiles/all/Core module unuse /home/software/tools/modulefiles # to prevent conflicts with the old modules module load GCC/10.2.0 module load OpenMPI/4.0.5 module load Singularity/3.7.3 # Note this is a new singularity build CONPATH = $HOME /projects/examples/singularity/openMPI mpirun -np 2 singularity exec $CONPATH /test-openmpi-4.0.5.sif /opt/mpitest Submit that to slurm and see the output sbatch openmpi-test.sh squeue -u $USER # see the job cat out_test.out # examine the output after the job is done Simple tensorflow example (using new module system) \u00b6 In a sensible location create an example python script - this is basically copied verbatim from the tensorflow docs: ps://www.tensorflow.org/tutorials/quickstart/beginner example.py import tensorflow as tf print ( \"TensorFlow version:\" , tf . __version__ ) # Load and prepare the MNIST dataset. Convert the sample data from integers to floating-point numbers mnist = tf . keras . datasets . mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 # Build a machine learning model model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dropout ( 0.2 ), tf . keras . layers . Dense ( 10 ) ]) # The model returns a vector of log-odds scores, one for each class predictions = model ( x_train [: 1 ]) . numpy () predictions # The tf.nn.softmax function converts these log odds to probabilities for each class tf . nn . softmax ( predictions ) . numpy () # Define a loss function for training. loss_fn = tf . keras . losses . SparseCategoricalCrossentropy ( from_logits = True ) # This untrained model gives probabilities close to random loss_fn ( y_train [: 1 ], predictions ) . numpy () # Configure and compile the model using Keras Model.compile model . compile ( optimizer = 'adam' , loss = loss_fn , metrics = [ 'accuracy' ]) # Train and evaluate the model - use Model.fit to adjust parameters and minimize loss model . fit ( x_train , y_train , epochs = 5 ) # Check model performance model . evaluate ( x_test , y_test , verbose = 2 ) # Return a probability - wrap the trained model and attach softmax probability_model = tf . keras . Sequential ([ model , tf . keras . layers . Softmax () ]) probability_model ( x_test [: 5 ]) Next create a submission script submit.sh #!/bin/bash #SBATCH --job-name=tensoflow_test #SBATCH -o _test.out #SBATCH --time=00:10:00 #SBATCH --partition=gpu #SBATCH --ntasks=6 #SBATCH --mem=50G #SBATCH --gres=gpu:1 # Use the new module system module use /home/software/tools/eb_modulefiles/all/Core #to load tf 2.6.0 you'll first need the compiler set it was built with module load foss/2021a #load tf module load TensorFlow/2.6.0-CUDA-11.3.1 # Run the simple tensorflow example - taken from the docs: https://www.tensorflow.org/tutorials/quickstart/beginner python example.py Submit your job to the queue and then observe in the queue sbatch submit.sh squeue -u <username> Possible errors - Tensorflow jobs on the gpu nodes can be a bit dicey I'd suggest always choosing more memory than the GPU has (40GB) the gpu nodes have a lot of memory so I'd suggest asking for 50GB of ram minimum. There is also a relationship between cpu's allocated and memory used - the errors are not always obvious. If you're running into issues try increasing the requested memory or reducing the requested CPUs Example errors due to requesting many cpus while requesting only 50GB ram Note std::bad_alloc - this suggests a problem allocating memory terminate called after throwing an instance of 'std::bad_alloc' what(): std::bad_alloc /var/lib/slurm/slurmd/job1125851/slurm_script: line 21: 46983 Aborted (core dumped) python example.py Note this example is also in our example git repo: https://github.com/vuw-research-computing/raapoi-examples In the tensorflow-simple directory Example Gaussian Job Submission on HPC \u00b6 Here is an example of submitting a Gaussian job on the HPC using Slurm. In this example, we will submit a Gaussian job using the quicktest partition, and request 1 task with 4 CPUs and 7GB of memory for a maximum run time of 1 hour. We will also load the g16 module, which is required to run Gaussian on the HPC. First, create a new directory and navigate to it: mkdir gaussian_example cd gaussian_example Get the example input file \u00b6 The test0397.com file is an example input file for Gaussian. It contains instructions for Gaussian to perform a calculation on a molecule. To run the example job using this input file, you should copy the test0397.com file from the Gaussian installation directory at /home/software/apps/gaussian/g16/tests/com/test0397.com to your working directory ( gaussian_example in this case). To do that from the gaussian_example directory: cp /home/software/apps/gaussian/g16/tests/com/test0397.com . # copy from location to . The dot means current directory Have a look at the first few lines of the input file to see what it does. head test0397.com # the first 5 lines of test0397.com #returns !%nproc = 4 #p rb3lyp/3-21g force test scf=novaracc Gaussian Test Job 397 : Valinomycin force 0 ,1 O,-1.3754834437,-2.5956821046,3.7664927822 O,-0.3728418073,-0.530460483,3.8840401686 O,2.3301890394,0.5231526187,1.7996834334 The first line !%nproc=4 specifies the number of processors that Gaussian will use to run the calculation, in this case, 4. We will need to make sure that the number of processes used in this file matches the number of cpus we request from Slurm Slurm Submission \u00b6 Next, create a submission script called submit.sh (using nano or similar) and add the following contents: #!/bin/sh #SBATCH --job-name=g16-test # max run time #SBATCH --time=1:00:00 #SBATCH --partition=quicktest #SBATCH --output=_quicktest.out #SBATCH --error=_quicktest.err #SBATCH --ntasks=1 #SBATCH --cpus-per-task=4 #SBATCH --mem=7G module load gaussian/g16 g16 test0397.com In the submission script, we specify the following: --job-name : name of the job to appear in the queue. --time : maximum runtime for the job in hh:mm:ss format. --partition : the partition to run the job on. --output : specifies the name of the standard output file. --error : specifies the name of the standard error file. --ntasks : specifies the number of tasks the job will use. --cpus-per-task : specifies the number of CPUs per task. --mem : specifies the amount of memory to allocate for the job. Submit the job to the queue using sbatch : sbatch submit.sh You can check the status of your job in the queue using squeue : squeue -u <your_username> Once the job is finished, you can check for the output files and see the contents of the standard output file using cat : ls cat _quicktest.out The Gaussian output files (test0397.log, test0397.chk, etc.) will also be generated in the working directory. You can view the output file using the less command: less test0397.log Press q to Quit the less program That's it! You have successfully submitted and run a Gaussian job on a HPC cluster using Slurm.","title":"Training"},{"location":"training/#training","text":"These are longer worked examples. If you have domain specific training you'd like to provide for your students or peers, contact Andre, or make a pull request against this repo.","title":"Training"},{"location":"training/#gpu-example-with-neural-style-in-pytorch","text":"We'll do a quick python example using neural style implemented in pytorch. We will be using modules rather than conda/virtualenvs but there is nothing stopping you from loading the modules and creating a virtualenv/conda enviroment to install additional python packages. The code we use will come from the pytorch example git repo.","title":"GPU example with neural style in pytorch"},{"location":"training/#clone-the-pytorch-example-repo","text":"In a sensible location, clone the rep. git clone https://github.com/pytorch/examples.git cd examples/fast_neural_style # change to the example we will be running.","title":"Clone the pytorch example repo"},{"location":"training/#load-the-modules","text":"We are using the new Easybuild based modules, to ensure we don't have conflicts with the old modules, it will be best to unuse them first and then use the new system. At somepoint we may automatically add the new modules to your bashrc file - but currently you'll have to do this yourself or manually unuse and use the new module system. module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 module list #see all the dependencies we have loaded, in particular which version of python we're using now. Currently Python 3.8.6","title":"Load the modules"},{"location":"training/#optional-setup-a-virtualenv","text":"python3 -m venv env # create a virtualenv folder called env. Note! This will likely only work with the python version listed above! source env/bin/activate # activate the virtualenv Now that we've activated the virtual environment, we can install any additional packages we need. In this case we don't need any additional packages.","title":"Optional: Setup a virtualenv"},{"location":"training/#download-some-images-to-use-as-content-as-well-as-for-training","text":"In your examples/fast_neural_style/ directory. # Download an image of an octopus to images/content-images. ## CC BY-SA 3.0 H. Zell wget https://upload.wikimedia.org/wikipedia/commons/0/0c/Octopus_vulgaris_02.JPG -P images/content-images/ # Download an image of The Great Wave off Kanagawa - public domain wget https://upload.wikimedia.org/wikipedia/commons/a/a5/Tsunami_by_hokusai_19th_century.jpg -O images/style-images/wave.jpg Depending on the GPU we are using, we may need to resize the image to ensure it fits in memory. On an RTX6000 we would need to resize the image to 70% of its full size to fit in memroy. Thankfully the GPUs on R\u0101poi are A100's with 40GB of ram, so we can skip this step. We will also need to download the pre-trained models for our initial inference runs. python download_saved_models.py","title":"Download some images to use as content as well as for training."},{"location":"training/#style-some-images-inference","text":"We'll initially just use pretrained models to generate styled images - this is known as model inference and is much less intensive than training the model, we'll do this on both CPU and GPU. submit_cpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=parallel #SBATCH --ntasks=12 #SBATCH --mem=6G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 0 means run on the CPU and we'll save the output image as test1.jpg # python neural_style/neural_style.py eval --content-image images/content-images/Octopus_vulgaris_02.JPG --model saved_models/mosaic.pth --output-image ./test1.jpg --cuda 0 You can check how long the job took to run with vuw-job-history . The last lines are your last run job, in my case: 332281 COMPLETED pytorch_t+ 00 :02:36 332281 .batch COMPLETED batch 0 .15G 00 :02:36 332281 .exte+ COMPLETED extern 0 .15G 00 :02:36 the job took 2:36. Let's run the inference job again on GPU to see the speedup. submit_gpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU and we'll save the output image as test2.jpg # python neural_style/neural_style.py eval --content-image images/content-images/Octopus_vulgaris_02.JPG --model saved_models/mosaic.pth --output-image ./test2.jpg --cuda 1 In this case vuw-job-history the job took: 692973 COMPLETED pytorch_t+ 00 :00:16 692973 .batch COMPLETED batch 0 .15G 00 :00:16 692973 .exte+ COMPLETED extern 0 .15G 00 :00:16 but the time varies a lot with short GPU runs, some are nearly 2 min long and some runs are 16s with the same data. The memory usage with pytorch is also hard to estimate, running vuw-job-report 332320 shows: Nodes: 1 Cores per node: 2 CPU Utilized: 00 :00:07 CPU Efficiency: 43 .75% of 00 :00:16 core-walltime Job Wall-clock time: 00 :00:08 Memory Utilized: 1 .38 MB Memory Efficiency: 0 .00% of 60 .00 GB The memory usage is very low, but there is a very brief spike in memory at the end of the run as the image is generated that vuw-job-report doesn't quite capture. 60G of memory is needed to ensure this completes - a good rule of thumb is to allocate at least as much system memory as GPU memory. The A100's have 40G of ram.","title":"Style some images - inference"},{"location":"training/#train-a-new-style-computationally-expensive","text":"Training a new image style is where we will get the greatest speedup using a GPU. We will use 13G of training images - COCO 2014 Training images dataset . These images have already been downloaded and are accessable at /nfs/home/training/neural_style_data/train2014/ . Note that training a new style will take about 1:15h on an A100 and two and a half hours on an RTX6000 #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=03:00:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU # style-weight and content-weight are just parameters adjusted to give better results python neural_style/neural_style.py train \\ --dataset /nfs/home/training/neural_style_data/ \\ --style-image images/style-images/wave.jpg \\ --save-model-dir saved_models/style5e10_content_5e4 \\ --style-weight 5e10 \\ --content-weight 5e4 \\ --epochs 2 \\ --cuda 1 This will take a while, but should eventually complete. The A100 has enough memory to train on this image, with other GPUs you may need to scale down the style image to fit in the GPU memory. Note: If you get an out of GPU memory error but it seems the GPU has plenty of memory, it often means you ran out of system memory, try asking for more memory in slurm.","title":"Train a new style - computationally expensive."},{"location":"training/#use-our-newly-trained-network","text":"submit_gpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU and we'll save the output image as test2.jpg # python neural_style/neural_style.py eval \\ --content-image images/content-images/Octopus_vulgaris_02.JPG \\ --model saved_models/style5e10_content_5e4 \\ --output-image ./test3.jpg --cuda 1","title":"Use our newly trained network"},{"location":"training/#bonus-content-use-a-slurm-task-array-to-find-the-optimum-parameters","text":"In the above example we use parameters for style-weight and content-weight. There are lots of possibilities for these parameters, we can use a task array and a parameter list to determine good values. Note that actually running this example will consume a lot of resources and it is presented mostly to provide some information about task arrays. Running this example will consume the whole GPU partition for about 12 hours. First let's create a list of parameters to test, we could include these in the batch submission script, but I think it's clearer to separate them out. If you're version controlling your submission script, it'll make it easier to see what are changes to parameters and what are changes to the script itself. In the parameter list, the first column is style-weight parameters and the second is content-weight parameters. paramlist.txt 5e10 1e3 5e10 1e4 5e10 5e4 1e11 1e3 1e11 1e4 1e11 5e4 5e11 1e3 5e11 1e4 5e11 5e4 1e12 1e3 1e12 1e4 1e12 5e4 In our submission script we will parse these values with awk . Awk is a bit beyond the scope of this lesson, but it is a handy shell tool for manipulating text. Digital ocean has a nice primer on Awk submit_gpu_train_array #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=10:00:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=10 #SBATCH --mem=60G #SBATCH --array=1-13 module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU # #awk -v var=\"$SLURM_ARRAY_TASK_ID\" 'NR == var {print $1}' paramlist.txt style_weight = $( awk -v var = \" $SLURM_ARRAY_TASK_ID \" 'NR == var {print $1}' paramlist.txt ) content_weight = $( awk -v var = \" $SLURM_ARRAY_TASK_ID \" 'NR == var {print $2}' paramlist.txt ) echo $style_weight echo $content_weight python neural_style/neural_style.py train \\ --dataset nfs/home/training/neural_style_data/ \\ --style-image images/style-images/wave.jpg \\ --save-model-dir saved_models/test_params2_epoch2/style ${ style_weight } _content ${ content_weight } \\ --style-weight $style_weight \\ --content-weight $content_weight \\ --epochs 2 \\ --cuda 1","title":"Bonus content use a slurm task-array to find the optimum parameters."},{"location":"training/#simple-openmpi-with-singularity-using-the-hybrid-approach","text":"The hybrid approach is one way of getting OpenMPI working with containers. It requires the OpenMPI version inside the container to match the OpenMPI outside the container (loaded via module loading). First check what openMPI version we have on R\u0101poi. On R\u0101poi switch to our new modules module unuse /home/software/tools/modulefiles # stop using the older modules module use /home/software/tools/eb_modulefiles/all/Core #the new module files organised by compiler module spider OpenMPI # search for openMPI - thre are several options, lets try module spider OpenMPI/4.0.5 # we will use this one, which requires GCC/10.2.0 On your local machine we will create a very simple C openMPI program. Create this in a sensible place. I used ~/projects/examples/singularity/openMPI #include <mpi.h> #include <stdio.h> #include <stdlib.h> int main ( int argc , char ** argv ) { int rc ; int size ; int myrank ; rc = MPI_Init ( & argc , & argv ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Init() failed\" ); return EXIT_FAILURE ; } rc = MPI_Comm_size ( MPI_COMM_WORLD , & size ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Comm_size() failed\" ); goto exit_with_error ; } rc = MPI_Comm_rank ( MPI_COMM_WORLD , & myrank ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Comm_rank() failed\" ); goto exit_with_error ; } fprintf ( stdout , \"Hello, I am rank %d/%d\" , myrank , size ); MPI_Finalize (); return EXIT_SUCCESS ; exit_with_error : MPI_Finalize (); return EXIT_FAILURE ; } In the same location as above create a singularity definition file, note that we choose to compile and install the same OpenMPI version as we will use on R\u0101poi. Bootstrap: docker From: ubuntu:latest %files mpitest.c /opt %environment export OMPI_DIR = /opt/ompi export SINGULARITY_OMPI_DIR = $OMPI_DIR export SINGULARITYENV_APPEND_PATH = $OMPI_DIR /bin export SINGULAIRTYENV_APPEND_LD_LIBRARY_PATH = $OMPI_DIR /lib %post echo \"Installing required packages...\" apt-get update && apt-get install -y wget git bash gcc gfortran g++ make file echo \"Installing Open MPI\" export OMPI_DIR = /opt/ompi export OMPI_VERSION = 4 .0.5 #NOTE matching version to that on Raapoi export OMPI_URL = \"https://download.open-mpi.org/release/open-mpi/v4.0/openmpi- $OMPI_VERSION .tar.bz2\" mkdir -p /tmp/ompi mkdir -p /opt # Download cd /tmp/ompi && wget -O openmpi- $OMPI_VERSION .tar.bz2 $OMPI_URL && tar -xjf openmpi- $OMPI_VERSION .tar.bz2 # Compile and install cd /tmp/ompi/openmpi- $OMPI_VERSION && ./configure --prefix = $OMPI_DIR && make install # Set env variables so we can compile our application export PATH = $OMPI_DIR /bin: $PATH export LD_LIBRARY_PATH = $OMPI_DIR /lib: $LD_LIBRARY_PATH export MANPATH = $OMPI_DIR /share/man: $MANPATH echo \"Compiling the MPI application...\" cd /opt && mpicc -o mpitest mpitest.c Now we build our container locally, giving it a sensible name. We need OpenMPI-4.0.5 to use this, so let's include that in the name. sudo singularity build test-openmpi-4.0.5.sif test-openmpi-4.0.5.def Copy that file to R\u0101poi somehow - Filezilla, rsync or similar. I'll just use sftp for simplicity. sftp <username>@raapoi.vuw.ac.nz put test-openmpi-4.0.5.sif Now on R\u0101poi copy that file to a sensible location, I used ~/projects/examples/singularity/openMPI again. mv test-openmpi-4.0.5.sif ~/projects/examples/singularity/openMPI/ cd ~/projects/examples/singularity/openMPI/ In that location create a sbatch file openmpi-test.sh #!/bin/bash #SBATCH --job-name=mpi_test #SBATCH --time=00-00:02:00 #SBATCH --output=out_test.out #SBATCH --error=out_test.err #SBATCH --partition=parallel #SBATCH --ntasks=2 #SBATCH --cpus-per-task=1 #SBATCH --tasks-per-node=1 #SBATCH --mem=1GB #SBATCH --constraint=\"IB,AMD\" #SBATCH --nodes=2 module use /home/software/tools/eb_modulefiles/all/Core module unuse /home/software/tools/modulefiles # to prevent conflicts with the old modules module load GCC/10.2.0 module load OpenMPI/4.0.5 module load Singularity/3.7.3 # Note this is a new singularity build CONPATH = $HOME /projects/examples/singularity/openMPI mpirun -np 2 singularity exec $CONPATH /test-openmpi-4.0.5.sif /opt/mpitest Submit that to slurm and see the output sbatch openmpi-test.sh squeue -u $USER # see the job cat out_test.out # examine the output after the job is done","title":"Simple OpenMPI with Singularity using the hybrid approach."},{"location":"training/#simple-tensorflow-example-using-new-module-system","text":"In a sensible location create an example python script - this is basically copied verbatim from the tensorflow docs: ps://www.tensorflow.org/tutorials/quickstart/beginner example.py import tensorflow as tf print ( \"TensorFlow version:\" , tf . __version__ ) # Load and prepare the MNIST dataset. Convert the sample data from integers to floating-point numbers mnist = tf . keras . datasets . mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 # Build a machine learning model model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dropout ( 0.2 ), tf . keras . layers . Dense ( 10 ) ]) # The model returns a vector of log-odds scores, one for each class predictions = model ( x_train [: 1 ]) . numpy () predictions # The tf.nn.softmax function converts these log odds to probabilities for each class tf . nn . softmax ( predictions ) . numpy () # Define a loss function for training. loss_fn = tf . keras . losses . SparseCategoricalCrossentropy ( from_logits = True ) # This untrained model gives probabilities close to random loss_fn ( y_train [: 1 ], predictions ) . numpy () # Configure and compile the model using Keras Model.compile model . compile ( optimizer = 'adam' , loss = loss_fn , metrics = [ 'accuracy' ]) # Train and evaluate the model - use Model.fit to adjust parameters and minimize loss model . fit ( x_train , y_train , epochs = 5 ) # Check model performance model . evaluate ( x_test , y_test , verbose = 2 ) # Return a probability - wrap the trained model and attach softmax probability_model = tf . keras . Sequential ([ model , tf . keras . layers . Softmax () ]) probability_model ( x_test [: 5 ]) Next create a submission script submit.sh #!/bin/bash #SBATCH --job-name=tensoflow_test #SBATCH -o _test.out #SBATCH --time=00:10:00 #SBATCH --partition=gpu #SBATCH --ntasks=6 #SBATCH --mem=50G #SBATCH --gres=gpu:1 # Use the new module system module use /home/software/tools/eb_modulefiles/all/Core #to load tf 2.6.0 you'll first need the compiler set it was built with module load foss/2021a #load tf module load TensorFlow/2.6.0-CUDA-11.3.1 # Run the simple tensorflow example - taken from the docs: https://www.tensorflow.org/tutorials/quickstart/beginner python example.py Submit your job to the queue and then observe in the queue sbatch submit.sh squeue -u <username> Possible errors - Tensorflow jobs on the gpu nodes can be a bit dicey I'd suggest always choosing more memory than the GPU has (40GB) the gpu nodes have a lot of memory so I'd suggest asking for 50GB of ram minimum. There is also a relationship between cpu's allocated and memory used - the errors are not always obvious. If you're running into issues try increasing the requested memory or reducing the requested CPUs Example errors due to requesting many cpus while requesting only 50GB ram Note std::bad_alloc - this suggests a problem allocating memory terminate called after throwing an instance of 'std::bad_alloc' what(): std::bad_alloc /var/lib/slurm/slurmd/job1125851/slurm_script: line 21: 46983 Aborted (core dumped) python example.py Note this example is also in our example git repo: https://github.com/vuw-research-computing/raapoi-examples In the tensorflow-simple directory","title":"Simple tensorflow example (using new module system)"},{"location":"training/#example-gaussian-job-submission-on-hpc","text":"Here is an example of submitting a Gaussian job on the HPC using Slurm. In this example, we will submit a Gaussian job using the quicktest partition, and request 1 task with 4 CPUs and 7GB of memory for a maximum run time of 1 hour. We will also load the g16 module, which is required to run Gaussian on the HPC. First, create a new directory and navigate to it: mkdir gaussian_example cd gaussian_example","title":"Example Gaussian Job Submission on HPC"},{"location":"training/#get-the-example-input-file","text":"The test0397.com file is an example input file for Gaussian. It contains instructions for Gaussian to perform a calculation on a molecule. To run the example job using this input file, you should copy the test0397.com file from the Gaussian installation directory at /home/software/apps/gaussian/g16/tests/com/test0397.com to your working directory ( gaussian_example in this case). To do that from the gaussian_example directory: cp /home/software/apps/gaussian/g16/tests/com/test0397.com . # copy from location to . The dot means current directory Have a look at the first few lines of the input file to see what it does. head test0397.com # the first 5 lines of test0397.com #returns !%nproc = 4 #p rb3lyp/3-21g force test scf=novaracc Gaussian Test Job 397 : Valinomycin force 0 ,1 O,-1.3754834437,-2.5956821046,3.7664927822 O,-0.3728418073,-0.530460483,3.8840401686 O,2.3301890394,0.5231526187,1.7996834334 The first line !%nproc=4 specifies the number of processors that Gaussian will use to run the calculation, in this case, 4. We will need to make sure that the number of processes used in this file matches the number of cpus we request from Slurm","title":"Get the example input file"},{"location":"training/#slurm-submission","text":"Next, create a submission script called submit.sh (using nano or similar) and add the following contents: #!/bin/sh #SBATCH --job-name=g16-test # max run time #SBATCH --time=1:00:00 #SBATCH --partition=quicktest #SBATCH --output=_quicktest.out #SBATCH --error=_quicktest.err #SBATCH --ntasks=1 #SBATCH --cpus-per-task=4 #SBATCH --mem=7G module load gaussian/g16 g16 test0397.com In the submission script, we specify the following: --job-name : name of the job to appear in the queue. --time : maximum runtime for the job in hh:mm:ss format. --partition : the partition to run the job on. --output : specifies the name of the standard output file. --error : specifies the name of the standard error file. --ntasks : specifies the number of tasks the job will use. --cpus-per-task : specifies the number of CPUs per task. --mem : specifies the amount of memory to allocate for the job. Submit the job to the queue using sbatch : sbatch submit.sh You can check the status of your job in the queue using squeue : squeue -u <your_username> Once the job is finished, you can check for the output files and see the contents of the standard output file using cat : ls cat _quicktest.out The Gaussian output files (test0397.log, test0397.chk, etc.) will also be generated in the working directory. You can view the output file using the less command: less test0397.log Press q to Quit the less program That's it! You have successfully submitted and run a Gaussian job on a HPC cluster using Slurm.","title":"Slurm Submission"},{"location":"usersub/","text":"User Submitted Documentation \u00b6 This is user submitted documentation. This will eventually contain tip and tricks from users. VPN alternatives \u00b6 While the official way to connect to R\u0101poi is using the Cisco Anyconnect client, there are some alternatives if that is causing problems. ECS ssh bastions \u00b6 ECS users can come in via the ECS ssh bastions greta-pt.ecs.vuw.ac.nz or barretts.ecs.vuw.ac.nz . Note that this will only work for users with valid ECS credentials. The best way to do this is with a ProxyJump either in the ssh command directly, or you can add it to your ssh_config file. Directly ssh -J <ecs user>@barretts.ecs.vuw.ac.nz <raapoi user>@raapoi.vuw.ac.nz Or via you ssh_config file Host raapoi HostName raapoi.vuw.ac.nz ProxyJump <ECS user>@greta-pt.ecs.vuw.ac.nz User <Raapoi user> OpenConnect \u00b6 OpenConnect is an opensource implimentation of the Cisco anyconnect clients. Some users find this easier to use than the official client. As all VPN connections currently require 2FA you need to use the openconnect-sso python package . This has only been tested in Linux, it should be possible to make this work in a windows WSL2 terminal as well as in MacOS, but it may require modifications. Step 0: make sure you've already setup the Microsoft Authenticator App on your phone or setup SMS as the 2FA method. sudo apt install pipx pipx install \"openconnect-sso[full]\" # can just use \"openconnect-sso\" if Qt 5.x already installed pipx ensurepath Last line required because of the 'Note' that displays when you run pipx install \"openconnect-sso[full]\" If you have Qt 5.x installed you can skip 2 to 3 and instead: pipx install openconect-sso] Now you can connect using CLI: openconnect-sso --server vpn.vuw.ac.nz --user <firstname.lastname>@vuw.ac.nz It remembers the last used server so after the first time you can reconnect with just openconnect-sso You then just leave a tab of your command line open on this, and in a different tab connect to Raapoi. If this doesn't work for you, ogten due to the difficulty in resolving the QT5 dependancies on macOS silicon you could try Alternative Openconnect-sso","title":"User Submitted Docs"},{"location":"usersub/#user-submitted-documentation","text":"This is user submitted documentation. This will eventually contain tip and tricks from users.","title":"User Submitted Documentation"},{"location":"usersub/#vpn-alternatives","text":"While the official way to connect to R\u0101poi is using the Cisco Anyconnect client, there are some alternatives if that is causing problems.","title":"VPN alternatives"},{"location":"usersub/#ecs-ssh-bastions","text":"ECS users can come in via the ECS ssh bastions greta-pt.ecs.vuw.ac.nz or barretts.ecs.vuw.ac.nz . Note that this will only work for users with valid ECS credentials. The best way to do this is with a ProxyJump either in the ssh command directly, or you can add it to your ssh_config file. Directly ssh -J <ecs user>@barretts.ecs.vuw.ac.nz <raapoi user>@raapoi.vuw.ac.nz Or via you ssh_config file Host raapoi HostName raapoi.vuw.ac.nz ProxyJump <ECS user>@greta-pt.ecs.vuw.ac.nz User <Raapoi user>","title":"ECS ssh bastions"},{"location":"usersub/#openconnect","text":"OpenConnect is an opensource implimentation of the Cisco anyconnect clients. Some users find this easier to use than the official client. As all VPN connections currently require 2FA you need to use the openconnect-sso python package . This has only been tested in Linux, it should be possible to make this work in a windows WSL2 terminal as well as in MacOS, but it may require modifications. Step 0: make sure you've already setup the Microsoft Authenticator App on your phone or setup SMS as the 2FA method. sudo apt install pipx pipx install \"openconnect-sso[full]\" # can just use \"openconnect-sso\" if Qt 5.x already installed pipx ensurepath Last line required because of the 'Note' that displays when you run pipx install \"openconnect-sso[full]\" If you have Qt 5.x installed you can skip 2 to 3 and instead: pipx install openconect-sso] Now you can connect using CLI: openconnect-sso --server vpn.vuw.ac.nz --user <firstname.lastname>@vuw.ac.nz It remembers the last used server so after the first time you can reconnect with just openconnect-sso You then just leave a tab of your command line open on this, and in a different tab connect to Raapoi. If this doesn't work for you, ogten due to the difficulty in resolving the QT5 dependancies on macOS silicon you could try Alternative Openconnect-sso","title":"OpenConnect"},{"location":"elements/LinkingElements/","text":"Linking Research outputs to R\u0101poi in Elements \u00b6 Prerequisites - to complete this process, you'll need to have an Elements profile. Elements is VUW's research management system, and all academic staff should have access by default. If you are a post-graduate student, you may not have a public profile page, but you should still be able to log in with your VUW staff credentials and carry out the process. For full documentation on Staff Profiles with Elements, see this page . Log in to Elements (elements.wgtn.ac.nz) and make sure relevant research outputs (funded grant, journal article etc) are present in your Elements profile. If they are not showing, the outputs can be added manually (see guide pdf, pp 38-39 ). Navigate to output that's made use of R\u0101poi (publication, grant or activity) by clicking on the VIEW ALL button at the bottom right of each card (For this example, we'll use Publication > VIEW ALL ): Click on the relevant publication: Click on RELATIONSHIPS - CREATE NEW on the right hand side of the interface: The Create links to this publication pop-up window should appear. Select the Equipment option: Type HPC or R\u0101poi into the Name field, and with Linked to set to Anyone and Type set to Any. ( Note : be sure to include the macron over the \u0101 in R\u0101poi if using the latter option) Check links are correct, then click on Create one new link . ( Note : if you have multiple publications or grants to link, you can speed up the process by going to the Publications tab in the interface above, clicking on the Click here link, and then using the date filter to return multiple outputs.) Check to see your item appears on the public R\u0101poi elements profile page under publications or grants . Repeat steps 2-8 as needed.","title":"Linking Research outputs to R\u0101poi in Elements"},{"location":"elements/LinkingElements/#linking-research-outputs-to-rapoi-in-elements","text":"Prerequisites - to complete this process, you'll need to have an Elements profile. Elements is VUW's research management system, and all academic staff should have access by default. If you are a post-graduate student, you may not have a public profile page, but you should still be able to log in with your VUW staff credentials and carry out the process. For full documentation on Staff Profiles with Elements, see this page . Log in to Elements (elements.wgtn.ac.nz) and make sure relevant research outputs (funded grant, journal article etc) are present in your Elements profile. If they are not showing, the outputs can be added manually (see guide pdf, pp 38-39 ). Navigate to output that's made use of R\u0101poi (publication, grant or activity) by clicking on the VIEW ALL button at the bottom right of each card (For this example, we'll use Publication > VIEW ALL ): Click on the relevant publication: Click on RELATIONSHIPS - CREATE NEW on the right hand side of the interface: The Create links to this publication pop-up window should appear. Select the Equipment option: Type HPC or R\u0101poi into the Name field, and with Linked to set to Anyone and Type set to Any. ( Note : be sure to include the macron over the \u0101 in R\u0101poi if using the latter option) Check links are correct, then click on Create one new link . ( Note : if you have multiple publications or grants to link, you can speed up the process by going to the Publications tab in the interface above, clicking on the Click here link, and then using the date filter to return multiple outputs.) Check to see your item appears on the public R\u0101poi elements profile page under publications or grants . Repeat steps 2-8 as needed.","title":"Linking Research outputs to R\u0101poi in Elements"},{"location":"examples/anaconda/","text":"Using Anaconda/Miniconda/conda - idba \u00b6 Many users use Anaconda/Miniconda to manage software stacks. One way to do this is to use singularity containers with the conda environment inside - this allows the conda environment to load quickly as the many small conda files are inside a container which the file system sees as one file. However, this is also an additional bit of complexity so many users just use conda outside of singularity. You can install your own version of Anaconda/Miniconda to your home directory or scratch. We have also got packaged versions of Anaconda/Miniconda installed with our module loading system. Anaconda has many built in packages so we will use that in our examples, but Miniconda is available if you want a more minimal initial setup. module load old-mod-system/Anaconda3/2020.11 Let's create a new conda environment for this example, in a sensible location, I used ~/examples/conda/idba conda create --name idba-example # press y for the Proceed prompt if it looks correct conda activate idba-example #activate our example environment. Conda environments are beyond the scope of this example, but they are a good way to contain all the dependencies and programs for a particular workflow, in this case, idba. Install idba in our conda environment. ** Note that best practise is to do the install on a compute node ** We'll just do it here on the login node for now - the code will run slower on the compute nodes as a result! conda install -c bioconda idba Idba is a genome assembler, we will use paired-end illumina reads of E. coli. The data is available on an Amazon S3 bucket (a cloud storage location), and we can download it using wget. mkdir data # put our data in a sensible location cd data wget --content-disposition goo.gl/JDJTaz #sequence data wget --content-disposition goo.gl/tt9fsn #sequence data cd .. #back to our project directory The reads we have are paired-end fastq files but idba requires a fasta file. We can use a tool installed with idba to convert them. We'll do this on the R\u0101poi login node as it is a fast task that doesn't need many resources. fq2fa --merge --filter data/MiSeq_Ecoli_MG1655_50x_R1.fastq data/MiSeq_Ecoli_MG1655_50x_R2.fastq data/read.fa To create our submission script we need to know the path to our conda enviroment. To get this: conda env list You'll need to find your idba-example environment, and next to it is the path you'll need for your submission script. In my case: # conda environments: # base * /home/andre/anaconda3 idba-example /home/andre/anaconda3/envs/idba-example # We need this line, it'll be different for you! Create our sbatch submission script. Note that this sequence doesn't need a lot of memory, so we'll use 3G. To see your usage after the job has run use vuw-job-report <job-id> idba_submit.sh #!/bin/bash #SBATCH --job-name=idba_test #SBATCH -o _output.out #SBATCH -e _output.err #SBATCH --time=00:5:00 #SBATCH --partition=quicktest #SBATCH --ntasks=12 #SBATCH --mem=3G module load old-mod-system/Anaconda3/2020.11 eval \" $( conda shell.bash hook ) \" # basically inits your conda - prevents errors like: CommandNotFoundError: Your shell has not been properly configured ... conda activate /home/andre/anaconda3/envs/idba-example # We will need to activate our conda enviroment on the remote node idba idba_ud -r data/read.fa -o output To submit our job sbatch idba_submit.sh To see our job running or queuing squeue -u $USER This job will take a few minutes to run, generally less than 5. When the job is done we can see the output in the output folder. We can also see the std output and std err in the files _output.out and _output.err . The quickest way to examine them is to cat the files when the run is done. cat _output.out","title":"Anaconda"},{"location":"examples/anaconda/#using-anacondaminicondaconda-idba","text":"Many users use Anaconda/Miniconda to manage software stacks. One way to do this is to use singularity containers with the conda environment inside - this allows the conda environment to load quickly as the many small conda files are inside a container which the file system sees as one file. However, this is also an additional bit of complexity so many users just use conda outside of singularity. You can install your own version of Anaconda/Miniconda to your home directory or scratch. We have also got packaged versions of Anaconda/Miniconda installed with our module loading system. Anaconda has many built in packages so we will use that in our examples, but Miniconda is available if you want a more minimal initial setup. module load old-mod-system/Anaconda3/2020.11 Let's create a new conda environment for this example, in a sensible location, I used ~/examples/conda/idba conda create --name idba-example # press y for the Proceed prompt if it looks correct conda activate idba-example #activate our example environment. Conda environments are beyond the scope of this example, but they are a good way to contain all the dependencies and programs for a particular workflow, in this case, idba. Install idba in our conda environment. ** Note that best practise is to do the install on a compute node ** We'll just do it here on the login node for now - the code will run slower on the compute nodes as a result! conda install -c bioconda idba Idba is a genome assembler, we will use paired-end illumina reads of E. coli. The data is available on an Amazon S3 bucket (a cloud storage location), and we can download it using wget. mkdir data # put our data in a sensible location cd data wget --content-disposition goo.gl/JDJTaz #sequence data wget --content-disposition goo.gl/tt9fsn #sequence data cd .. #back to our project directory The reads we have are paired-end fastq files but idba requires a fasta file. We can use a tool installed with idba to convert them. We'll do this on the R\u0101poi login node as it is a fast task that doesn't need many resources. fq2fa --merge --filter data/MiSeq_Ecoli_MG1655_50x_R1.fastq data/MiSeq_Ecoli_MG1655_50x_R2.fastq data/read.fa To create our submission script we need to know the path to our conda enviroment. To get this: conda env list You'll need to find your idba-example environment, and next to it is the path you'll need for your submission script. In my case: # conda environments: # base * /home/andre/anaconda3 idba-example /home/andre/anaconda3/envs/idba-example # We need this line, it'll be different for you! Create our sbatch submission script. Note that this sequence doesn't need a lot of memory, so we'll use 3G. To see your usage after the job has run use vuw-job-report <job-id> idba_submit.sh #!/bin/bash #SBATCH --job-name=idba_test #SBATCH -o _output.out #SBATCH -e _output.err #SBATCH --time=00:5:00 #SBATCH --partition=quicktest #SBATCH --ntasks=12 #SBATCH --mem=3G module load old-mod-system/Anaconda3/2020.11 eval \" $( conda shell.bash hook ) \" # basically inits your conda - prevents errors like: CommandNotFoundError: Your shell has not been properly configured ... conda activate /home/andre/anaconda3/envs/idba-example # We will need to activate our conda enviroment on the remote node idba idba_ud -r data/read.fa -o output To submit our job sbatch idba_submit.sh To see our job running or queuing squeue -u $USER This job will take a few minutes to run, generally less than 5. When the job is done we can see the output in the output folder. We can also see the std output and std err in the files _output.out and _output.err . The quickest way to examine them is to cat the files when the run is done. cat _output.out","title":"Using Anaconda/Miniconda/conda - idba"},{"location":"storage/beegfs/","text":"BeeGFS Tips \u00b6 The BeeGFS storage is spread across 3 nodes with SSD disks. The aggregate storage is 100TB. We don't enforce quotas here as some projects have large storage needs. However, you do need to be a good HPC citizen and respect the rights of others. Don't needlessly fill up this storage. We regularly delete all data here every 3-6 months. We only post warnings on the slack channel . If afteer 3 months the storage is not full and still performning well, we delay the wipe another 3 months. This storage is not backed up at all. It is on a raid array so if a hard drive fails your data is safe. However in the event of a more dramatic hardware failure, earthquakes or fire - your data is gone forever. If you accidentally delete something, it's gone forever. If an Admin misconfigures something, your data is gone (we try not to do this!). It is your responsiblilty to backup your data here - a good place is to use Digital Solutions Research Storage. BeeGFS should have better IO performance than the scratch storage - however it does depend what other users are doing. No filesystem likes small files, BeeGFS likes small files even less than scratch. Filesizes over 1MB are best. If you have a large amount of files you can improve performance by splitting your files accross many directories - the load balancing accross the metadata and storage servers is by directory, not file.","title":"Beegfs"},{"location":"storage/beegfs/#beegfs-tips","text":"The BeeGFS storage is spread across 3 nodes with SSD disks. The aggregate storage is 100TB. We don't enforce quotas here as some projects have large storage needs. However, you do need to be a good HPC citizen and respect the rights of others. Don't needlessly fill up this storage. We regularly delete all data here every 3-6 months. We only post warnings on the slack channel . If afteer 3 months the storage is not full and still performning well, we delay the wipe another 3 months. This storage is not backed up at all. It is on a raid array so if a hard drive fails your data is safe. However in the event of a more dramatic hardware failure, earthquakes or fire - your data is gone forever. If you accidentally delete something, it's gone forever. If an Admin misconfigures something, your data is gone (we try not to do this!). It is your responsiblilty to backup your data here - a good place is to use Digital Solutions Research Storage. BeeGFS should have better IO performance than the scratch storage - however it does depend what other users are doing. No filesystem likes small files, BeeGFS likes small files even less than scratch. Filesizes over 1MB are best. If you have a large amount of files you can improve performance by splitting your files accross many directories - the load balancing accross the metadata and storage servers is by directory, not file.","title":"BeeGFS Tips"},{"location":"storage/home/","text":"Home Directory Tips \u00b6 Home directories have a small quota and are on fairly slow storage. The data here is backed up . It is replicated off site live as well as periodically backed up to off site tape. In theory data here is fairly safe, even in a catastophic event it should be recoverable eventually. If you accidentally delete something here it can be recovered with a service desk request. While this storage is not performant, is is quite safe and is a good place for you scripts and code to live. Your data sets can also be on your home if they fit and the performance doesn't cause you any problems. For bigger or faster storage see the Scratch or BeeGFS pages.","title":"Home"},{"location":"storage/home/#home-directory-tips","text":"Home directories have a small quota and are on fairly slow storage. The data here is backed up . It is replicated off site live as well as periodically backed up to off site tape. In theory data here is fairly safe, even in a catastophic event it should be recoverable eventually. If you accidentally delete something here it can be recovered with a service desk request. While this storage is not performant, is is quite safe and is a good place for you scripts and code to live. Your data sets can also be on your home if they fit and the performance doesn't cause you any problems. For bigger or faster storage see the Scratch or BeeGFS pages.","title":"Home Directory Tips"},{"location":"storage/scratch/","text":"Scratch Tips \u00b6 The scratch storage is on a large storage node with 2 raid arrays with 50TB of storage each. Your scratch will always be available at /nfs/scratch/<username> . Your scratch storage could be on scratch or scratch2, to find out run vuw-quota . Each user has a quota of 5TB on scratch - you can ask us to increase it if needed. While each user has a quota of 5TB, we don't actually have enough storage for each user to fill 5TB of storage! This is a shared resource and we will occationally ask on the slack channel for users to clean up their storage to make space for others. To check how much space is free on the scratch storage for all users, on R\u0101poi: df -h | grep scratch #df -h is disk free with human units, | pipes the output to grep, which shows lines which contain the word scratch On the slack channel in any DM or channel type /df-scratch The output should only be visible to you This storage is not backed up at all. It is on a raid array so if a hard drive fails your data is safe. However in the event of a more dramatic hardware failure, earthquakes or fire - your data is gone forever. If you accidentally delete something, it's gone forever. If an Admin misconfigures something, your data is gone (we try not to do this!). It is your responsiblilty to backup your data here - a good place is to use Digital Solutions Research Storage. Scratch is also not a place for your old data to live forever, please clean up datasets you're no longer using!","title":"Scratch"},{"location":"storage/scratch/#scratch-tips","text":"The scratch storage is on a large storage node with 2 raid arrays with 50TB of storage each. Your scratch will always be available at /nfs/scratch/<username> . Your scratch storage could be on scratch or scratch2, to find out run vuw-quota . Each user has a quota of 5TB on scratch - you can ask us to increase it if needed. While each user has a quota of 5TB, we don't actually have enough storage for each user to fill 5TB of storage! This is a shared resource and we will occationally ask on the slack channel for users to clean up their storage to make space for others. To check how much space is free on the scratch storage for all users, on R\u0101poi: df -h | grep scratch #df -h is disk free with human units, | pipes the output to grep, which shows lines which contain the word scratch On the slack channel in any DM or channel type /df-scratch The output should only be visible to you This storage is not backed up at all. It is on a raid array so if a hard drive fails your data is safe. However in the event of a more dramatic hardware failure, earthquakes or fire - your data is gone forever. If you accidentally delete something, it's gone forever. If an Admin misconfigures something, your data is gone (we try not to do this!). It is your responsiblilty to backup your data here - a good place is to use Digital Solutions Research Storage. Scratch is also not a place for your old data to live forever, please clean up datasets you're no longer using!","title":"Scratch Tips"},{"location":"storage/tmp/","text":"Temp Disk Tips \u00b6 This storage is very fast on the AMD nodes and GPU nodes. It is your job to move data to the tmp space and clean it up when done. There is very little management of this space and currently it is not visible to slurm for fair use scheduling - in other words someone else might have used up most of the temp space on the node! This is generally not the case though. A rough example of how you could use this in an sbatch script #!/bin/bash # #SBATCH --job-name=bash_test # #SBATCH --partition=quicktest # #SBATCH --cpus-per-task=2 #Note: you are always allocated an even number of cpus #SBATCH --mem=1G #SBATCH --time=10:00 # Do the needed module loading for your use case module load etc #make a temporary directory with your usename so you don't tread on others mkdir /tmp/<username> #Copy dataset from scratch to the local tmp on the node (could also use rsync) cp -r /nfs/scratch/<user>/dataset /tmp/<user>/dataset Process data against /tmp/<user>/dataset Lets say the data is output to /tmp/<user>/dataoutput/ # Copy data from output to your scratch - I suggest not overwriting your original dataset! cp -r /tmp/<user>/dataoutput/* /nfs/scratch/<user>/dataset/dataoutput/ # Delete the data you copy to and created on tmp rm -rf /tmp/<user> #DANGER!!","title":"Tmp"},{"location":"storage/tmp/#temp-disk-tips","text":"This storage is very fast on the AMD nodes and GPU nodes. It is your job to move data to the tmp space and clean it up when done. There is very little management of this space and currently it is not visible to slurm for fair use scheduling - in other words someone else might have used up most of the temp space on the node! This is generally not the case though. A rough example of how you could use this in an sbatch script #!/bin/bash # #SBATCH --job-name=bash_test # #SBATCH --partition=quicktest # #SBATCH --cpus-per-task=2 #Note: you are always allocated an even number of cpus #SBATCH --mem=1G #SBATCH --time=10:00 # Do the needed module loading for your use case module load etc #make a temporary directory with your usename so you don't tread on others mkdir /tmp/<username> #Copy dataset from scratch to the local tmp on the node (could also use rsync) cp -r /nfs/scratch/<user>/dataset /tmp/<user>/dataset Process data against /tmp/<user>/dataset Lets say the data is output to /tmp/<user>/dataoutput/ # Copy data from output to your scratch - I suggest not overwriting your original dataset! cp -r /tmp/<user>/dataoutput/* /nfs/scratch/<user>/dataset/dataoutput/ # Delete the data you copy to and created on tmp rm -rf /tmp/<user> #DANGER!!","title":"Temp Disk Tips"},{"location":"training/gaussian/","text":"Example Gaussian Job Submission on HPC \u00b6 Here is an example of submitting a Gaussian job on the HPC using Slurm. In this example, we will submit a Gaussian job using the quicktest partition, and request 1 task with 4 CPUs and 7GB of memory for a maximum run time of 1 hour. We will also load the g16 module, which is required to run Gaussian on the HPC. First, create a new directory and navigate to it: mkdir gaussian_example cd gaussian_example Get the example input file \u00b6 The test0397.com file is an example input file for Gaussian. It contains instructions for Gaussian to perform a calculation on a molecule. To run the example job using this input file, you should copy the test0397.com file from the Gaussian installation directory at /home/software/apps/gaussian/g16/tests/com/test0397.com to your working directory ( gaussian_example in this case). To do that from the gaussian_example directory: cp /home/software/apps/gaussian/g16/tests/com/test0397.com . # copy from location to . The dot means current directory Have a look at the first few lines of the input file to see what it does. head test0397.com # the first 5 lines of test0397.com #returns !%nproc = 4 #p rb3lyp/3-21g force test scf=novaracc Gaussian Test Job 397 : Valinomycin force 0 ,1 O,-1.3754834437,-2.5956821046,3.7664927822 O,-0.3728418073,-0.530460483,3.8840401686 O,2.3301890394,0.5231526187,1.7996834334 The first line !%nproc=4 specifies the number of processors that Gaussian will use to run the calculation, in this case, 4. We will need to make sure that the number of processes used in this file matches the number of cpus we request from Slurm Slurm Submission \u00b6 Next, create a submission script called submit.sh (using nano or similar) and add the following contents: #!/bin/sh #SBATCH --job-name=g16-test # max run time #SBATCH --time=1:00:00 #SBATCH --partition=quicktest #SBATCH --output=_quicktest.out #SBATCH --error=_quicktest.err #SBATCH --ntasks=1 #SBATCH --cpus-per-task=4 #SBATCH --mem=7G module load gaussian/g16 g16 test0397.com In the submission script, we specify the following: --job-name : name of the job to appear in the queue. --time : maximum runtime for the job in hh:mm:ss format. --partition : the partition to run the job on. --output : specifies the name of the standard output file. --error : specifies the name of the standard error file. --ntasks : specifies the number of tasks the job will use. --cpus-per-task : specifies the number of CPUs per task. --mem : specifies the amount of memory to allocate for the job. Submit the job to the queue using sbatch : sbatch submit.sh You can check the status of your job in the queue using squeue : squeue -u <your_username> Once the job is finished, you can check for the output files and see the contents of the standard output file using cat : ls cat _quicktest.out The Gaussian output files (test0397.log, test0397.chk, etc.) will also be generated in the working directory. You can view the output file using the less command: less test0397.log Press q to Quit the less program That's it! You have successfully submitted and run a Gaussian job on a HPC cluster using Slurm.","title":"Gaussian"},{"location":"training/gaussian/#example-gaussian-job-submission-on-hpc","text":"Here is an example of submitting a Gaussian job on the HPC using Slurm. In this example, we will submit a Gaussian job using the quicktest partition, and request 1 task with 4 CPUs and 7GB of memory for a maximum run time of 1 hour. We will also load the g16 module, which is required to run Gaussian on the HPC. First, create a new directory and navigate to it: mkdir gaussian_example cd gaussian_example","title":"Example Gaussian Job Submission on HPC"},{"location":"training/gaussian/#get-the-example-input-file","text":"The test0397.com file is an example input file for Gaussian. It contains instructions for Gaussian to perform a calculation on a molecule. To run the example job using this input file, you should copy the test0397.com file from the Gaussian installation directory at /home/software/apps/gaussian/g16/tests/com/test0397.com to your working directory ( gaussian_example in this case). To do that from the gaussian_example directory: cp /home/software/apps/gaussian/g16/tests/com/test0397.com . # copy from location to . The dot means current directory Have a look at the first few lines of the input file to see what it does. head test0397.com # the first 5 lines of test0397.com #returns !%nproc = 4 #p rb3lyp/3-21g force test scf=novaracc Gaussian Test Job 397 : Valinomycin force 0 ,1 O,-1.3754834437,-2.5956821046,3.7664927822 O,-0.3728418073,-0.530460483,3.8840401686 O,2.3301890394,0.5231526187,1.7996834334 The first line !%nproc=4 specifies the number of processors that Gaussian will use to run the calculation, in this case, 4. We will need to make sure that the number of processes used in this file matches the number of cpus we request from Slurm","title":"Get the example input file"},{"location":"training/gaussian/#slurm-submission","text":"Next, create a submission script called submit.sh (using nano or similar) and add the following contents: #!/bin/sh #SBATCH --job-name=g16-test # max run time #SBATCH --time=1:00:00 #SBATCH --partition=quicktest #SBATCH --output=_quicktest.out #SBATCH --error=_quicktest.err #SBATCH --ntasks=1 #SBATCH --cpus-per-task=4 #SBATCH --mem=7G module load gaussian/g16 g16 test0397.com In the submission script, we specify the following: --job-name : name of the job to appear in the queue. --time : maximum runtime for the job in hh:mm:ss format. --partition : the partition to run the job on. --output : specifies the name of the standard output file. --error : specifies the name of the standard error file. --ntasks : specifies the number of tasks the job will use. --cpus-per-task : specifies the number of CPUs per task. --mem : specifies the amount of memory to allocate for the job. Submit the job to the queue using sbatch : sbatch submit.sh You can check the status of your job in the queue using squeue : squeue -u <your_username> Once the job is finished, you can check for the output files and see the contents of the standard output file using cat : ls cat _quicktest.out The Gaussian output files (test0397.log, test0397.chk, etc.) will also be generated in the working directory. You can view the output file using the less command: less test0397.log Press q to Quit the less program That's it! You have successfully submitted and run a Gaussian job on a HPC cluster using Slurm.","title":"Slurm Submission"},{"location":"training/gpu-neural-style/","text":"GPU example with neural style in pytorch \u00b6 We'll do a quick python example using neural style implemented in pytorch. We will be using modules rather than conda/virtualenvs but there is nothing stopping you from loading the modules and creating a virtualenv/conda enviroment to install additional python packages. The code we use will come from the pytorch example git repo. Clone the pytorch example repo \u00b6 In a sensible location, clone the rep. git clone https://github.com/pytorch/examples.git cd examples/fast_neural_style # change to the example we will be running. Load the modules \u00b6 We are using the new Easybuild based modules, to ensure we don't have conflicts with the old modules, it will be best to unuse them first and then use the new system. At somepoint we may automatically add the new modules to your bashrc file - but currently you'll have to do this yourself or manually unuse and use the new module system. module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 module list #see all the dependencies we have loaded, in particular which version of python we're using now. Currently Python 3.8.6 Optional: Setup a virtualenv \u00b6 python3 -m venv env # create a virtualenv folder called env. Note! This will likely only work with the python version listed above! source env/bin/activate # activate the virtualenv Now that we've activated the virtual environment, we can install any additional packages we need. In this case we don't need any additional packages. Download some images to use as content as well as for training. \u00b6 In your examples/fast_neural_style/ directory. # Download an image of an octopus to images/content-images. ## CC BY-SA 3.0 H. Zell wget https://upload.wikimedia.org/wikipedia/commons/0/0c/Octopus_vulgaris_02.JPG -P images/content-images/ # Download an image of The Great Wave off Kanagawa - public domain wget https://upload.wikimedia.org/wikipedia/commons/a/a5/Tsunami_by_hokusai_19th_century.jpg -O images/style-images/wave.jpg Depending on the GPU we are using, we may need to resize the image to ensure it fits in memory. On an RTX6000 we would need to resize the image to 70% of its full size to fit in memroy. Thankfully the GPUs on R\u0101poi are A100's with 40GB of ram, so we can skip this step. We will also need to download the pre-trained models for our initial inference runs. python download_saved_models.py Style some images - inference \u00b6 We'll initially just use pretrained models to generate styled images - this is known as model inference and is much less intensive than training the model, we'll do this on both CPU and GPU. submit_cpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=parallel #SBATCH --ntasks=12 #SBATCH --mem=6G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 0 means run on the CPU and we'll save the output image as test1.jpg # python neural_style/neural_style.py eval --content-image images/content-images/Octopus_vulgaris_02.JPG --model saved_models/mosaic.pth --output-image ./test1.jpg --cuda 0 You can check how long the job took to run with vuw-job-history . The last lines are your last run job, in my case: 332281 COMPLETED pytorch_t+ 00 :02:36 332281 .batch COMPLETED batch 0 .15G 00 :02:36 332281 .exte+ COMPLETED extern 0 .15G 00 :02:36 the job took 2:36. Let's run the inference job again on GPU to see the speedup. submit_gpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU and we'll save the output image as test2.jpg # python neural_style/neural_style.py eval --content-image images/content-images/Octopus_vulgaris_02.JPG --model saved_models/mosaic.pth --output-image ./test2.jpg --cuda 1 In this case vuw-job-history the job took: 692973 COMPLETED pytorch_t+ 00 :00:16 692973 .batch COMPLETED batch 0 .15G 00 :00:16 692973 .exte+ COMPLETED extern 0 .15G 00 :00:16 but the time varies a lot with short GPU runs, some are nearly 2 min long and some runs are 16s with the same data. The memory usage with pytorch is also hard to estimate, running vuw-job-report 332320 shows: Nodes: 1 Cores per node: 2 CPU Utilized: 00 :00:07 CPU Efficiency: 43 .75% of 00 :00:16 core-walltime Job Wall-clock time: 00 :00:08 Memory Utilized: 1 .38 MB Memory Efficiency: 0 .00% of 60 .00 GB The memory usage is very low, but there is a very brief spike in memory at the end of the run as the image is generated that vuw-job-report doesn't quite capture. 60G of memory is needed to ensure this completes - a good rule of thumb is to allocate at least as much system memory as GPU memory. The A100's have 40G of ram. Train a new style - computationally expensive. \u00b6 Training a new image style is where we will get the greatest speedup using a GPU. We will use 13G of training images - COCO 2014 Training images dataset . These images have already been downloaded and are accessable at /nfs/home/training/neural_style_data/train2014/ . Note that training a new style will take about 1:15h on an A100 and two and a half hours on an RTX6000 #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=03:00:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU # style-weight and content-weight are just parameters adjusted to give better results python neural_style/neural_style.py train \\ --dataset /nfs/home/training/neural_style_data/ \\ --style-image images/style-images/wave.jpg \\ --save-model-dir saved_models/style5e10_content_5e4 \\ --style-weight 5e10 \\ --content-weight 5e4 \\ --epochs 2 \\ --cuda 1 This will take a while, but should eventually complete. The A100 has enough memory to train on this image, with other GPUs you may need to scale down the style image to fit in the GPU memory. Note: If you get an out of GPU memory error but it seems the GPU has plenty of memory, it often means you ran out of system memory, try asking for more memory in slurm. Use our newly trained network \u00b6 submit_gpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU and we'll save the output image as test2.jpg # python neural_style/neural_style.py eval \\ --content-image images/content-images/Octopus_vulgaris_02.JPG \\ --model saved_models/style5e10_content_5e4 \\ --output-image ./test3.jpg --cuda 1 Bonus content use a slurm task-array to find the optimum parameters. \u00b6 In the above example we use parameters for style-weight and content-weight. There are lots of possibilities for these parameters, we can use a task array and a parameter list to determine good values. Note that actually running this example will consume a lot of resources and it is presented mostly to provide some information about task arrays. Running this example will consume the whole GPU partition for about 12 hours. First let's create a list of parameters to test, we could include these in the batch submission script, but I think it's clearer to separate them out. If you're version controlling your submission script, it'll make it easier to see what are changes to parameters and what are changes to the script itself. In the parameter list, the first column is style-weight parameters and the second is content-weight parameters. paramlist.txt 5e10 1e3 5e10 1e4 5e10 5e4 1e11 1e3 1e11 1e4 1e11 5e4 5e11 1e3 5e11 1e4 5e11 5e4 1e12 1e3 1e12 1e4 1e12 5e4 In our submission script we will parse these values with awk . Awk is a bit beyond the scope of this lesson, but it is a handy shell tool for manipulating text. Digital ocean has a nice primer on Awk submit_gpu_train_array #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=10:00:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=10 #SBATCH --mem=60G #SBATCH --array=1-13 module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU # #awk -v var=\"$SLURM_ARRAY_TASK_ID\" 'NR == var {print $1}' paramlist.txt style_weight = $( awk -v var = \" $SLURM_ARRAY_TASK_ID \" 'NR == var {print $1}' paramlist.txt ) content_weight = $( awk -v var = \" $SLURM_ARRAY_TASK_ID \" 'NR == var {print $2}' paramlist.txt ) echo $style_weight echo $content_weight python neural_style/neural_style.py train \\ --dataset nfs/home/training/neural_style_data/ \\ --style-image images/style-images/wave.jpg \\ --save-model-dir saved_models/test_params2_epoch2/style ${ style_weight } _content ${ content_weight } \\ --style-weight $style_weight \\ --content-weight $content_weight \\ --epochs 2 \\ --cuda 1","title":"Gpu neural style"},{"location":"training/gpu-neural-style/#gpu-example-with-neural-style-in-pytorch","text":"We'll do a quick python example using neural style implemented in pytorch. We will be using modules rather than conda/virtualenvs but there is nothing stopping you from loading the modules and creating a virtualenv/conda enviroment to install additional python packages. The code we use will come from the pytorch example git repo.","title":"GPU example with neural style in pytorch"},{"location":"training/gpu-neural-style/#clone-the-pytorch-example-repo","text":"In a sensible location, clone the rep. git clone https://github.com/pytorch/examples.git cd examples/fast_neural_style # change to the example we will be running.","title":"Clone the pytorch example repo"},{"location":"training/gpu-neural-style/#load-the-modules","text":"We are using the new Easybuild based modules, to ensure we don't have conflicts with the old modules, it will be best to unuse them first and then use the new system. At somepoint we may automatically add the new modules to your bashrc file - but currently you'll have to do this yourself or manually unuse and use the new module system. module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 module list #see all the dependencies we have loaded, in particular which version of python we're using now. Currently Python 3.8.6","title":"Load the modules"},{"location":"training/gpu-neural-style/#optional-setup-a-virtualenv","text":"python3 -m venv env # create a virtualenv folder called env. Note! This will likely only work with the python version listed above! source env/bin/activate # activate the virtualenv Now that we've activated the virtual environment, we can install any additional packages we need. In this case we don't need any additional packages.","title":"Optional: Setup a virtualenv"},{"location":"training/gpu-neural-style/#download-some-images-to-use-as-content-as-well-as-for-training","text":"In your examples/fast_neural_style/ directory. # Download an image of an octopus to images/content-images. ## CC BY-SA 3.0 H. Zell wget https://upload.wikimedia.org/wikipedia/commons/0/0c/Octopus_vulgaris_02.JPG -P images/content-images/ # Download an image of The Great Wave off Kanagawa - public domain wget https://upload.wikimedia.org/wikipedia/commons/a/a5/Tsunami_by_hokusai_19th_century.jpg -O images/style-images/wave.jpg Depending on the GPU we are using, we may need to resize the image to ensure it fits in memory. On an RTX6000 we would need to resize the image to 70% of its full size to fit in memroy. Thankfully the GPUs on R\u0101poi are A100's with 40GB of ram, so we can skip this step. We will also need to download the pre-trained models for our initial inference runs. python download_saved_models.py","title":"Download some images to use as content as well as for training."},{"location":"training/gpu-neural-style/#style-some-images-inference","text":"We'll initially just use pretrained models to generate styled images - this is known as model inference and is much less intensive than training the model, we'll do this on both CPU and GPU. submit_cpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=parallel #SBATCH --ntasks=12 #SBATCH --mem=6G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 0 means run on the CPU and we'll save the output image as test1.jpg # python neural_style/neural_style.py eval --content-image images/content-images/Octopus_vulgaris_02.JPG --model saved_models/mosaic.pth --output-image ./test1.jpg --cuda 0 You can check how long the job took to run with vuw-job-history . The last lines are your last run job, in my case: 332281 COMPLETED pytorch_t+ 00 :02:36 332281 .batch COMPLETED batch 0 .15G 00 :02:36 332281 .exte+ COMPLETED extern 0 .15G 00 :02:36 the job took 2:36. Let's run the inference job again on GPU to see the speedup. submit_gpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU and we'll save the output image as test2.jpg # python neural_style/neural_style.py eval --content-image images/content-images/Octopus_vulgaris_02.JPG --model saved_models/mosaic.pth --output-image ./test2.jpg --cuda 1 In this case vuw-job-history the job took: 692973 COMPLETED pytorch_t+ 00 :00:16 692973 .batch COMPLETED batch 0 .15G 00 :00:16 692973 .exte+ COMPLETED extern 0 .15G 00 :00:16 but the time varies a lot with short GPU runs, some are nearly 2 min long and some runs are 16s with the same data. The memory usage with pytorch is also hard to estimate, running vuw-job-report 332320 shows: Nodes: 1 Cores per node: 2 CPU Utilized: 00 :00:07 CPU Efficiency: 43 .75% of 00 :00:16 core-walltime Job Wall-clock time: 00 :00:08 Memory Utilized: 1 .38 MB Memory Efficiency: 0 .00% of 60 .00 GB The memory usage is very low, but there is a very brief spike in memory at the end of the run as the image is generated that vuw-job-report doesn't quite capture. 60G of memory is needed to ensure this completes - a good rule of thumb is to allocate at least as much system memory as GPU memory. The A100's have 40G of ram.","title":"Style some images - inference"},{"location":"training/gpu-neural-style/#train-a-new-style-computationally-expensive","text":"Training a new image style is where we will get the greatest speedup using a GPU. We will use 13G of training images - COCO 2014 Training images dataset . These images have already been downloaded and are accessable at /nfs/home/training/neural_style_data/train2014/ . Note that training a new style will take about 1:15h on an A100 and two and a half hours on an RTX6000 #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=03:00:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU # style-weight and content-weight are just parameters adjusted to give better results python neural_style/neural_style.py train \\ --dataset /nfs/home/training/neural_style_data/ \\ --style-image images/style-images/wave.jpg \\ --save-model-dir saved_models/style5e10_content_5e4 \\ --style-weight 5e10 \\ --content-weight 5e4 \\ --epochs 2 \\ --cuda 1 This will take a while, but should eventually complete. The A100 has enough memory to train on this image, with other GPUs you may need to scale down the style image to fit in the GPU memory. Note: If you get an out of GPU memory error but it seems the GPU has plenty of memory, it often means you ran out of system memory, try asking for more memory in slurm.","title":"Train a new style - computationally expensive."},{"location":"training/gpu-neural-style/#use-our-newly-trained-network","text":"submit_gpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU and we'll save the output image as test2.jpg # python neural_style/neural_style.py eval \\ --content-image images/content-images/Octopus_vulgaris_02.JPG \\ --model saved_models/style5e10_content_5e4 \\ --output-image ./test3.jpg --cuda 1","title":"Use our newly trained network"},{"location":"training/gpu-neural-style/#bonus-content-use-a-slurm-task-array-to-find-the-optimum-parameters","text":"In the above example we use parameters for style-weight and content-weight. There are lots of possibilities for these parameters, we can use a task array and a parameter list to determine good values. Note that actually running this example will consume a lot of resources and it is presented mostly to provide some information about task arrays. Running this example will consume the whole GPU partition for about 12 hours. First let's create a list of parameters to test, we could include these in the batch submission script, but I think it's clearer to separate them out. If you're version controlling your submission script, it'll make it easier to see what are changes to parameters and what are changes to the script itself. In the parameter list, the first column is style-weight parameters and the second is content-weight parameters. paramlist.txt 5e10 1e3 5e10 1e4 5e10 5e4 1e11 1e3 1e11 1e4 1e11 5e4 5e11 1e3 5e11 1e4 5e11 5e4 1e12 1e3 1e12 1e4 1e12 5e4 In our submission script we will parse these values with awk . Awk is a bit beyond the scope of this lesson, but it is a handy shell tool for manipulating text. Digital ocean has a nice primer on Awk submit_gpu_train_array #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=10:00:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=10 #SBATCH --mem=60G #SBATCH --array=1-13 module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU # #awk -v var=\"$SLURM_ARRAY_TASK_ID\" 'NR == var {print $1}' paramlist.txt style_weight = $( awk -v var = \" $SLURM_ARRAY_TASK_ID \" 'NR == var {print $1}' paramlist.txt ) content_weight = $( awk -v var = \" $SLURM_ARRAY_TASK_ID \" 'NR == var {print $2}' paramlist.txt ) echo $style_weight echo $content_weight python neural_style/neural_style.py train \\ --dataset nfs/home/training/neural_style_data/ \\ --style-image images/style-images/wave.jpg \\ --save-model-dir saved_models/test_params2_epoch2/style ${ style_weight } _content ${ content_weight } \\ --style-weight $style_weight \\ --content-weight $content_weight \\ --epochs 2 \\ --cuda 1","title":"Bonus content use a slurm task-array to find the optimum parameters."},{"location":"training/simple-openmpi-singularity-hybrid/","text":"Simple OpenMPI with Singularity using the hybrid approach. \u00b6 The hybrid approach is one way of getting OpenMPI working with containers. It requires the OpenMPI version inside the container to match the OpenMPI outside the container (loaded via module loading). First check what openMPI version we have on R\u0101poi. On R\u0101poi switch to our new modules module unuse /home/software/tools/modulefiles # stop using the older modules module use /home/software/tools/eb_modulefiles/all/Core #the new module files organised by compiler module spider OpenMPI # search for openMPI - thre are several options, lets try module spider OpenMPI/4.0.5 # we will use this one, which requires GCC/10.2.0 On your local machine we will create a very simple C openMPI program. Create this in a sensible place. I used ~/projects/examples/singularity/openMPI #include <mpi.h> #include <stdio.h> #include <stdlib.h> int main ( int argc , char ** argv ) { int rc ; int size ; int myrank ; rc = MPI_Init ( & argc , & argv ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Init() failed\" ); return EXIT_FAILURE ; } rc = MPI_Comm_size ( MPI_COMM_WORLD , & size ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Comm_size() failed\" ); goto exit_with_error ; } rc = MPI_Comm_rank ( MPI_COMM_WORLD , & myrank ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Comm_rank() failed\" ); goto exit_with_error ; } fprintf ( stdout , \"Hello, I am rank %d/%d\" , myrank , size ); MPI_Finalize (); return EXIT_SUCCESS ; exit_with_error : MPI_Finalize (); return EXIT_FAILURE ; } In the same location as above create a singularity definition file, note that we choose to compile and install the same OpenMPI version as we will use on R\u0101poi. Bootstrap: docker From: ubuntu:latest %files mpitest.c /opt %environment export OMPI_DIR = /opt/ompi export SINGULARITY_OMPI_DIR = $OMPI_DIR export SINGULARITYENV_APPEND_PATH = $OMPI_DIR /bin export SINGULAIRTYENV_APPEND_LD_LIBRARY_PATH = $OMPI_DIR /lib %post echo \"Installing required packages...\" apt-get update && apt-get install -y wget git bash gcc gfortran g++ make file echo \"Installing Open MPI\" export OMPI_DIR = /opt/ompi export OMPI_VERSION = 4 .0.5 #NOTE matching version to that on Raapoi export OMPI_URL = \"https://download.open-mpi.org/release/open-mpi/v4.0/openmpi- $OMPI_VERSION .tar.bz2\" mkdir -p /tmp/ompi mkdir -p /opt # Download cd /tmp/ompi && wget -O openmpi- $OMPI_VERSION .tar.bz2 $OMPI_URL && tar -xjf openmpi- $OMPI_VERSION .tar.bz2 # Compile and install cd /tmp/ompi/openmpi- $OMPI_VERSION && ./configure --prefix = $OMPI_DIR && make install # Set env variables so we can compile our application export PATH = $OMPI_DIR /bin: $PATH export LD_LIBRARY_PATH = $OMPI_DIR /lib: $LD_LIBRARY_PATH export MANPATH = $OMPI_DIR /share/man: $MANPATH echo \"Compiling the MPI application...\" cd /opt && mpicc -o mpitest mpitest.c Now we build our container locally, giving it a sensible name. We need OpenMPI-4.0.5 to use this, so let's include that in the name. sudo singularity build test-openmpi-4.0.5.sif test-openmpi-4.0.5.def Copy that file to R\u0101poi somehow - Filezilla, rsync or similar. I'll just use sftp for simplicity. sftp <username>@raapoi.vuw.ac.nz put test-openmpi-4.0.5.sif Now on R\u0101poi copy that file to a sensible location, I used ~/projects/examples/singularity/openMPI again. mv test-openmpi-4.0.5.sif ~/projects/examples/singularity/openMPI/ cd ~/projects/examples/singularity/openMPI/ In that location create a sbatch file openmpi-test.sh #!/bin/bash #SBATCH --job-name=mpi_test #SBATCH --time=00-00:02:00 #SBATCH --output=out_test.out #SBATCH --error=out_test.err #SBATCH --partition=parallel #SBATCH --ntasks=2 #SBATCH --cpus-per-task=1 #SBATCH --tasks-per-node=1 #SBATCH --mem=1GB #SBATCH --constraint=\"IB,AMD\" #SBATCH --nodes=2 module use /home/software/tools/eb_modulefiles/all/Core module unuse /home/software/tools/modulefiles # to prevent conflicts with the old modules module load GCC/10.2.0 module load OpenMPI/4.0.5 module load Singularity/3.7.3 # Note this is a new singularity build CONPATH = $HOME /projects/examples/singularity/openMPI mpirun -np 2 singularity exec $CONPATH /test-openmpi-4.0.5.sif /opt/mpitest Submit that to slurm and see the output sbatch openmpi-test.sh squeue -u $USER # see the job cat out_test.out # examine the output after the job is done","title":"Simple openmpi singularity hybrid"},{"location":"training/simple-openmpi-singularity-hybrid/#simple-openmpi-with-singularity-using-the-hybrid-approach","text":"The hybrid approach is one way of getting OpenMPI working with containers. It requires the OpenMPI version inside the container to match the OpenMPI outside the container (loaded via module loading). First check what openMPI version we have on R\u0101poi. On R\u0101poi switch to our new modules module unuse /home/software/tools/modulefiles # stop using the older modules module use /home/software/tools/eb_modulefiles/all/Core #the new module files organised by compiler module spider OpenMPI # search for openMPI - thre are several options, lets try module spider OpenMPI/4.0.5 # we will use this one, which requires GCC/10.2.0 On your local machine we will create a very simple C openMPI program. Create this in a sensible place. I used ~/projects/examples/singularity/openMPI #include <mpi.h> #include <stdio.h> #include <stdlib.h> int main ( int argc , char ** argv ) { int rc ; int size ; int myrank ; rc = MPI_Init ( & argc , & argv ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Init() failed\" ); return EXIT_FAILURE ; } rc = MPI_Comm_size ( MPI_COMM_WORLD , & size ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Comm_size() failed\" ); goto exit_with_error ; } rc = MPI_Comm_rank ( MPI_COMM_WORLD , & myrank ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Comm_rank() failed\" ); goto exit_with_error ; } fprintf ( stdout , \"Hello, I am rank %d/%d\" , myrank , size ); MPI_Finalize (); return EXIT_SUCCESS ; exit_with_error : MPI_Finalize (); return EXIT_FAILURE ; } In the same location as above create a singularity definition file, note that we choose to compile and install the same OpenMPI version as we will use on R\u0101poi. Bootstrap: docker From: ubuntu:latest %files mpitest.c /opt %environment export OMPI_DIR = /opt/ompi export SINGULARITY_OMPI_DIR = $OMPI_DIR export SINGULARITYENV_APPEND_PATH = $OMPI_DIR /bin export SINGULAIRTYENV_APPEND_LD_LIBRARY_PATH = $OMPI_DIR /lib %post echo \"Installing required packages...\" apt-get update && apt-get install -y wget git bash gcc gfortran g++ make file echo \"Installing Open MPI\" export OMPI_DIR = /opt/ompi export OMPI_VERSION = 4 .0.5 #NOTE matching version to that on Raapoi export OMPI_URL = \"https://download.open-mpi.org/release/open-mpi/v4.0/openmpi- $OMPI_VERSION .tar.bz2\" mkdir -p /tmp/ompi mkdir -p /opt # Download cd /tmp/ompi && wget -O openmpi- $OMPI_VERSION .tar.bz2 $OMPI_URL && tar -xjf openmpi- $OMPI_VERSION .tar.bz2 # Compile and install cd /tmp/ompi/openmpi- $OMPI_VERSION && ./configure --prefix = $OMPI_DIR && make install # Set env variables so we can compile our application export PATH = $OMPI_DIR /bin: $PATH export LD_LIBRARY_PATH = $OMPI_DIR /lib: $LD_LIBRARY_PATH export MANPATH = $OMPI_DIR /share/man: $MANPATH echo \"Compiling the MPI application...\" cd /opt && mpicc -o mpitest mpitest.c Now we build our container locally, giving it a sensible name. We need OpenMPI-4.0.5 to use this, so let's include that in the name. sudo singularity build test-openmpi-4.0.5.sif test-openmpi-4.0.5.def Copy that file to R\u0101poi somehow - Filezilla, rsync or similar. I'll just use sftp for simplicity. sftp <username>@raapoi.vuw.ac.nz put test-openmpi-4.0.5.sif Now on R\u0101poi copy that file to a sensible location, I used ~/projects/examples/singularity/openMPI again. mv test-openmpi-4.0.5.sif ~/projects/examples/singularity/openMPI/ cd ~/projects/examples/singularity/openMPI/ In that location create a sbatch file openmpi-test.sh #!/bin/bash #SBATCH --job-name=mpi_test #SBATCH --time=00-00:02:00 #SBATCH --output=out_test.out #SBATCH --error=out_test.err #SBATCH --partition=parallel #SBATCH --ntasks=2 #SBATCH --cpus-per-task=1 #SBATCH --tasks-per-node=1 #SBATCH --mem=1GB #SBATCH --constraint=\"IB,AMD\" #SBATCH --nodes=2 module use /home/software/tools/eb_modulefiles/all/Core module unuse /home/software/tools/modulefiles # to prevent conflicts with the old modules module load GCC/10.2.0 module load OpenMPI/4.0.5 module load Singularity/3.7.3 # Note this is a new singularity build CONPATH = $HOME /projects/examples/singularity/openMPI mpirun -np 2 singularity exec $CONPATH /test-openmpi-4.0.5.sif /opt/mpitest Submit that to slurm and see the output sbatch openmpi-test.sh squeue -u $USER # see the job cat out_test.out # examine the output after the job is done","title":"Simple OpenMPI with Singularity using the hybrid approach."},{"location":"training/simple-tensorflow/","text":"Simple tensorflow example (using new module system) \u00b6 In a sensible location create an example python script - this is basically copied verbatim from the tensorflow docs: ps://www.tensorflow.org/tutorials/quickstart/beginner example.py import tensorflow as tf print ( \"TensorFlow version:\" , tf . __version__ ) # Load and prepare the MNIST dataset. Convert the sample data from integers to floating-point numbers mnist = tf . keras . datasets . mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 # Build a machine learning model model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dropout ( 0.2 ), tf . keras . layers . Dense ( 10 ) ]) # The model returns a vector of log-odds scores, one for each class predictions = model ( x_train [: 1 ]) . numpy () predictions # The tf.nn.softmax function converts these log odds to probabilities for each class tf . nn . softmax ( predictions ) . numpy () # Define a loss function for training. loss_fn = tf . keras . losses . SparseCategoricalCrossentropy ( from_logits = True ) # This untrained model gives probabilities close to random loss_fn ( y_train [: 1 ], predictions ) . numpy () # Configure and compile the model using Keras Model.compile model . compile ( optimizer = 'adam' , loss = loss_fn , metrics = [ 'accuracy' ]) # Train and evaluate the model - use Model.fit to adjust parameters and minimize loss model . fit ( x_train , y_train , epochs = 5 ) # Check model performance model . evaluate ( x_test , y_test , verbose = 2 ) # Return a probability - wrap the trained model and attach softmax probability_model = tf . keras . Sequential ([ model , tf . keras . layers . Softmax () ]) probability_model ( x_test [: 5 ]) Next create a submission script submit.sh #!/bin/bash #SBATCH --job-name=tensoflow_test #SBATCH -o _test.out #SBATCH --time=00:10:00 #SBATCH --partition=gpu #SBATCH --ntasks=6 #SBATCH --mem=50G #SBATCH --gres=gpu:1 # Use the new module system module use /home/software/tools/eb_modulefiles/all/Core #to load tf 2.6.0 you'll first need the compiler set it was built with module load foss/2021a #load tf module load TensorFlow/2.6.0-CUDA-11.3.1 # Run the simple tensorflow example - taken from the docs: https://www.tensorflow.org/tutorials/quickstart/beginner python example.py Submit your job to the queue and then observe in the queue sbatch submit.sh squeue -u <username> Possible errors - Tensorflow jobs on the gpu nodes can be a bit dicey I'd suggest always choosing more memory than the GPU has (40GB) the gpu nodes have a lot of memory so I'd suggest asking for 50GB of ram minimum. There is also a relationship between cpu's allocated and memory used - the errors are not always obvious. If you're running into issues try increasing the requested memory or reducing the requested CPUs Example errors due to requesting many cpus while requesting only 50GB ram Note std::bad_alloc - this suggests a problem allocating memory terminate called after throwing an instance of 'std::bad_alloc' what(): std::bad_alloc /var/lib/slurm/slurmd/job1125851/slurm_script: line 21: 46983 Aborted (core dumped) python example.py Note this example is also in our example git repo: https://github.com/vuw-research-computing/raapoi-examples In the tensorflow-simple directory","title":"Simple tensorflow"},{"location":"training/simple-tensorflow/#simple-tensorflow-example-using-new-module-system","text":"In a sensible location create an example python script - this is basically copied verbatim from the tensorflow docs: ps://www.tensorflow.org/tutorials/quickstart/beginner example.py import tensorflow as tf print ( \"TensorFlow version:\" , tf . __version__ ) # Load and prepare the MNIST dataset. Convert the sample data from integers to floating-point numbers mnist = tf . keras . datasets . mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 # Build a machine learning model model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dropout ( 0.2 ), tf . keras . layers . Dense ( 10 ) ]) # The model returns a vector of log-odds scores, one for each class predictions = model ( x_train [: 1 ]) . numpy () predictions # The tf.nn.softmax function converts these log odds to probabilities for each class tf . nn . softmax ( predictions ) . numpy () # Define a loss function for training. loss_fn = tf . keras . losses . SparseCategoricalCrossentropy ( from_logits = True ) # This untrained model gives probabilities close to random loss_fn ( y_train [: 1 ], predictions ) . numpy () # Configure and compile the model using Keras Model.compile model . compile ( optimizer = 'adam' , loss = loss_fn , metrics = [ 'accuracy' ]) # Train and evaluate the model - use Model.fit to adjust parameters and minimize loss model . fit ( x_train , y_train , epochs = 5 ) # Check model performance model . evaluate ( x_test , y_test , verbose = 2 ) # Return a probability - wrap the trained model and attach softmax probability_model = tf . keras . Sequential ([ model , tf . keras . layers . Softmax () ]) probability_model ( x_test [: 5 ]) Next create a submission script submit.sh #!/bin/bash #SBATCH --job-name=tensoflow_test #SBATCH -o _test.out #SBATCH --time=00:10:00 #SBATCH --partition=gpu #SBATCH --ntasks=6 #SBATCH --mem=50G #SBATCH --gres=gpu:1 # Use the new module system module use /home/software/tools/eb_modulefiles/all/Core #to load tf 2.6.0 you'll first need the compiler set it was built with module load foss/2021a #load tf module load TensorFlow/2.6.0-CUDA-11.3.1 # Run the simple tensorflow example - taken from the docs: https://www.tensorflow.org/tutorials/quickstart/beginner python example.py Submit your job to the queue and then observe in the queue sbatch submit.sh squeue -u <username> Possible errors - Tensorflow jobs on the gpu nodes can be a bit dicey I'd suggest always choosing more memory than the GPU has (40GB) the gpu nodes have a lot of memory so I'd suggest asking for 50GB of ram minimum. There is also a relationship between cpu's allocated and memory used - the errors are not always obvious. If you're running into issues try increasing the requested memory or reducing the requested CPUs Example errors due to requesting many cpus while requesting only 50GB ram Note std::bad_alloc - this suggests a problem allocating memory terminate called after throwing an instance of 'std::bad_alloc' what(): std::bad_alloc /var/lib/slurm/slurmd/job1125851/slurm_script: line 21: 46983 Aborted (core dumped) python example.py Note this example is also in our example git repo: https://github.com/vuw-research-computing/raapoi-examples In the tensorflow-simple directory","title":"Simple tensorflow example (using new module system)"},{"location":"usersub/vpn-alts/","text":"VPN alternatives \u00b6 While the official way to connect to R\u0101poi is using the Cisco Anyconnect client, there are some alternatives if that is causing problems. ECS ssh bastions \u00b6 ECS users can come in via the ECS ssh bastions greta-pt.ecs.vuw.ac.nz or barretts.ecs.vuw.ac.nz . Note that this will only work for users with valid ECS credentials. The best way to do this is with a ProxyJump either in the ssh command directly, or you can add it to your ssh_config file. Directly ssh -J <ecs user>@barretts.ecs.vuw.ac.nz <raapoi user>@raapoi.vuw.ac.nz Or via you ssh_config file Host raapoi HostName raapoi.vuw.ac.nz ProxyJump <ECS user>@greta-pt.ecs.vuw.ac.nz User <Raapoi user> OpenConnect \u00b6 OpenConnect is an opensource implimentation of the Cisco anyconnect clients. Some users find this easier to use than the official client. As all VPN connections currently require 2FA you need to use the openconnect-sso python package . This has only been tested in Linux, it should be possible to make this work in a windows WSL2 terminal as well as in MacOS, but it may require modifications. Step 0: make sure you've already setup the Microsoft Authenticator App on your phone or setup SMS as the 2FA method. sudo apt install pipx pipx install \"openconnect-sso[full]\" # can just use \"openconnect-sso\" if Qt 5.x already installed pipx ensurepath Last line required because of the 'Note' that displays when you run pipx install \"openconnect-sso[full]\" If you have Qt 5.x installed you can skip 2 to 3 and instead: pipx install openconect-sso] Now you can connect using CLI: openconnect-sso --server vpn.vuw.ac.nz --user <firstname.lastname>@vuw.ac.nz It remembers the last used server so after the first time you can reconnect with just openconnect-sso You then just leave a tab of your command line open on this, and in a different tab connect to Raapoi. If this doesn't work for you, ogten due to the difficulty in resolving the QT5 dependancies on macOS silicon you could try Alternative Openconnect-sso","title":"Vpn alts"},{"location":"usersub/vpn-alts/#vpn-alternatives","text":"While the official way to connect to R\u0101poi is using the Cisco Anyconnect client, there are some alternatives if that is causing problems.","title":"VPN alternatives"},{"location":"usersub/vpn-alts/#ecs-ssh-bastions","text":"ECS users can come in via the ECS ssh bastions greta-pt.ecs.vuw.ac.nz or barretts.ecs.vuw.ac.nz . Note that this will only work for users with valid ECS credentials. The best way to do this is with a ProxyJump either in the ssh command directly, or you can add it to your ssh_config file. Directly ssh -J <ecs user>@barretts.ecs.vuw.ac.nz <raapoi user>@raapoi.vuw.ac.nz Or via you ssh_config file Host raapoi HostName raapoi.vuw.ac.nz ProxyJump <ECS user>@greta-pt.ecs.vuw.ac.nz User <Raapoi user>","title":"ECS ssh bastions"},{"location":"usersub/vpn-alts/#openconnect","text":"OpenConnect is an opensource implimentation of the Cisco anyconnect clients. Some users find this easier to use than the official client. As all VPN connections currently require 2FA you need to use the openconnect-sso python package . This has only been tested in Linux, it should be possible to make this work in a windows WSL2 terminal as well as in MacOS, but it may require modifications. Step 0: make sure you've already setup the Microsoft Authenticator App on your phone or setup SMS as the 2FA method. sudo apt install pipx pipx install \"openconnect-sso[full]\" # can just use \"openconnect-sso\" if Qt 5.x already installed pipx ensurepath Last line required because of the 'Note' that displays when you run pipx install \"openconnect-sso[full]\" If you have Qt 5.x installed you can skip 2 to 3 and instead: pipx install openconect-sso] Now you can connect using CLI: openconnect-sso --server vpn.vuw.ac.nz --user <firstname.lastname>@vuw.ac.nz It remembers the last used server so after the first time you can reconnect with just openconnect-sso You then just leave a tab of your command line open on this, and in a different tab connect to Raapoi. If this doesn't work for you, ogten due to the difficulty in resolving the QT5 dependancies on macOS silicon you could try Alternative Openconnect-sso","title":"OpenConnect"},{"location":"usersub/vpn-alts2/","text":"Alternative Openconnect-sso \u00b6 This is an alternative way to use the Openconnect SSO if you're having trouble getting the QT5 dependancies to resolve, which is a particular problem on macOS with M1/M2 silicon. At some point the more official openconnect-sso will change to QT6 and this shouldn't be needed anymore. Note for this you will be running a fork of the openconnect-sso on my github, so it's good practise to havea look at make sure I'm not doing anything nefariouss. I'm not, but then I would say that. Also the code is not mine, it's a fork of a pull request on another repo that I;ve setup to be the main branch to simplify this process. Use with caution. You'll also need a new version of chrome or chromium-browser to do this as we'll be using selenium to drive a webbrowser (in this case a chromelike browser) to handle the web part of the 2fa. create a directory somewhere convenient, like vuwvpn or omething and cd there. then python3 -m venv env source env/bin/activate # activate our python venv Then download the requirements file from my fork of the openconnect-sso. It's not my code, I just made a repo from a pull request that hasn't been merged in yet, but we need to increase the timeout. wget https://raw.githubusercontent.com/andre-geldenhuis/openconnect-sso/master/requirements.txt Then install those requirements, note they will install some stuff from my fork pip install -r requirements.txt Vuw's ssl stuff on the VPN 2fa uses an old ssl config so we need to downgrade the ssl on newer linux versions, we can do that just for the vpn, best not do this systemwide. wget https://raw.githubusercontent.com/andre-geldenhuis/openconnect-sso/master/vuwssl.conf Now to test as a one liner OPENSSL_CONF = vuwssl.conf openconnect-sso --server vpn.victoria.ac.nz --user andre.geldenhuis@vuw.ac.nz If that works, ctrl-c to end it and make a script to make this easy and avoid needing to activate a python venv everytime: Somewhere sensible to you make a file that contains the following, you'll need to adjust the paths to match yours #!/bin/bash source ~/vuwvpn/env/bin/activate OPENSSL_CONF = ~/vuwvpn/ssl.conf openconnect-sso --server vpn.victoria.ac.nz --user <firstname>.<lastname>@vuw.ac.nz chmod it to be executable and just run it in a separeate terminal window when you need to connect to the vpn","title":"Vpn alts2"},{"location":"usersub/vpn-alts2/#alternative-openconnect-sso","text":"This is an alternative way to use the Openconnect SSO if you're having trouble getting the QT5 dependancies to resolve, which is a particular problem on macOS with M1/M2 silicon. At some point the more official openconnect-sso will change to QT6 and this shouldn't be needed anymore. Note for this you will be running a fork of the openconnect-sso on my github, so it's good practise to havea look at make sure I'm not doing anything nefariouss. I'm not, but then I would say that. Also the code is not mine, it's a fork of a pull request on another repo that I;ve setup to be the main branch to simplify this process. Use with caution. You'll also need a new version of chrome or chromium-browser to do this as we'll be using selenium to drive a webbrowser (in this case a chromelike browser) to handle the web part of the 2fa. create a directory somewhere convenient, like vuwvpn or omething and cd there. then python3 -m venv env source env/bin/activate # activate our python venv Then download the requirements file from my fork of the openconnect-sso. It's not my code, I just made a repo from a pull request that hasn't been merged in yet, but we need to increase the timeout. wget https://raw.githubusercontent.com/andre-geldenhuis/openconnect-sso/master/requirements.txt Then install those requirements, note they will install some stuff from my fork pip install -r requirements.txt Vuw's ssl stuff on the VPN 2fa uses an old ssl config so we need to downgrade the ssl on newer linux versions, we can do that just for the vpn, best not do this systemwide. wget https://raw.githubusercontent.com/andre-geldenhuis/openconnect-sso/master/vuwssl.conf Now to test as a one liner OPENSSL_CONF = vuwssl.conf openconnect-sso --server vpn.victoria.ac.nz --user andre.geldenhuis@vuw.ac.nz If that works, ctrl-c to end it and make a script to make this easy and avoid needing to activate a python venv everytime: Somewhere sensible to you make a file that contains the following, you'll need to adjust the paths to match yours #!/bin/bash source ~/vuwvpn/env/bin/activate OPENSSL_CONF = ~/vuwvpn/ssl.conf openconnect-sso --server vpn.victoria.ac.nz --user <firstname>.<lastname>@vuw.ac.nz chmod it to be executable and just run it in a separeate terminal window when you need to connect to the vpn","title":"Alternative Openconnect-sso"}]}