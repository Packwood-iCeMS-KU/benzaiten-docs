<!DOCTYPE html>

<html class="writer-html5" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><link href="https://icems-research-computing.github.io/benzaiten-docs/training/" rel="canonical"/>
<link href="../img/favicon.ico" rel="shortcut icon"/>
<title>Training - Benzaiten Cluster Documentation</title>
<link href="../css/theme.css" rel="stylesheet"/>
<link href="../css/theme_extra.css" rel="stylesheet"/>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" rel="stylesheet"/>
<link href="../extra.css" rel="stylesheet"/>
<link href="../css/neoteroi-mkdocs.css" rel="stylesheet"/>
<script>
        // Current page data
        var mkdocs_page_name = "Training";
        var mkdocs_page_input_path = "training.md";
        var mkdocs_page_url = "/benzaiten-docs/training/";
      </script>
<!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>
</head>
<body class="wy-body-for-nav" role="document">
<div class="wy-grid-for-nav">
<nav class="wy-nav-side stickynav" data-toggle="wy-nav-shift">
<div class="wy-side-scroll">
<div class="wy-side-nav-search">
<a class="icon icon-home" href=".."> Benzaiten Cluster Documentation
        </a><div role="search">
<form action="../search.html" class="wy-form" id="rtd-search-form" method="get">
<input aria-label="Search docs" name="q" placeholder="Search docs" title="Type search term here" type="text"/>
</form>
</div>
</div>
<div aria-label="Navigation menu" class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation">
<ul>
<li class="toctree-l1"><a class="reference internal" href="..">Overview</a>
</li>
</ul>
<p class="caption"><span class="caption-text">Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../accessing_the_cluster/">Accessing the Cluster</a>
</li>
<li class="toctree-l1"><a class="reference internal" href="../basic_commands/">Basic Commands</a>
</li>
<li class="toctree-l1"><a class="reference internal" href="../storage/">Storage and Quotas</a>
</li>
<li class="toctree-l1"><a class="reference internal" href="../partitions/">Using Partitions</a>
</li>
<li class="toctree-l1"><a class="reference internal" href="../running_jobs/">Running Jobs</a>
</li>
<li class="toctree-l1"><a class="reference internal" href="../parallel_processing/">Parallel Processing</a>
</li>
<li class="toctree-l1"><a class="reference internal" href="../managing_jobs/">Managing Jobs</a>
</li>
<li class="toctree-l1"><a class="reference internal" href="../solar_vuw/">Connecting to High Capacity Storage</a>
</li>
<li class="toctree-l1"><a class="reference internal" href="../cloud_providers/">Connecting to Cloud Providers</a>
</li>
<li class="toctree-l1"><a class="reference internal" href="../containers/">Using Containers</a>
</li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/">Using Jupyter Notebooks</a>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples/">Examples</a>
</li>
<li class="toctree-l1"><a class="reference internal" href="../usersub/">User Submitted Docs</a>
</li>
<li class="toctree-l1"><a class="reference internal" href="../faq/">FAQ</a>
</li>
<li class="toctree-l1"><a class="reference internal" href="../hpclayout/">HPC Hardware Layout</a>
</li>
<li class="toctree-l1"><a class="reference internal" href="../support/">Support</a>
</li>
</ul>
</div>
</div>
</nav>
<section class="wy-nav-content-wrap" data-toggle="wy-nav-shift">
<nav aria-label="Mobile navigation menu" class="wy-nav-top" role="navigation">
<i class="fa fa-bars" data-toggle="wy-nav-top"></i>
<a href="..">Benzaiten Cluster Documentation</a>
</nav>
<div class="wy-nav-content">
<div class="rst-content"><div aria-label="breadcrumbs navigation" role="navigation">
<ul class="wy-breadcrumbs">
<li><a aria-label="Docs" class="icon icon-home" href=".."></a> »</li>
<li class="breadcrumb-item active">Training</li>
<li class="wy-breadcrumbs-aside">
</li>
</ul>
<hr/>
</div>
<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div class="section" itemprop="articleBody">
<h1 id="training">Training<a class="headerlink" href="#training" title="Permanent link">¶</a></h1>
<p>These are longer worked examples.  If you have domain specific training you'd like to provide for your students or peers, contact Andre, or make a pull request against this repo.</p>
<!-- BEGIN INCLUDE training/gpu-neural-style.md -->
<h2 id="gpu-example-with-neural-style-in-pytorch">GPU example with neural style in pytorch<a class="headerlink" href="#gpu-example-with-neural-style-in-pytorch" title="Permanent link">¶</a></h2>
<p>We'll do a quick python example using neural style implemented in pytorch. We will be using modules rather than conda/virtualenvs but there is nothing stopping you from loading the modules and creating a virtualenv/conda enviroment to install additional python packages.</p>
<p>The code we use will come from the pytorch example git repo.</p>
<h3 id="clone-the-pytorch-example-repo">Clone the pytorch example repo<a class="headerlink" href="#clone-the-pytorch-example-repo" title="Permanent link">¶</a></h3>
<p>In a sensible location, clone the rep.</p>
<div class="highlight"><pre><span></span><code>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/pytorch/examples.git
<span class="nb">cd</span><span class="w"> </span>examples/fast_neural_style<span class="w">  </span><span class="c1"># change to the example we will be running.</span>
</code></pre></div>
<h3 id="load-the-modules">Load the modules<a class="headerlink" href="#load-the-modules" title="Permanent link">¶</a></h3>
<p>We are using the new Easybuild based modules, to ensure we don't have conflicts with the old modules, it will be best to unuse them first and then use the new system.  At somepoint we may automatically add the new modules to your bashrc file - but currently you'll have to do this yourself or manually unuse and use the new module system.</p>
<div class="highlight"><pre><span></span><code>module<span class="w"> </span>unuse<span class="w"> </span>/home/software/tools/modulefiles/<span class="w">  </span><span class="c1">#unuse the old module system</span>
module<span class="w"> </span>use<span class="w"> </span>/home/software/tools/eb_modulefiles/all/Core<span class="w"> </span><span class="c1">#use the new module system</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>module<span class="w"> </span>load<span class="w"> </span>fosscuda/2020b
module<span class="w"> </span>load<span class="w"> </span>PyTorch/1.7.1
module<span class="w"> </span>load<span class="w"> </span>torchvision/0.8.2-PyTorch-1.7.1
module<span class="w"> </span>list<span class="w"> </span><span class="c1">#see all the dependencies we have loaded, in particular which version of python we're using now. Currently Python 3.8.6</span>
</code></pre></div>
<h3 id="optional-setup-a-virtualenv">Optional: Setup a virtualenv<a class="headerlink" href="#optional-setup-a-virtualenv" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code>python3<span class="w"> </span>-m<span class="w"> </span>venv<span class="w"> </span>env<span class="w">  </span><span class="c1"># create a virtualenv folder called env. Note! This will likely only work with the python version listed above!</span>
<span class="nb">source</span><span class="w"> </span>env/bin/activate<span class="w"> </span><span class="c1"># activate the virtualenv</span>
</code></pre></div>
<p>Now that we've activated the virtual environment, we can install any additional packages we need.  In this case we don't need any additional packages.</p>
<h3 id="download-some-images-to-use-as-content-as-well-as-for-training">Download some images to use as content as well as for training.<a class="headerlink" href="#download-some-images-to-use-as-content-as-well-as-for-training" title="Permanent link">¶</a></h3>
<p>In your <code>examples/fast_neural_style/</code> directory.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Download an image of an octopus to images/content-images. </span>
<span class="c1">## CC BY-SA 3.0 H. Zell</span>
wget<span class="w"> </span>https://upload.wikimedia.org/wikipedia/commons/0/0c/Octopus_vulgaris_02.JPG<span class="w"> </span>-P<span class="w"> </span>images/content-images/<span class="w"> </span>

<span class="c1"># Download an image of The Great Wave off Kanagawa - public domain</span>
wget<span class="w"> </span>https://upload.wikimedia.org/wikipedia/commons/a/a5/Tsunami_by_hokusai_19th_century.jpg<span class="w"> </span>-O<span class="w"> </span>images/style-images/wave.jpg
</code></pre></div>
<p>Depending on the GPU we are using, we may need to resize the image to ensure it fits in memory.  On an RTX6000 we would need to resize the image to 70% of its full size to fit in memroy.  Thankfully the GPUs on Rāpoi are A100's with 40GB of ram, so we can skip this step.</p>
<p>We will also need to download the pre-trained models for our initial inference runs.
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>download_saved_models.py
</code></pre></div></p>
<h3 id="style-some-images-inference">Style some images - inference<a class="headerlink" href="#style-some-images-inference" title="Permanent link">¶</a></h3>
<p>We'll initially just use pretrained models to generate styled images - this is known as model inference and is much less intensive than training the model, we'll do this on both CPU and GPU. </p>
<p>submit_cpu.sh
<div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash</span>

<span class="c1">#SBATCH --job-name=pytorch_test</span>
<span class="c1">#SBATCH -o _test.out</span>
<span class="c1">#SBATCH -e _test.err</span>
<span class="c1">#SBATCH --time=00:15:00</span>
<span class="c1">#SBATCH --partition=parallel</span>
<span class="c1">#SBATCH --ntasks=12</span>
<span class="c1">#SBATCH --mem=6G</span>

module<span class="w"> </span>unuse<span class="w"> </span>/home/software/tools/modulefiles/<span class="w">  </span><span class="c1">#unuse the old module system</span>
module<span class="w"> </span>use<span class="w"> </span>/home/software/tools/eb_modulefiles/all/Core<span class="w"> </span><span class="c1">#use the new module system</span>
module<span class="w"> </span>load<span class="w"> </span>fosscuda/2020b
module<span class="w"> </span>load<span class="w"> </span>PyTorch/1.7.1
module<span class="w"> </span>load<span class="w"> </span>torchvision/0.8.2-PyTorch-1.7.1

<span class="c1">#Optional</span>
<span class="nb">source</span><span class="w"> </span>env/bin/activate<span class="w">  </span><span class="c1">#activate the virtualenv</span>

<span class="c1"># Run our job --cuda 0 means run on the CPU and we'll save the output image as test1.jpg</span>
<span class="c1">#</span>
python<span class="w"> </span>neural_style/neural_style.py<span class="w"> </span><span class="nb">eval</span><span class="w"> </span>--content-image<span class="w"> </span>images/content-images/Octopus_vulgaris_02.JPG<span class="w">  </span>--model<span class="w"> </span>saved_models/mosaic.pth<span class="w"> </span>--output-image<span class="w"> </span>./test1.jpg<span class="w"> </span>--cuda<span class="w"> </span><span class="m">0</span>
</code></pre></div></p>
<p>You can check how long the job took to run with <code>vuw-job-history</code>.  The last lines are your last run job, in my case:</p>
<div class="highlight"><pre><span></span><code><span class="m">332281</span><span class="w">        </span>COMPLETED<span class="w"> </span>pytorch_t+<span class="w">              </span><span class="m">00</span>:02:36<span class="w"> </span>
<span class="m">332281</span>.batch<span class="w">  </span>COMPLETED<span class="w">      </span>batch<span class="w">      </span><span class="m">0</span>.15G<span class="w">   </span><span class="m">00</span>:02:36<span class="w"> </span>
<span class="m">332281</span>.exte+<span class="w">  </span>COMPLETED<span class="w">     </span>extern<span class="w">      </span><span class="m">0</span>.15G<span class="w">   </span><span class="m">00</span>:02:36
</code></pre></div>
<p>the job took 2:36.</p>
<p>Let's run the inference job again on GPU to see the speedup.</p>
<p>submit_gpu.sh
<div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash</span>

<span class="c1">#SBATCH --job-name=pytorch_test</span>
<span class="c1">#SBATCH -o _test.out</span>
<span class="c1">#SBATCH -e _test.err</span>
<span class="c1">#SBATCH --time=00:15:00</span>
<span class="c1">#SBATCH --partition=gpu</span>
<span class="c1">#SBATCH --gres=gpu:1</span>
<span class="c1">#SBATCH --ntasks=2</span>
<span class="c1">#SBATCH --mem=60G</span>

module<span class="w"> </span>unuse<span class="w"> </span>/home/software/tools/modulefiles/<span class="w">  </span><span class="c1">#unuse the old module system</span>
module<span class="w"> </span>use<span class="w"> </span>/home/software/tools/eb_modulefiles/all/Core<span class="w"> </span><span class="c1">#use the new module system</span>
module<span class="w"> </span>load<span class="w"> </span>fosscuda/2020b
module<span class="w"> </span>load<span class="w"> </span>PyTorch/1.7.1
module<span class="w"> </span>load<span class="w"> </span>torchvision/0.8.2-PyTorch-1.7.1

<span class="c1">#optional</span>
<span class="nb">source</span><span class="w"> </span>env/bin/activate<span class="w">  </span><span class="c1">#activate the virtualenv</span>

<span class="c1"># Run our job --cuda 1 means run on the GPU and we'll save the output image as test2.jpg</span>
<span class="c1">#</span>
python<span class="w"> </span>neural_style/neural_style.py<span class="w"> </span><span class="nb">eval</span><span class="w"> </span>--content-image<span class="w"> </span>images/content-images/Octopus_vulgaris_02.JPG<span class="w">  </span>--model<span class="w"> </span>saved_models/mosaic.pth<span class="w"> </span>--output-image<span class="w"> </span>./test2.jpg<span class="w"> </span>--cuda<span class="w"> </span><span class="m">1</span>
</code></pre></div></p>
<p>In this case <code>vuw-job-history</code> the job took:
<div class="highlight"><pre><span></span><code><span class="m">692973</span><span class="w">        </span>COMPLETED<span class="w"> </span>pytorch_t+<span class="w">              </span><span class="m">00</span>:00:16<span class="w"> </span>
<span class="m">692973</span>.batch<span class="w">  </span>COMPLETED<span class="w">      </span>batch<span class="w">      </span><span class="m">0</span>.15G<span class="w">   </span><span class="m">00</span>:00:16<span class="w"> </span>
<span class="m">692973</span>.exte+<span class="w">  </span>COMPLETED<span class="w">     </span>extern<span class="w">      </span><span class="m">0</span>.15G<span class="w">   </span><span class="m">00</span>:00:16<span class="w"> </span>
</code></pre></div></p>
<p>but the time varies a lot with short GPU runs, some are nearly 2 min long and some runs are 16s with the same data. The memory usage with pytorch is also hard to estimate, running <code>vuw-job-report 332320</code> shows:
<div class="highlight"><pre><span></span><code>Nodes:<span class="w"> </span><span class="m">1</span>
Cores<span class="w"> </span>per<span class="w"> </span>node:<span class="w"> </span><span class="m">2</span>
CPU<span class="w"> </span>Utilized:<span class="w"> </span><span class="m">00</span>:00:07
CPU<span class="w"> </span>Efficiency:<span class="w"> </span><span class="m">43</span>.75%<span class="w"> </span>of<span class="w"> </span><span class="m">00</span>:00:16<span class="w"> </span>core-walltime
Job<span class="w"> </span>Wall-clock<span class="w"> </span>time:<span class="w"> </span><span class="m">00</span>:00:08
Memory<span class="w"> </span>Utilized:<span class="w"> </span><span class="m">1</span>.38<span class="w"> </span>MB
Memory<span class="w"> </span>Efficiency:<span class="w"> </span><span class="m">0</span>.00%<span class="w"> </span>of<span class="w"> </span><span class="m">60</span>.00<span class="w"> </span>GB
</code></pre></div></p>
<p>The memory usage is very low, but there is a very brief spike in memory at the end of the run as the image is generated that <code>vuw-job-report</code> doesn't quite capture. 60G of memory is needed to ensure this completes - a good rule of thumb is to allocate at least as much system memory as GPU memory.  The A100's have 40G of ram.</p>
<h3 id="train-a-new-style-computationally-expensive">Train a new style - computationally expensive.<a class="headerlink" href="#train-a-new-style-computationally-expensive" title="Permanent link">¶</a></h3>
<p>Training a new image style is where we will get the greatest speedup using a GPU.</p>
<p>We will use 13G of training images - <a href="http://cocodataset.org/#download">COCO 2014 Training images dataset </a>. These images have already been downloaded and are accessable at <code>/nfs/home/training/neural_style_data/train2014/</code>.  Note that training a new style will take about 1:15h on an A100 and two and a half hours on an RTX6000</p>
<div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash</span>

<span class="c1">#SBATCH --job-name=pytorch_test</span>
<span class="c1">#SBATCH -o _test.out</span>
<span class="c1">#SBATCH -e _test.err</span>
<span class="c1">#SBATCH --time=03:00:00</span>
<span class="c1">#SBATCH --partition=gpu</span>
<span class="c1">#SBATCH --gres=gpu:1</span>
<span class="c1">#SBATCH --ntasks=2</span>
<span class="c1">#SBATCH --mem=60G</span>

module<span class="w"> </span>unuse<span class="w"> </span>/home/software/tools/modulefiles/<span class="w">  </span><span class="c1">#unuse the old module system</span>
module<span class="w"> </span>use<span class="w"> </span>/home/software/tools/eb_modulefiles/all/Core<span class="w"> </span><span class="c1">#use the new module system</span>
module<span class="w"> </span>load<span class="w"> </span>fosscuda/2020b
module<span class="w"> </span>load<span class="w"> </span>PyTorch/1.7.1
module<span class="w"> </span>load<span class="w"> </span>torchvision/0.8.2-PyTorch-1.7.1

<span class="c1">#Optional</span>
<span class="nb">source</span><span class="w"> </span>env/bin/activate<span class="w">  </span><span class="c1">#activate the virtualenv</span>

<span class="c1"># Run our job --cuda 1 means run on the GPU                                   </span>
<span class="c1"># style-weight and content-weight are just parameters adjusted to give better results</span>
python<span class="w"> </span>neural_style/neural_style.py<span class="w"> </span>train<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--dataset<span class="w"> </span>/nfs/home/training/neural_style_data/<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--style-image<span class="w"> </span>images/style-images/wave.jpg<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--save-model-dir<span class="w"> </span>saved_models/style5e10_content_5e4<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--style-weight<span class="w"> </span>5e10<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--content-weight<span class="w"> </span>5e4<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--epochs<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--cuda<span class="w"> </span><span class="m">1</span>
</code></pre></div>
<p>This will take a while, but should eventually complete. The A100 has enough memory to train on this image, with other GPUs you may need to scale down the style image to fit in the GPU memory.  Note: If you get an out of GPU memory error but it seems the GPU has plenty of memory, it often means you ran out of system memory, try asking for more memory in slurm.</p>
<h3 id="use-our-newly-trained-network">Use our newly trained network<a class="headerlink" href="#use-our-newly-trained-network" title="Permanent link">¶</a></h3>
<p>submit_gpu.sh
<div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash</span>

<span class="c1">#SBATCH --job-name=pytorch_test</span>
<span class="c1">#SBATCH -o _test.out</span>
<span class="c1">#SBATCH -e _test.err</span>
<span class="c1">#SBATCH --time=00:15:00</span>
<span class="c1">#SBATCH --partition=gpu</span>
<span class="c1">#SBATCH --gres=gpu:1</span>
<span class="c1">#SBATCH --ntasks=2</span>
<span class="c1">#SBATCH --mem=60G</span>

module<span class="w"> </span>unuse<span class="w"> </span>/home/software/tools/modulefiles/<span class="w">  </span><span class="c1">#unuse the old module system</span>
module<span class="w"> </span>use<span class="w"> </span>/home/software/tools/eb_modulefiles/all/Core<span class="w"> </span><span class="c1">#use the new module system</span>
module<span class="w"> </span>load<span class="w"> </span>fosscuda/2020b
module<span class="w"> </span>load<span class="w"> </span>PyTorch/1.7.1
module<span class="w"> </span>load<span class="w"> </span>torchvision/0.8.2-PyTorch-1.7.1

<span class="c1">#Optional</span>
<span class="nb">source</span><span class="w"> </span>env/bin/activate<span class="w">  </span><span class="c1">#activate the virtualenv</span>

<span class="c1"># Run our job --cuda 1 means run on the GPU and we'll save the output image as test2.jpg</span>
<span class="c1">#</span>
python<span class="w"> </span>neural_style/neural_style.py<span class="w"> </span><span class="nb">eval</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--content-image<span class="w"> </span>images/content-images/Octopus_vulgaris_02.JPG<span class="w">  </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>saved_models/style5e10_content_5e4<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output-image<span class="w"> </span>./test3.jpg<span class="w"> </span>--cuda<span class="w"> </span><span class="m">1</span>
</code></pre></div></p>
<h3 id="bonus-content-use-a-slurm-task-array-to-find-the-optimum-parameters">Bonus content use a slurm task-array to find the optimum parameters.<a class="headerlink" href="#bonus-content-use-a-slurm-task-array-to-find-the-optimum-parameters" title="Permanent link">¶</a></h3>
<p>In the above example we use parameters for style-weight and content-weight.  There are lots of possibilities for these parameters, we can use a task array and a parameter list to determine good values.   Note that actually running this example will consume a lot of resources and it is presented mostly to provide some information about task arrays.  Running this example will consume the whole GPU partition for about 12 hours.</p>
<p>First let's create a list of parameters to test, we could include these in the batch submission script, but I think it's clearer to separate them out. If you're version controlling your submission script, it'll make it easier to see what are changes to parameters and what are changes to the script itself.</p>
<p>In the parameter list, the first column is style-weight parameters and the second is content-weight parameters.
paramlist.txt
<div class="highlight"><pre><span></span><code>5e10<span class="w"> </span>1e3
5e10<span class="w"> </span>1e4
5e10<span class="w"> </span>5e4
1e11<span class="w"> </span>1e3
1e11<span class="w"> </span>1e4
1e11<span class="w"> </span>5e4
5e11<span class="w"> </span>1e3
5e11<span class="w"> </span>1e4
5e11<span class="w"> </span>5e4
1e12<span class="w"> </span>1e3
1e12<span class="w"> </span>1e4
1e12<span class="w"> </span>5e4
</code></pre></div></p>
<p>In our submission script we will parse these values with <code>awk</code>.  Awk is a bit beyond the scope of this lesson, but it is a handy shell tool for manipulating text. <a href="https://www.digitalocean.com/community/tutorials/how-to-use-the-awk-language-to-manipulate-text-in-linux">Digital ocean has a nice primer on Awk</a></p>
<p>submit_gpu_train_array
<div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash</span>

<span class="c1">#SBATCH --job-name=pytorch_test</span>
<span class="c1">#SBATCH -o _test.out</span>
<span class="c1">#SBATCH -e _test.err</span>
<span class="c1">#SBATCH --time=10:00:00</span>
<span class="c1">#SBATCH --partition=gpu</span>
<span class="c1">#SBATCH --gres=gpu:1</span>
<span class="c1">#SBATCH --ntasks=10</span>
<span class="c1">#SBATCH --mem=60G</span>
<span class="c1">#SBATCH --array=1-13</span>

module<span class="w"> </span>unuse<span class="w"> </span>/home/software/tools/modulefiles/<span class="w">  </span><span class="c1">#unuse the old module system</span>
module<span class="w"> </span>use<span class="w"> </span>/home/software/tools/eb_modulefiles/all/Core<span class="w"> </span><span class="c1">#use the new module system</span>
module<span class="w"> </span>load<span class="w"> </span>fosscuda/2020b
module<span class="w"> </span>load<span class="w"> </span>PyTorch/1.7.1
module<span class="w"> </span>load<span class="w"> </span>torchvision/0.8.2-PyTorch-1.7.1

<span class="c1">#Optional</span>
<span class="nb">source</span><span class="w"> </span>env/bin/activate<span class="w">  </span><span class="c1">#activate the virtualenv</span>

<span class="c1"># Run our job --cuda 1 means run on the GPU                                   </span>
<span class="c1">#</span>
<span class="c1">#awk -v var="$SLURM_ARRAY_TASK_ID" 'NR == var {print $1}' paramlist.txt </span>
<span class="nv">style_weight</span><span class="o">=</span><span class="k">$(</span>awk<span class="w"> </span>-v<span class="w"> </span><span class="nv">var</span><span class="o">=</span><span class="s2">"</span><span class="nv">$SLURM_ARRAY_TASK_ID</span><span class="s2">"</span><span class="w"> </span><span class="s1">'NR == var {print $1}'</span><span class="w"> </span>paramlist.txt<span class="k">)</span>
<span class="nv">content_weight</span><span class="o">=</span><span class="k">$(</span>awk<span class="w"> </span>-v<span class="w"> </span><span class="nv">var</span><span class="o">=</span><span class="s2">"</span><span class="nv">$SLURM_ARRAY_TASK_ID</span><span class="s2">"</span><span class="w"> </span><span class="s1">'NR == var {print $2}'</span><span class="w"> </span>paramlist.txt<span class="k">)</span>

<span class="nb">echo</span><span class="w"> </span><span class="nv">$style_weight</span>
<span class="nb">echo</span><span class="w"> </span><span class="nv">$content_weight</span>
python<span class="w"> </span>neural_style/neural_style.py<span class="w"> </span>train<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset<span class="w"> </span>nfs/home/training/neural_style_data/<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--style-image<span class="w"> </span>images/style-images/wave.jpg<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save-model-dir<span class="w"> </span>saved_models/test_params2_epoch2/style<span class="si">${</span><span class="nv">style_weight</span><span class="si">}</span>_content<span class="si">${</span><span class="nv">content_weight</span><span class="si">}</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--style-weight<span class="w"> </span><span class="nv">$style_weight</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--content-weight<span class="w"> </span><span class="nv">$content_weight</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--epochs<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cuda<span class="w"> </span><span class="m">1</span>
</code></pre></div></p>
<!-- END INCLUDE -->
<!-- BEGIN INCLUDE training/simple-openmpi-singularity-hybrid.md -->
<h2 id="simple-openmpi-with-singularity-using-the-hybrid-approach">Simple OpenMPI with Singularity using the hybrid approach.<a class="headerlink" href="#simple-openmpi-with-singularity-using-the-hybrid-approach" title="Permanent link">¶</a></h2>
<p>The hybrid approach is one way of getting OpenMPI working with containers. It requires the OpenMPI version inside the container to match the OpenMPI outside the container (loaded via module loading).</p>
<p>First check what openMPI version we have on Rāpoi.</p>
<p>On <strong>Rāpoi</strong> switch to our new modules
<div class="highlight"><pre><span></span><code>module<span class="w"> </span>unuse<span class="w"> </span>/home/software/tools/modulefiles<span class="w"> </span><span class="c1"># stop using the older modules</span>
module<span class="w"> </span>use<span class="w"> </span>/home/software/tools/eb_modulefiles/all/Core<span class="w"> </span><span class="c1">#the new module files organised by compiler</span>
module<span class="w"> </span>spider<span class="w"> </span>OpenMPI<span class="w"> </span><span class="c1"># search for openMPI - thre are several options, lets try</span>
module<span class="w"> </span>spider<span class="w"> </span>OpenMPI/4.0.5<span class="w">  </span><span class="c1"># we will use this one, which requires GCC/10.2.0</span>
</code></pre></div></p>
<p>On your <strong>local machine</strong> we will create a very simple C openMPI program. Create this in a sensible place.  I used <code>~/projects/examples/singularity/openMPI</code></p>
<div class="highlight"><pre><span></span><code><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;mpi.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdio.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdlib.h&gt;</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">argc</span><span class="p">,</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">**</span><span class="n">argv</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">rc</span><span class="p">;</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">size</span><span class="p">;</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">myrank</span><span class="p">;</span>

<span class="w">        </span><span class="n">rc</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MPI_Init</span><span class="w"> </span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">rc</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">MPI_SUCCESS</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="n">fprintf</span><span class="w"> </span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span><span class="w"> </span><span class="s">"MPI_Init() failed"</span><span class="p">);</span>
<span class="w">                </span><span class="k">return</span><span class="w"> </span><span class="n">EXIT_FAILURE</span><span class="p">;</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="n">rc</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MPI_Comm_size</span><span class="w"> </span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">size</span><span class="p">);</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">rc</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">MPI_SUCCESS</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="n">fprintf</span><span class="w"> </span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span><span class="w"> </span><span class="s">"MPI_Comm_size() failed"</span><span class="p">);</span>
<span class="w">                </span><span class="k">goto</span><span class="w"> </span><span class="n">exit_with_error</span><span class="p">;</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="n">rc</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MPI_Comm_rank</span><span class="w"> </span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">myrank</span><span class="p">);</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">rc</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">MPI_SUCCESS</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="n">fprintf</span><span class="w"> </span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span><span class="w"> </span><span class="s">"MPI_Comm_rank() failed"</span><span class="p">);</span>
<span class="w">                </span><span class="k">goto</span><span class="w"> </span><span class="n">exit_with_error</span><span class="p">;</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="n">fprintf</span><span class="w"> </span><span class="p">(</span><span class="n">stdout</span><span class="p">,</span><span class="w"> </span><span class="s">"Hello, I am rank %d/%d"</span><span class="p">,</span><span class="w"> </span><span class="n">myrank</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">);</span>

<span class="w">        </span><span class="n">MPI_Finalize</span><span class="p">();</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">EXIT_SUCCESS</span><span class="p">;</span>

<span class="w"> </span><span class="nl">exit_with_error</span><span class="p">:</span>
<span class="w">        </span><span class="n">MPI_Finalize</span><span class="p">();</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">EXIT_FAILURE</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div>
<p>In the same location as above create a singularity definition file, note that we choose to compile and install the same OpenMPI version as we will use on Rāpoi.</p>
<div class="highlight"><pre><span></span><code>Bootstrap:<span class="w"> </span>docker
From:<span class="w"> </span>ubuntu:latest

%files
<span class="w">    </span>mpitest.c<span class="w"> </span>/opt

%environment
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMPI_DIR</span><span class="o">=</span>/opt/ompi
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">SINGULARITY_OMPI_DIR</span><span class="o">=</span><span class="nv">$OMPI_DIR</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">SINGULARITYENV_APPEND_PATH</span><span class="o">=</span><span class="nv">$OMPI_DIR</span>/bin
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">SINGULAIRTYENV_APPEND_LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$OMPI_DIR</span>/lib

%post
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">"Installing required packages..."</span>
<span class="w">    </span>apt-get<span class="w"> </span>update<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>apt-get<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>wget<span class="w"> </span>git<span class="w"> </span>bash<span class="w"> </span>gcc<span class="w"> </span>gfortran<span class="w"> </span>g++<span class="w"> </span>make<span class="w"> </span>file

<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">"Installing Open MPI"</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMPI_DIR</span><span class="o">=</span>/opt/ompi
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMPI_VERSION</span><span class="o">=</span><span class="m">4</span>.0.5<span class="w">  </span><span class="c1">#NOTE matching version to that on Raapoi</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMPI_URL</span><span class="o">=</span><span class="s2">"https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-</span><span class="nv">$OMPI_VERSION</span><span class="s2">.tar.bz2"</span>
<span class="w">    </span>mkdir<span class="w"> </span>-p<span class="w"> </span>/tmp/ompi
<span class="w">    </span>mkdir<span class="w"> </span>-p<span class="w"> </span>/opt
<span class="w">    </span><span class="c1"># Download</span>
<span class="w">    </span><span class="nb">cd</span><span class="w"> </span>/tmp/ompi<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>wget<span class="w"> </span>-O<span class="w"> </span>openmpi-<span class="nv">$OMPI_VERSION</span>.tar.bz2<span class="w"> </span><span class="nv">$OMPI_URL</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>tar<span class="w"> </span>-xjf<span class="w"> </span>openmpi-<span class="nv">$OMPI_VERSION</span>.tar.bz2
<span class="w">    </span><span class="c1"># Compile and install</span>
<span class="w">    </span><span class="nb">cd</span><span class="w"> </span>/tmp/ompi/openmpi-<span class="nv">$OMPI_VERSION</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>./configure<span class="w"> </span>--prefix<span class="o">=</span><span class="nv">$OMPI_DIR</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>make<span class="w"> </span>install
<span class="w">    </span><span class="c1"># Set env variables so we can compile our application</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$OMPI_DIR</span>/bin:<span class="nv">$PATH</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$OMPI_DIR</span>/lib:<span class="nv">$LD_LIBRARY_PATH</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MANPATH</span><span class="o">=</span><span class="nv">$OMPI_DIR</span>/share/man:<span class="nv">$MANPATH</span>

<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">"Compiling the MPI application..."</span>
<span class="w">    </span><span class="nb">cd</span><span class="w"> </span>/opt<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>mpicc<span class="w"> </span>-o<span class="w"> </span>mpitest<span class="w"> </span>mpitest.c
</code></pre></div>
<p>Now we build our container locally, giving it a sensible name.  We need <code>OpenMPI-4.0.5</code> to use this, so let's include that in the name.
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>singularity<span class="w"> </span>build<span class="w"> </span>test-openmpi-4.0.5.sif<span class="w"> </span>test-openmpi-4.0.5.def
</code></pre></div></p>
<p>Copy that file to Rāpoi somehow - Filezilla, rsync or similar.  I'll just use sftp for simplicity.
<div class="highlight"><pre><span></span><code>sftp<span class="w"> </span>&lt;username&gt;@raapoi.vuw.ac.nz
put<span class="w"> </span>test-openmpi-4.0.5.sif
</code></pre></div>
Now on <strong>Rāpoi</strong> copy that file to a sensible location, I used <code>~/projects/examples/singularity/openMPI</code> again.</p>
<div class="highlight"><pre><span></span><code>mv<span class="w"> </span>test-openmpi-4.0.5.sif<span class="w"> </span>~/projects/examples/singularity/openMPI/
<span class="nb">cd</span><span class="w"> </span>~/projects/examples/singularity/openMPI/
</code></pre></div>
<p>In that location create a sbatch file</p>
<p>openmpi-test.sh
<div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --job-name=mpi_test</span>
<span class="c1">#SBATCH --time=00-00:02:00</span>
<span class="c1">#SBATCH --output=out_test.out</span>
<span class="c1">#SBATCH --error=out_test.err</span>
<span class="c1">#SBATCH --partition=parallel</span>
<span class="c1">#SBATCH --ntasks=2</span>
<span class="c1">#SBATCH --cpus-per-task=1</span>
<span class="c1">#SBATCH --tasks-per-node=1</span>
<span class="c1">#SBATCH --mem=1GB</span>
<span class="c1">#SBATCH --constraint="IB,AMD"</span>
<span class="c1">#SBATCH --nodes=2</span>

module<span class="w"> </span>use<span class="w"> </span>/home/software/tools/eb_modulefiles/all/Core
module<span class="w"> </span>unuse<span class="w"> </span>/home/software/tools/modulefiles<span class="w"> </span><span class="c1"># to prevent conflicts with the old modules</span>
module<span class="w"> </span>load<span class="w"> </span>GCC/10.2.0
module<span class="w"> </span>load<span class="w"> </span>OpenMPI/4.0.5
module<span class="w"> </span>load<span class="w"> </span>Singularity/3.7.3<span class="w"> </span><span class="c1"># Note this is a new singularity build</span>

<span class="nv">CONPATH</span><span class="o">=</span><span class="nv">$HOME</span>/projects/examples/singularity/openMPI
mpirun<span class="w"> </span>-np<span class="w"> </span><span class="m">2</span><span class="w"> </span>singularity<span class="w"> </span><span class="nb">exec</span><span class="w"> </span><span class="nv">$CONPATH</span>/test-openmpi-4.0.5.sif<span class="w"> </span>/opt/mpitest
</code></pre></div></p>
<p>Submit that to slurm and see the output
<div class="highlight"><pre><span></span><code>sbatch<span class="w"> </span>openmpi-test.sh
squeue<span class="w"> </span>-u<span class="w"> </span><span class="nv">$USER</span><span class="w">  </span><span class="c1"># see the job</span>
cat<span class="w"> </span>out_test.out<span class="w"> </span><span class="c1"># examine the output after the job is done</span>
</code></pre></div></p>
<!-- END INCLUDE -->
<!-- BEGIN INCLUDE training/simple-tensorflow.md -->
<h2 id="simple-tensorflow-example-using-new-module-system">Simple tensorflow example (using new module system)<a class="headerlink" href="#simple-tensorflow-example-using-new-module-system" title="Permanent link">¶</a></h2>
<p>In a sensible location create an example python script - this is basically copied verbatim from the tensorflow docs: ps://www.tensorflow.org/tutorials/quickstart/beginner</p>
<p>example.py
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"TensorFlow version:"</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>


<span class="c1"># Load and prepare the MNIST dataset. Convert the sample data from integers to floating-point numbers</span>
<span class="n">mnist</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">mnist</span>

<span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span> <span class="o">=</span> <span class="n">x_train</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span> <span class="n">x_test</span> <span class="o">/</span> <span class="mf">255.0</span>


<span class="c1"># Build a machine learning model</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)),</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">),</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="p">])</span>


<span class="c1"># The model returns a vector of log-odds scores, one for each class</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_train</span><span class="p">[:</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">predictions</span>

<span class="c1"># The tf.nn.softmax function converts these log odds to probabilities for each class</span>

<span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="c1"># Define a loss function for training.</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># This untrained model gives probabilities close to random </span>
<span class="n">loss_fn</span><span class="p">(</span><span class="n">y_train</span><span class="p">[:</span><span class="mi">1</span><span class="p">],</span> <span class="n">predictions</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="c1"># Configure and compile the model using Keras Model.compile</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span>

<span class="c1"># Train and evaluate the model - use Model.fit to adjust parameters and minimize loss</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># Check model performance</span>
<span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span>  <span class="n">y_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Return a probability - wrap the trained model and attach softmax</span>
<span class="n">probability_model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
  <span class="n">model</span><span class="p">,</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()</span>
<span class="p">])</span>
<span class="n">probability_model</span><span class="p">(</span><span class="n">x_test</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</code></pre></div></p>
<p>Next create a submission script
submit.sh
<div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash</span>

<span class="c1">#SBATCH --job-name=tensoflow_test</span>
<span class="c1">#SBATCH -o _test.out</span>
<span class="c1">#SBATCH --time=00:10:00</span>
<span class="c1">#SBATCH --partition=gpu</span>
<span class="c1">#SBATCH --ntasks=6</span>
<span class="c1">#SBATCH --mem=50G</span>
<span class="c1">#SBATCH --gres=gpu:1</span>

<span class="c1"># Use the new module system</span>
module<span class="w"> </span>use<span class="w"> </span>/home/software/tools/eb_modulefiles/all/Core

<span class="c1">#to load tf 2.6.0 you'll first need the compiler set it was built with</span>
module<span class="w"> </span>load<span class="w"> </span>foss/2021a

<span class="c1">#load tf</span>
module<span class="w"> </span>load<span class="w"> </span>TensorFlow/2.6.0-CUDA-11.3.1

<span class="c1"># Run the simple tensorflow example - taken from the docs: https://www.tensorflow.org/tutorials/quickstart/beginner</span>
python<span class="w"> </span>example.py
</code></pre></div></p>
<p>Submit your job to the queue and then observe in the queue
<div class="highlight"><pre><span></span><code>sbatch<span class="w"> </span>submit.sh
squeue<span class="w"> </span>-u<span class="w"> </span>&lt;username&gt;
</code></pre></div></p>
<p>Possible errors - Tensorflow jobs on the gpu nodes can be a bit dicey
I'd suggest always choosing more memory than the GPU has (40GB) the gpu nodes have a lot of memory so I'd suggest asking for 50GB of ram minimum.</p>
<p>There is also a relationship between cpu's allocated and memory used - the errors are not always obvious.  If you're running into issues try increasing the requested memory or reducing the requested CPUs</p>
<p>Example errors due to requesting many cpus while requesting only 50GB ram
Note std::bad_alloc - this suggests a problem allocating memory
<div class="highlight"><pre><span></span><code>terminate called after throwing an instance of 'std::bad_alloc'
  what():  std::bad_alloc
/var/lib/slurm/slurmd/job1125851/slurm_script: line 21: 46983 Aborted                 (core dumped) python example.py
</code></pre></div></p>
<p>Note this example is also in our example git repo: https://github.com/vuw-research-computing/raapoi-examples</p>
<p>In the <em>tensorflow-simple</em> directory</p>
<!-- END INCLUDE -->
<!-- BEGIN INCLUDE training/gaussian.md -->
<h2 id="example-gaussian-job-submission-on-hpc">Example Gaussian Job Submission on HPC<a class="headerlink" href="#example-gaussian-job-submission-on-hpc" title="Permanent link">¶</a></h2>
<p>Here is an example of submitting a Gaussian job on the HPC using Slurm. In this example, we will submit a Gaussian job using the <code>quicktest</code> partition, and request 1 task with 4 CPUs and 7GB of memory for a maximum run time of 1 hour. We will also load the <code>g16</code> module, which is required to run Gaussian on the HPC.</p>
<p>First, create a new directory and navigate to it:</p>
<div class="highlight"><pre><span></span><code>mkdir<span class="w"> </span>gaussian_example
<span class="nb">cd</span><span class="w"> </span>gaussian_example
</code></pre></div>
<h3 id="get-the-example-input-file">Get the example input file<a class="headerlink" href="#get-the-example-input-file" title="Permanent link">¶</a></h3>
<p>The <code>test0397.com</code> file is an example input file for Gaussian. It contains instructions for Gaussian to perform a calculation on a molecule.</p>
<p>To run the example job using this input file, you should copy the <code>test0397.com</code> file from the Gaussian installation directory at <code>/home/software/apps/gaussian/g16/tests/com/test0397.com</code> to your working directory (<code>gaussian_example</code> in this case). </p>
<p>To do that from the <code>gaussian_example</code> directory:
<div class="highlight"><pre><span></span><code>cp<span class="w"> </span>/home/software/apps/gaussian/g16/tests/com/test0397.com<span class="w"> </span>.<span class="w"> </span><span class="c1"># copy from location to . The dot means current directory</span>
</code></pre></div></p>
<p>Have a look at the first few lines of the input file to see what it does.</p>
<div class="highlight"><pre><span></span><code>head<span class="w"> </span>test0397.com<span class="w">  </span><span class="c1"># the first 5 lines of test0397.com</span>

<span class="c1">#returns</span>
!%nproc<span class="o">=</span><span class="m">4</span>
<span class="c1">#p rb3lyp/3-21g force test scf=novaracc</span>

Gaussian<span class="w"> </span>Test<span class="w"> </span>Job<span class="w"> </span><span class="m">397</span>:
Valinomycin<span class="w"> </span>force

<span class="m">0</span>,1
O,-1.3754834437,-2.5956821046,3.7664927822
O,-0.3728418073,-0.530460483,3.8840401686
O,2.3301890394,0.5231526187,1.7996834334
</code></pre></div>
<p>The first line <code>!%nproc=4</code> specifies the number of processors that Gaussian will use to run the calculation, in this case, 4.</p>
<p>We will need to make sure that the number of processes used in this file matches the number of cpus we request from Slurm</p>
<h3 id="slurm-submission">Slurm Submission<a class="headerlink" href="#slurm-submission" title="Permanent link">¶</a></h3>
<p>Next, create a submission script called <code>submit.sh</code> (using nano or similar) and add the following contents:</p>
<div class="highlight"><pre><span></span><code><span class="ch">#!/bin/sh</span>
<span class="c1">#SBATCH --job-name=g16-test</span>

<span class="c1"># max run time</span>
<span class="c1">#SBATCH --time=1:00:00</span>
<span class="c1">#SBATCH --partition=quicktest</span>

<span class="c1">#SBATCH --output=_quicktest.out</span>
<span class="c1">#SBATCH --error=_quicktest.err</span>

<span class="c1">#SBATCH --ntasks=1</span>
<span class="c1">#SBATCH --cpus-per-task=4</span>
<span class="c1">#SBATCH --mem=7G</span>

module<span class="w"> </span>load<span class="w"> </span>gaussian/g16

g16<span class="w"> </span>test0397.com
</code></pre></div>
<p>In the submission script, we specify the following:</p>
<ul>
<li><code>--job-name</code>: name of the job to appear in the queue.</li>
<li><code>--time</code>: maximum runtime for the job in <code>hh:mm:ss</code> format.</li>
<li><code>--partition</code>: the partition to run the job on.</li>
<li><code>--output</code>: specifies the name of the standard output file.</li>
<li><code>--error</code>: specifies the name of the standard error file.</li>
<li><code>--ntasks</code>: specifies the number of tasks the job will use.</li>
<li><code>--cpus-per-task</code>: specifies the number of CPUs per task.</li>
<li><code>--mem</code>: specifies the amount of memory to allocate for the job.</li>
</ul>
<p>Submit the job to the queue using <code>sbatch</code>:</p>
<div class="highlight"><pre><span></span><code>sbatch<span class="w"> </span>submit.sh
</code></pre></div>
<p>You can check the status of your job in the queue using <code>squeue</code>:</p>
<div class="highlight"><pre><span></span><code>squeue<span class="w"> </span>-u<span class="w"> </span>&lt;your_username&gt;
</code></pre></div>
<p>Once the job is finished, you can check for the output files and see the contents of the standard output file using <code>cat</code>:</p>
<div class="highlight"><pre><span></span><code>ls
cat<span class="w"> </span>_quicktest.out
</code></pre></div>
<p>The Gaussian output files (test0397.log, test0397.chk, etc.) will also be generated in the working directory. You can view the output file using the less command:</p>
<div class="highlight"><pre><span></span><code>less<span class="w"> </span>test0397.log
</code></pre></div>
<p>Press <code>q</code> to Quit the less program</p>
<p>That's it! You have successfully submitted and run a Gaussian job on a HPC cluster using Slurm.</p>
<!-- END INCLUDE -->
</div>
</div><footer>
<hr/>
<div role="contentinfo">
<!-- Copyright etc -->
</div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
</div>
</div>
</section>
</div>
<div aria-label="Versions" class="rst-versions" role="note">
<span class="rst-current-version" data-toggle="rst-current-version">
</span>
</div>
<script src="../js/jquery-3.6.0.min.js"></script>
<script>var base_url = "..";</script>
<script src="../js/theme_extra.js"></script>
<script src="../js/theme.js"></script>
<script src="https://unpkg.com/mermaid@8.7.0/dist/mermaid.min.js"></script>
<script src="../search/main.js"></script>
<script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>
</body>
</html>
